{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "#import torch.utils\n",
    "#import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.nn.parameter import Parameter\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "dataset = 'PROTEINS2'\n",
    "dataset = './graph_nn-master/graph_nn-master/data/%s'%dataset\n",
    "training_size_p = 0.9\n",
    "balance = False\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "log_interval = 10\n",
    "lr = 0.001\n",
    "wdecay = 1e-4\n",
    "\n",
    "model_name = 'Graph-U-Net' #GCN', 'Graph-U-Net'\"\"\n",
    "filters_gcn = [64,64,64,64]\n",
    "pooling_ratios = [0.9,0.8,0.7]\n",
    "topk_activation = 'sigmoid' #'sigmoid' 'tanh'\n",
    "dropout_gcn = 0.3\n",
    "gcn_bias = True\n",
    "n_hidden_fc = 512 #or int\n",
    "fc_bias = True\n",
    "fc_activation = eval('nn.'+'ELU'+'(inplace=True)')#'ELU' #'ReLU'\n",
    "dropout_fc = 0.3\n",
    "gcn_activation = eval('nn.'+'ELU'+'(inplace=True)')#'ELU' #'ReLU'\n",
    "\n",
    "device = 'cpu'  # 'cuda', 'cpu'\n",
    "seed = 'Random'\n",
    "threads = 0 #線程數目\n",
    "\n",
    "n_folds = 1  # 10-fold cross validation\n",
    "output_folder = dataset\n",
    "\n",
    "gUnpool = 'gUnpool+residual_connection' #'gUnpool' 'None'\n",
    "pooling_method = 'max,sum,mean'\n",
    "pooing_by_layer_concatenation = True\n",
    "if gUnpool != 'None':\n",
    "    gUnpool_gcn = filters_gcn[:-1]\n",
    "    gUnpool_gcn.reverse()\n",
    "#hier_pooling = True\n",
    "\n",
    "graph_kernel = 'Normalized-Laplacian' #Normalization\n",
    "graph_kernel = 'Normalization'\n",
    "\n",
    "\n",
    "dropout_gPool = 0.3\n",
    "filters_pre_gcn = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN layers and models\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_features,\n",
    "                out_features,\n",
    "                activation=None,\n",
    "                dropout_gcn=0,\n",
    "                gcn_bias=True):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=in_features, out_features=out_features,bias=gcn_bias)\n",
    "        self.activation = activation\n",
    "        self.drop = nn.Dropout(p=dropout_gcn) if dropout_gcn > 0.0 else nn.Identity()\n",
    "    \n",
    "    def L_batch(self, A):\n",
    "        batch, N = A.shape[:2]\n",
    "        I = torch.eye(N).unsqueeze(0).to(device)\n",
    "        A_hat = A + I \n",
    "        if graph_kernel == 'Normalized-Laplacian':\n",
    "            D_hat = (torch.sum(A_hat, 1)) ** (-0.5)\n",
    "            L = D_hat.view(batch, N, 1) * A_hat * D_hat.view(batch, 1, N)\n",
    "        elif graph_kernel == 'Normalization':\n",
    "            D_hat = (torch.sum(A_hat, 1)) ** (-1)\n",
    "            L = D_hat.view(batch, N, 1) * A_hat \n",
    "        return L\n",
    "    def forward(self, data):\n",
    "        x, A = data[:2]\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(torch.bmm(self.L_batch(A), x))\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x) \n",
    "        return (x, A)\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 filters_gcn=[64,64,64],\n",
    "                 dropout_gcn=0,\n",
    "                 n_hidden_fc=0,\n",
    "                 dropout_fc=0.2,\n",
    "                 gcn_bias=gcn_bias,\n",
    "                 gcn_activation=gcn_activation,\n",
    "                 fc_bias=fc_bias):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # GCN\n",
    "        self.gconv = nn.Sequential(*([GraphConv(in_features=in_features if layer == 0 else filters_gcn[layer - 1], \n",
    "                                                out_features=f, \n",
    "                                                activation=gcn_activation,\n",
    "                                                dropout_gcn=dropout_gcn,\n",
    "                                                gcn_bias=gcn_bias) for layer, f in enumerate(filters_gcn)]))\n",
    "        # Fully connected layers\n",
    "        fc = []\n",
    "        if dropout_fc > 0:\n",
    "            fc.append(nn.Dropout(p=dropout_fc))\n",
    "        if n_hidden_fc > 0:\n",
    "            fc.append(nn.Linear(filters_gcn[-1], n_hidden_fc, bias=fc_bias)) \n",
    "            if dropout_fc > 0:\n",
    "                fc.append(nn.Dropout(p=dropout_fc))\n",
    "            n_last = n_hidden_fc\n",
    "        else: \n",
    "            n_last = filters_gcn[-1]\n",
    "        fc.append(nn.Linear(n_last, out_features, bias=fc_bias))\n",
    "        self.fc = nn.Sequential(*fc) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = self.gconv(data)[0]\n",
    "        x = torch.max(x, dim=1)[0].squeeze()  # max pooling over nodes\n",
    "        x = self.fc(x) \n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x  \n",
    "    \n",
    "class GraphUnet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 filters_gcn=[64,64,64],\n",
    "                 dropout_gcn=0,\n",
    "                 gcn_bias=gcn_bias,\n",
    "                 gcn_activation=gcn_activation,\n",
    "                 fc_bias=fc_bias,\n",
    "                 fc_activation=fc_activation,\n",
    "                 n_hidden_fc=0,\n",
    "                 dropout_fc=0.2,\n",
    "                 pooling_ratios=[0.8, 0.8],dropout_gPool=0.3):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.pooling_ratios = pooling_ratios\n",
    "        if topk_activation == 'tanh':\n",
    "            self.activation_topk = torch.tanh\n",
    "        elif topk_activation == 'sigmoid':\n",
    "            self.activation_topk = torch.sigmoid\n",
    "        \n",
    "        #pre-GCN\n",
    "        self.pre_gconv = nn.ModuleList([GraphConv(in_features=in_features if layer == 0 else filters_pre_gcn[layer - 1], \n",
    "                                              out_features=f, \n",
    "                                              activation=gcn_activation,\n",
    "                                              dropout_gcn=dropout_gcn,\n",
    "                                              gcn_bias=gcn_bias) for layer, f in enumerate(filters_pre_gcn)])\n",
    "\n",
    "        # GCN\n",
    "        if filters_pre_gcn !='None':\n",
    "            self.gconv = nn.ModuleList([GraphConv(in_features=filters_pre_gcn[-1] if layer == 0 else filters_gcn[layer - 1], \n",
    "                                                  out_features=f, \n",
    "                                                  activation=gcn_activation,\n",
    "                                                  dropout_gcn=dropout_gcn,\n",
    "                                                  gcn_bias=gcn_bias) for layer, f in enumerate(filters_gcn)])            \n",
    "        else:\n",
    "            self.gconv = nn.ModuleList([GraphConv(in_features=in_features if layer == 0 else filters_gcn[layer - 1], \n",
    "                                                  out_features=f, \n",
    "                                                  activation=gcn_activation,\n",
    "                                                  dropout_gcn=dropout_gcn,\n",
    "                                                  gcn_bias=gcn_bias) for layer, f in enumerate(filters_gcn)])\n",
    "\n",
    "        # gPool\n",
    "        self.proj = []\n",
    "        self.dropout_gPool = nn.Dropout(p = dropout_gPool)\n",
    "        for layer, f in enumerate(filters_gcn[:-1]):\n",
    "            fan_in = filters_gcn[layer]\n",
    "            p = Parameter(torch.Tensor(fan_in, 1))\n",
    "            bound = 1 / math.sqrt(fan_in) \n",
    "            torch.nn.init.uniform_(p, -bound, bound)\n",
    "            self.proj.append(p) \n",
    "        # gUnpool\n",
    "        if gUnpool != 'None':\n",
    "            self.gUnpool_gcn = nn.ModuleList([GraphConv(in_features=filters_gcn[-1] if layer == 0 else gUnpool_gcn[layer-1], \n",
    "                                              out_features=f, \n",
    "                                              activation=gcn_activation,\n",
    "                                              dropout_gcn=dropout_gcn,\n",
    "                                              gcn_bias=gcn_bias) for layer, f in enumerate(gUnpool_gcn)])\n",
    "\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        pooling_noumber = len(pooling_method.split(','))\n",
    "        if pooing_by_layer_concatenation == True:\n",
    "            if gUnpool != 'None':\n",
    "                in_fc = sum(gUnpool_gcn)*pooling_noumber + filters_pre_gcn[-1]*pooling_noumber\n",
    "            elif gUnpool == 'None':\n",
    "                in_fc = sum(filters_gcn)*pooling_noumber\n",
    "        elif pooing_by_layer_concatenation == False: \n",
    "            if gUnpool != 'None':\n",
    "                in_fc = gUnpool_gcn[-1]*pooling_noumber\n",
    "            elif gUnpool == 'None':\n",
    "                in_fc = filters_gcn[-1]*pooling_noumber                  \n",
    "                    \n",
    "            \n",
    "        fc = []\n",
    "        if dropout_fc > 0:\n",
    "            fc.append(nn.Dropout(p=dropout_fc))\n",
    "        if n_hidden_fc > 0:\n",
    "            fc.append(nn.Linear(in_fc, n_hidden_fc,bias=fc_bias))\n",
    "            fc.append(fc_activation)\n",
    "            if dropout_fc > 0:\n",
    "                fc.append(nn.Dropout(p=dropout_fc))\n",
    "            n_last = n_hidden_fc\n",
    "        else:\n",
    "            n_last = in_fc\n",
    "        fc.append(nn.Linear(n_last, out_features,bias=fc_bias))  \n",
    "        self.fc = nn.Sequential(*fc)\n",
    "        \n",
    "            \n",
    "    def forward(self, data):\n",
    "        #pre-GCN\n",
    "        N_nodes_dict = dict()\n",
    "        mask = data[2].clone()\n",
    "        N_nodes_dict[0] = torch.sum(mask, dim=1).reshape(len(torch.sum(mask, dim=1)),1)\n",
    "        for layer,gconv in enumerate(self.pre_gconv):\n",
    "            x = gconv(data) [0]\n",
    "            data[0] = x\n",
    "        org_x = data[0]\n",
    "\n",
    "        A_dict = dict()\n",
    "        x_dict = dict()\n",
    "        for layer, gconv in enumerate(self.gconv):\n",
    "            \n",
    "            #GCN\n",
    "            N_nodes = data[3] \n",
    "            N_nodes_max = N_nodes.max()     \n",
    "            mask = data[2].clone()\n",
    "            data = gconv(data) \n",
    "            x, W = data \n",
    "            A_dict[layer] = W\n",
    "            x_dict[layer] = x\n",
    "            N_nodes_dict[layer+1] = torch.sum(mask, dim=1).reshape(len(torch.sum(mask, dim=1)),1)\n",
    "\n",
    "            # gPool\n",
    "            if layer < len(self.gconv) - 1: \n",
    "                B, N, C = x.shape \n",
    "                x = self.dropout_gPool(x)\n",
    "                y = torch.mm(x.view(B * N, C), self.proj[layer]).view(B, N)\n",
    "                y = y / (torch.sum(self.proj[layer] ** 2).view(1, 1) ** 0.5) \n",
    "                idx = torch.sort(y, dim=1)[1]  \n",
    "                N_remove = (N_nodes.float() * (1 - self.pooling_ratios[layer])).long() \n",
    "                N_nodes_prev = N_nodes \n",
    "                N_nodes = N_nodes - N_remove \n",
    "                for b in range(B):\n",
    "                    idx_b = idx[b, mask[b, idx[b]] == 1]\n",
    "                    assert len(idx_b) >= N_nodes[b], (len(idx_b), N_nodes[b])\n",
    "                    mask[b, idx_b[:N_remove[b]]] = 0\n",
    "                mask = mask.unsqueeze(2)     \n",
    "                x = x * self.activation_topk(y).unsqueeze(2) * mask\n",
    "                W = mask * W * mask.view(B, 1, N) \n",
    "                mask = mask.squeeze()\n",
    "                data = (x, W, mask, N_nodes)\n",
    "        \n",
    "\n",
    "        # gUnpool\n",
    "        if gUnpool != 'None':\n",
    "            hier_x_up = []\n",
    "            for layer, gconv in enumerate(self.gUnpool_gcn):\n",
    "                data = (x,A_dict[len(self.gUnpool_gcn)-1-layer])\n",
    "                x = gconv(data)[0]\n",
    "                if gUnpool == 'gUnpool+residual_connection':\n",
    "                    x = x.add(x_dict[len(self.gUnpool_gcn)-1-layer])\n",
    "                hier_x_up.append(x)\n",
    "            x = x.add(org_x)\n",
    "            hier_x_up.append(x)\n",
    "        # nodes pooling method\n",
    "        if pooing_by_layer_concatenation == True:\n",
    "            if gUnpool != 'None':\n",
    "                pooling_ls = []\n",
    "                if 'max' in pooling_method:\n",
    "                    heir_max = [torch.max(h, 1)[0] for h in hier_x_up]\n",
    "                    heir_max = torch.cat(heir_max,1)\n",
    "                    pooling_ls.append(heir_max)\n",
    "                if 'sum' in pooling_method:\n",
    "                    heir_sum = [torch.sum(h, 1) for h in hier_x_up]\n",
    "                    heir_sum = torch.cat(heir_sum,1)\n",
    "                    pooling_ls.append(heir_sum)\n",
    "                if 'mean' in pooling_method:\n",
    "                    heir_mean = [torch.sum(hier_x_up[h], 1)/N_nodes_dict[len(hier_x_up)-1-h] for h in range(len(hier_x_up))]\n",
    "                    heir_mean = torch.cat(heir_mean,1)\n",
    "                    pooling_ls.append(heir_mean)\n",
    "                x = torch.cat(pooling_ls,1)         \n",
    "            elif gUnpool == 'None':\n",
    "                pooling_ls = []\n",
    "                if 'max' in pooling_method:\n",
    "                    heir_max = [torch.max(x_dict[h], 1)[0] for h in x_dict.keys()]\n",
    "                    heir_max = torch.cat(heir_max,1)\n",
    "                    pooling_ls.append(heir_max)\n",
    "                if 'sum' in pooling_method:\n",
    "                    heir_sum = [torch.sum(x_dict[h], 1) for h in x_dict.keys()]\n",
    "                    heir_sum = torch.cat(heir_sum,1)\n",
    "                    pooling_ls.append(heir_sum)\n",
    "                if 'mean' in pooling_method:\n",
    "                    heir_mean = [torch.sum(x_dict[h], 1)/N_nodes_dict[h] for h in x_dict.keys()]\n",
    "                    heir_mean = torch.cat(heir_mean,1)\n",
    "                    pooling_ls.append(heir_mean)\n",
    "                x = torch.cat(pooling_ls,1)                      \n",
    "                \n",
    "        elif pooing_by_layer_concatenation == False:         \n",
    "            if gUnpool != 'None':\n",
    "                pooling_ls = []\n",
    "                if 'max' in pooling_method:\n",
    "                    heir_max = torch.max(hier_x_up[-1], 1)[0]\n",
    "                    pooling_ls.append(heir_max)\n",
    "                if 'sum' in pooling_method:\n",
    "                    heir_sum = torch.sum(hier_x_up[-1], 1)\n",
    "                    pooling_ls.append(heir_sum)\n",
    "                if 'mean' in pooling_method:\n",
    "                    heir_mean = torch.sum(hier_x_up[-1], 1)/N_nodes_dict[0]\n",
    "                    pooling_ls.append(heir_mean)\n",
    "                x = torch.cat(pooling_ls,1)               \n",
    "            elif gUnpool == 'None':\n",
    "                pooling_ls = []\n",
    "                if 'max' in pooling_method:\n",
    "                    heir_max = torch.max(x_dict[max(x_dict.keys())], 1)[0]\n",
    "                    pooling_ls.append(heir_max)\n",
    "                if 'sum' in pooling_method:\n",
    "                    heir_sum = torch.sum(x_dict[max(x_dict.keys())], 1)\n",
    "                    pooling_ls.append(heir_sum)\n",
    "                if 'mean' in pooling_method:\n",
    "                    heir_mean = torch.sum(x_dict[max(x_dict.keys())], 1)/N_nodes_dict[max(x_dict.keys())]\n",
    "                    pooling_ls.append(heir_mean)\n",
    "                x = torch.cat(pooling_ls,1)   \n",
    "                \n",
    "        # fc\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader and reader\n",
    "class GraphData(torch.utils.data.Dataset):\n",
    "    def __init__(self,datareader,fold_id,split):\n",
    "        self.fold_id = fold_id #預設0，只執行一次\n",
    "        self.split = split #\"train\" or \"test\"\n",
    "        self.rnd_state = datareader.rnd_state\n",
    "        self.set_fold(datareader.data, fold_id) #利用方法，建立屬性。set_fold()在下面\n",
    "        \n",
    "        \n",
    "    def set_fold(self, data, fold_id):\n",
    "        self.total = len(data['labels']) #graph數目\n",
    "        self.N_nodes_max = data['N_nodes_max'] #最多node的graph之node數目\n",
    "        self.n_classes = data['n_classes'] #graph分類的種類數目\n",
    "        self.features_dim = data['features_dim'] #node的feature數目\n",
    "        self.idx = data['splits'][fold_id][self.split]#train或test的index\n",
    "        self.labels = copy.deepcopy([data['labels'][i] for i in self.idx])#特定index(train or test)下的graph labels\n",
    "        self.adj_list = copy.deepcopy([data['adj_list'][i] for i in self.idx])#特定index(train or test)下的A矩陣\n",
    "        self.features_onehot = copy.deepcopy([data['features_onehot'][i] for i in self.idx])#特定index(train or test)下的node feature\n",
    "        print('%s: %d/%d' % (self.split.upper(), len(self.labels), len(data['labels'])))\n",
    "        self.indices = np.arange(len(self.idx))  # sample indices for this epoch(for這次epoch，index從新編碼)\n",
    "        self.label_to_target = data['label_to_target']\n",
    "        self.node_idx_to_id = data['node_idx_to_id']\n",
    "        self.targets = data['targets']\n",
    "        \n",
    "    def pad(self, mtx, desired_dim1, desired_dim2=None, value=0):\n",
    "        sz = mtx.shape\n",
    "        assert len(sz) == 2, ('only 2d arrays are supported', sz)\n",
    "        if desired_dim2 is not None:\n",
    "            mtx = np.pad(mtx, ((0, desired_dim1 - sz[0]), (0, desired_dim2 - sz[1])), 'constant', constant_values=value)\n",
    "        else:\n",
    "            mtx = np.pad(mtx, ((0, desired_dim1 - sz[0]), (0, 0)), 'constant', constant_values=value)\n",
    "        return mtx\n",
    "    \n",
    "    def nested_list_to_torch(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            keys = list(data.keys())           \n",
    "        for i in range(len(data)):\n",
    "            if isinstance(data, dict):\n",
    "                i = keys[i]\n",
    "            if isinstance(data[i], np.ndarray):\n",
    "                data[i] = torch.from_numpy(data[i]).float()\n",
    "            elif isinstance(data[i], list):\n",
    "                data[i] = list_to_torch(data[i])\n",
    "        return data\n",
    "        \n",
    "    def __len__(self): #__len__:未來可以len(類別)，呼叫下面code\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):#__getitem__:未來這個類別可以使用[]索引，來完成下面code\n",
    "        index = self.indices[index]\n",
    "        N_nodes_max = self.N_nodes_max\n",
    "        N_nodes = self.adj_list[index].shape[0]\n",
    "        graph_support = np.zeros(self.N_nodes_max)\n",
    "        graph_support[:N_nodes] = 1\n",
    "        #1.把features捕到620,預設補0\n",
    "        #2.把adj補到620*620,預設補0\n",
    "        #3.graph_support: mask\n",
    "        #4.每個圖的真正nodes數\n",
    "        return self.nested_list_to_torch([self.pad(self.features_onehot[index].copy(), self.N_nodes_max),  # node_features\n",
    "                                          self.pad(self.adj_list[index], self.N_nodes_max, self.N_nodes_max),  # adjacency matrix\n",
    "                                          graph_support,  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1 \n",
    "                                          N_nodes,\n",
    "                                          int(self.labels[index]),\n",
    "                                          self.idx[index]])  # convert to torch\n",
    "\n",
    "class DataReader():\n",
    "    def __init__(self,\n",
    "                 data_dir, \n",
    "                 rnd_state=None, \n",
    "                 training_size_p=None,\n",
    "                 folds=None,\n",
    "                 balance=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.rnd_state = np.random.RandomState() if rnd_state == 'Random' else np.random.RandomState(int(rnd_state))\n",
    "        \n",
    "        files = os.listdir(self.data_dir)\n",
    "        \n",
    "        #data starage!\n",
    "        data = {}\n",
    "        #1. nodes:為dict，{node_id:graph_id}\n",
    "        #2. graphs:為dict,{graph_id:np.array([node_id 1,node_id 2,...])}\n",
    "        nodes, graphs = self.read_graph_nodes_relations(list(filter(lambda f: f.find('graph_indicator') >= 0, files))[0])\n",
    "        #3. data['node_id_to_idx']\n",
    "        node_id_to_idx, node_idx_to_id= self.read_node_ID(list(filter(lambda f: f.find('node_features') >= 0, files))[0])\n",
    "        data['node_id_to_idx'] = node_id_to_idx\n",
    "        data['node_idx_to_id'] = node_idx_to_id\n",
    "        #4. data['features_onehot']\n",
    "        data['features_onehot'] = self.read_node_features(list(filter(lambda f: f.find('node_features') >= 0, files))[0], nodes, graphs)  \n",
    "        #data['adj_list']\n",
    "        data['adj_list'] = self.read_graph_adj(list(filter(lambda f: f.find('_A') >= 0, files))[0], nodes, graphs,node_id_to_idx) \n",
    "        #data['labels'] 0開始\n",
    "        target_to_label = {}\n",
    "        label_to_target = {}\n",
    "        targets = np.array(self.parse_txt_file(list(filter(lambda f: f.find('graph_labels') >= 0, files))[0], \n",
    "                                      line_parse_fn=lambda s: s.strip()))\n",
    "        data['targets'] = targets\n",
    "        target_category = sorted(list(set(targets)))\n",
    "        for l, t in enumerate(target_category): \n",
    "            target_to_label[t] = l\n",
    "            label_to_target[l] = t\n",
    "        data['labels'] = np.array([target_to_label[t] for t in targets])\n",
    "        data['target_to_label'] = target_to_label\n",
    "        data['label_to_target'] = label_to_target\n",
    "        n_edges, degrees = [], []\n",
    "        for sample_id, adj in enumerate(data['adj_list']):\n",
    "            N = len(adj)  # number of nodes\n",
    "            n = np.sum(adj)  # total sum of edges\n",
    "            n_edges.append(int(n/2))  # undirected edges, so need to divide by 2\n",
    "            degrees.extend(list(np.sum(adj, 1)))\n",
    "        features_dim = len(data['features_onehot'][0][0])\n",
    "        shapes = [len(adj) for adj in data['adj_list']]\n",
    "        N_nodes_max = np.max(shapes)\n",
    "        classes = target_category\n",
    "        n_classes = len(target_category)\n",
    "\n",
    "        print('N nodes avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(shapes), np.std(shapes), np.min(shapes), np.max(shapes)))\n",
    "        print('N edges avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(n_edges), np.std(n_edges), np.min(n_edges), np.max(n_edges)))\n",
    "        print('Node degree avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(degrees), np.std(degrees), np.min(degrees), np.max(degrees)))\n",
    "        print('Node features dim: \\t\\t%d' % features_dim)\n",
    "        print('N classes: \\t\\t\\t%d' % n_classes)\n",
    "        print('Classes: \\t\\t\\t%s' %(', '.join(classes)))\n",
    "        for lbl in classes:\n",
    "            print('Class %s: \\t\\t\\t%s samples' % (lbl, np.sum(targets == lbl)))\n",
    "        #判斷每個資料中，graph數量是否相等\n",
    "        N_graphs = len(data['labels']) \n",
    "        assert N_graphs == len(data['adj_list']) == len(data['features_onehot']), 'invalid data'\n",
    "\n",
    "        train_ids, test_ids = self.split_ids(data['labels'], rnd_state=self.rnd_state, training_size_p=training_size_p,\n",
    "                                             folds=n_folds, balance=balance)\n",
    "        splits = [] #塞入dict('train':[index...],'test':[index...])\n",
    "        for fold in range(folds):\n",
    "            splits.append({'train': train_ids[fold],\n",
    "                           'test': test_ids[fold]})\n",
    "        \n",
    "        data['splits'] = splits #folds份的train和test之index\n",
    "        data['N_nodes_max'] = N_nodes_max\n",
    "        data['features_dim'] = features_dim\n",
    "        data['n_classes'] = n_classes #graph label種類數目\n",
    "        \n",
    "        self.data = data # data為一個dict()\n",
    "\n",
    "    def split_ids(self, labels_all, rnd_state=None,folds=1, training_size_p=None, balance=False):\n",
    "        if folds == 1:\n",
    "            if balance == True:\n",
    "                classes = list(set(labels_all))\n",
    "                classes_dict = dict()\n",
    "                for i in classes:\n",
    "                    classes_dict[i] = []\n",
    "                for idx,l in enumerate(labels_all):\n",
    "                    classes_dict[l].append(idx)\n",
    "                min_classes_n = len(labels_all)\n",
    "                for i in classes:\n",
    "                    if len(classes_dict[i]) < min_classes_n:\n",
    "                        min_classes_n = len(classes_dict[i])\n",
    "                training_size_per_class = int(np.round(min_classes_n*training_size_p))\n",
    "                ids_all = np.arange(len(labels_all))\n",
    "                ids = ids_all[rnd_state.permutation(len(ids_all))]\n",
    "                train_ids = []\n",
    "                for i in classes:\n",
    "                    class_ls = np.array(classes_dict[i])\n",
    "                    sampling = class_ls[rnd_state.permutation(len(class_ls))][0:training_size_per_class]\n",
    "\n",
    "                    train_ids.extend(sampling)\n",
    "                test_ids = [np.array([e for e in ids if e not in train_ids])]    \n",
    "                train_ids = [np.array(train_ids)]\n",
    "            else:\n",
    "                ids_all = np.arange(len(labels_all))\n",
    "                n = len(ids_all) #n:graph的數目\n",
    "                ids = ids_all[rnd_state.permutation(n)]\n",
    "                testing_size = int(np.round(n*(1-training_size_p)))\n",
    "                test_ids = ids[0:testing_size] # 包著np.array()\n",
    "                train_ids = [np.array([e for e in ids if e not in test_ids])] # 包著np.array()\n",
    "                test_ids = [test_ids]\n",
    "        elif folds > 1:\n",
    "            ids_all = np.arange(len(labels_all))\n",
    "            n = len(ids_all)\n",
    "            ids = ids_all[rnd_state.permutation(n)]\n",
    "            stride = int(np.ceil(n / float(folds)))\n",
    "            test_ids = [ids[i: i + stride] for i in range(0, n, stride)]\n",
    "            assert np.all(np.unique(np.concatenate(test_ids)) == sorted(ids_all)), 'some graphs are missing in the test sets'\n",
    "            assert len(test_ids) == folds, 'invalid test sets'\n",
    "            train_ids = []\n",
    "            for fold in range(folds):\n",
    "                train_ids.append(np.array([e for e in ids if e not in test_ids[fold]]))\n",
    "                assert len(train_ids[fold]) + len(test_ids[fold]) == len(np.unique(list(train_ids[fold]) + list(test_ids[fold]))) == n, 'invalid splits'\n",
    "\n",
    "        return train_ids, test_ids\n",
    "\n",
    "    def parse_txt_file(self, fpath, line_parse_fn=None):\n",
    "        #pjoin=os.path.join:路徑拼接\n",
    "        #os.path.join([PATH_1], [PATH_2], [PATH_3], ...)-->return:[PATH_1]/[PATH_2]/[PATH_3]\n",
    "        with open(pjoin(self.data_dir, fpath), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        #if line_parse_fn is not None else s:代表如果有處理字串函數就執行，否則就保留原本的樣子\n",
    "        data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]\n",
    "        return data\n",
    "    \n",
    "    def read_graph_adj(self, fpath, nodes, graphs, node_id_to_idx):\n",
    "        def fn_read_graph_adj(s):\n",
    "            if ',' in s:\n",
    "                return s.strip().split(',')\n",
    "            else:\n",
    "                return s.strip().split()\n",
    "        edges = self.parse_txt_file(fpath, line_parse_fn=fn_read_graph_adj)\n",
    "        adj_dict = {}\n",
    "        for edge in edges:\n",
    "            node1 = node_id_to_idx[edge[0].strip()]\n",
    "            node2 = node_id_to_idx[edge[1].strip()]\n",
    "            graph_id = nodes[node1]\n",
    "            assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])\n",
    "            \n",
    "            if graph_id not in adj_dict:\n",
    "                n = len(graphs[graph_id])\n",
    "                adj_dict[graph_id] = np.zeros((n, n))\n",
    "            ind1 = np.where(graphs[graph_id] == node1)[0]\n",
    "            ind2 = np.where(graphs[graph_id] == node2)[0]\n",
    "            assert len(ind1) == len(ind2) == 1, (ind1, ind2)\n",
    "            adj_dict[graph_id][ind1, ind2] = 1\n",
    "            adj_dict[graph_id][ind2, ind1] = 1\n",
    "        adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]        \n",
    "        return adj_list\n",
    "        \n",
    "    #graph_indicator\n",
    "    def read_graph_nodes_relations(self, fpath):\n",
    "        #node從0開始\n",
    "        #graph沒限定，但要是整數\n",
    "        graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))\n",
    "        nodes, graphs = {}, {}\n",
    "        for node_id, graph_id in enumerate(graph_ids):\n",
    "            if graph_id not in graphs:\n",
    "                graphs[graph_id] = []\n",
    "            graphs[graph_id].append(node_id)\n",
    "            nodes[node_id] = graph_id\n",
    "        graph_ids = np.unique(list(graphs.keys()))\n",
    "        for graph_id in graphs:\n",
    "            graphs[graph_id] = np.array(graphs[graph_id])\n",
    "        return nodes, graphs\n",
    "\n",
    "    def read_node_features(self, fpath, nodes, graphs):\n",
    "        def fn_read_node_features(s):\n",
    "            if ',' in s:\n",
    "                return list(map(float,(s.strip().split(',')[1:])))\n",
    "            else:\n",
    "                return list(map(float,(s.strip().split()[1:])))\n",
    "        node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn_read_node_features)\n",
    "        node_features = {}\n",
    "        #node_features:資料格式和graphs相似\n",
    "        for node_id, x in enumerate(node_features_all):\n",
    "            graph_id = nodes[node_id]\n",
    "            if graph_id not in node_features:\n",
    "                node_features[graph_id] = [ None ] * len(graphs[graph_id])\n",
    "            ind = np.where(graphs[graph_id] == node_id)[0]\n",
    "            #assert 判斷式, 如果有誤回傳的內容\n",
    "            assert len(ind) == 1, ind\n",
    "            assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]\n",
    "            node_features[graph_id][ind[0]] = x\n",
    "        node_features_lst = [np.array(node_features[graph_id]) for graph_id in sorted(list(graphs.keys()))]\n",
    "        return node_features_lst\n",
    "    \n",
    "    def read_node_ID(self, fpath):\n",
    "        def fn_read_node_ID(s):\n",
    "            if ',' in s:\n",
    "                return s.strip().split(',')[0]\n",
    "            else:\n",
    "                return s.strip().split()[0]\n",
    "        node_ID_all = self.parse_txt_file(fpath, line_parse_fn=fn_read_node_ID)\n",
    "        assert len(node_ID_all) == len(set(node_ID_all))\n",
    "        \n",
    "        node_id_to_idx = {}#str:int\n",
    "        node_idx_to_id = {}\n",
    "        for node_idx, node_id in enumerate(node_ID_all):\n",
    "            node_id_to_idx[node_id] = node_idx\n",
    "            node_idx_to_id[node_idx] = node_id\n",
    "        return node_id_to_idx, node_idx_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "N nodes avg/std/min/max: \t39.06/45.76/4/620\n",
      "N edges avg/std/min/max: \t72.82/84.60/5/1049\n",
      "Node degree avg/std/min/max: \t3.73/1.15/0/25\n",
      "Node features dim: \t\t3\n",
      "N classes: \t\t\t2\n",
      "Classes: \t\t\t1, 2\n",
      "Class 1: \t\t\t663 samples\n",
      "Class 2: \t\t\t450 samples\n",
      "\n",
      "FOLD 1\n",
      "TRAIN: 1002/1113\n",
      "TEST: 111/1113\n",
      "\n",
      "Initialize model\n",
      "GraphUnet(\n",
      "  (pre_gconv): ModuleList(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=3, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0, inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gconv): ModuleList(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0, inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0, inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0, inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (3): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0, inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout_gPool): Dropout(p=0.3, inplace=False)\n",
      "  (gUnpool_gcn): ModuleList(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0, inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0, inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0, inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "N trainable parameters: 424130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chienhua\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [64/1002 (6%)]\tLoss: 11.0175(avg: 11.0175)\tAcc: 64.06%(41/64) \tsec/iter: 2.4438\n",
      "Train Epoch: 0 [704/1002 (69%)]\tLoss: 7.0827(avg: 23.7571)\tAcc: 55.97%(394/704) \tsec/iter: 2.3953\n",
      "Train Epoch: 0 [1002/1002 (100%)]\tLoss: 4.2501(avg: 18.4117)\tAcc: 57.78%(579/1002) \tsec/iter: 2.2465\n",
      "Test set (epoch 1): Average loss: 4.5148, Accuracy: 69/111 (62.16%)\n",
      "\n",
      "Train Epoch: 1 [64/1002 (6%)]\tLoss: 6.2478(avg: 6.2478)\tAcc: 56.25%(36/64) \tsec/iter: 2.2856\n",
      "Train Epoch: 1 [704/1002 (69%)]\tLoss: 3.1804(avg: 5.9940)\tAcc: 59.66%(420/704) \tsec/iter: 2.1313\n",
      "Train Epoch: 1 [1002/1002 (100%)]\tLoss: 1.9729(avg: 5.3239)\tAcc: 60.58%(607/1002) \tsec/iter: 2.0953\n",
      "Test set (epoch 2): Average loss: 2.0628, Accuracy: 55/111 (49.55%)\n",
      "\n",
      "Train Epoch: 2 [64/1002 (6%)]\tLoss: 1.6565(avg: 1.6565)\tAcc: 64.06%(41/64) \tsec/iter: 2.2022\n",
      "Train Epoch: 2 [704/1002 (69%)]\tLoss: 1.6926(avg: 2.5801)\tAcc: 61.93%(436/704) \tsec/iter: 2.1494\n",
      "Train Epoch: 2 [1002/1002 (100%)]\tLoss: 2.0483(avg: 2.4130)\tAcc: 63.17%(633/1002) \tsec/iter: 2.1136\n",
      "Test set (epoch 3): Average loss: 1.9504, Accuracy: 48/111 (43.24%)\n",
      "\n",
      "Train Epoch: 3 [64/1002 (6%)]\tLoss: 2.7863(avg: 2.7863)\tAcc: 50.00%(32/64) \tsec/iter: 2.0957\n",
      "Train Epoch: 3 [704/1002 (69%)]\tLoss: 1.8669(avg: 1.5172)\tAcc: 63.35%(446/704) \tsec/iter: 2.1684\n",
      "Train Epoch: 3 [1002/1002 (100%)]\tLoss: 1.1239(avg: 1.3875)\tAcc: 63.67%(638/1002) \tsec/iter: 2.1402\n",
      "Test set (epoch 4): Average loss: 0.6945, Accuracy: 61/111 (54.95%)\n",
      "\n",
      "Train Epoch: 4 [64/1002 (6%)]\tLoss: 1.0691(avg: 1.0691)\tAcc: 64.06%(41/64) \tsec/iter: 2.0350\n",
      "Train Epoch: 4 [704/1002 (69%)]\tLoss: 0.6008(avg: 1.0833)\tAcc: 63.21%(445/704) \tsec/iter: 2.0369\n",
      "Train Epoch: 4 [1002/1002 (100%)]\tLoss: 0.5510(avg: 0.9925)\tAcc: 64.57%(647/1002) \tsec/iter: 2.0567\n",
      "Test set (epoch 5): Average loss: 0.6597, Accuracy: 69/111 (62.16%)\n",
      "\n",
      "Train Epoch: 5 [64/1002 (6%)]\tLoss: 0.7064(avg: 0.7064)\tAcc: 73.44%(47/64) \tsec/iter: 2.0796\n",
      "Train Epoch: 5 [704/1002 (69%)]\tLoss: 0.8115(avg: 0.7494)\tAcc: 66.62%(469/704) \tsec/iter: 2.1066\n",
      "Train Epoch: 5 [1002/1002 (100%)]\tLoss: 0.6118(avg: 0.7578)\tAcc: 63.87%(640/1002) \tsec/iter: 2.0876\n",
      "Test set (epoch 6): Average loss: 0.6842, Accuracy: 52/111 (46.85%)\n",
      "\n",
      "Train Epoch: 6 [64/1002 (6%)]\tLoss: 0.6861(avg: 0.6861)\tAcc: 71.88%(46/64) \tsec/iter: 2.1768\n",
      "Train Epoch: 6 [704/1002 (69%)]\tLoss: 0.7587(avg: 0.7280)\tAcc: 65.91%(464/704) \tsec/iter: 2.0992\n",
      "Train Epoch: 6 [1002/1002 (100%)]\tLoss: 0.6492(avg: 0.7150)\tAcc: 65.47%(656/1002) \tsec/iter: 2.0665\n",
      "Test set (epoch 7): Average loss: 0.6756, Accuracy: 62/111 (55.86%)\n",
      "\n",
      "Train Epoch: 7 [64/1002 (6%)]\tLoss: 0.7912(avg: 0.7912)\tAcc: 65.62%(42/64) \tsec/iter: 1.8825\n",
      "Train Epoch: 7 [704/1002 (69%)]\tLoss: 0.6293(avg: 0.6941)\tAcc: 63.35%(446/704) \tsec/iter: 2.0009\n",
      "Train Epoch: 7 [1002/1002 (100%)]\tLoss: 0.5906(avg: 0.6891)\tAcc: 62.77%(629/1002) \tsec/iter: 2.0242\n",
      "Test set (epoch 8): Average loss: 0.6344, Accuracy: 69/111 (62.16%)\n",
      "\n",
      "Train Epoch: 8 [64/1002 (6%)]\tLoss: 0.6491(avg: 0.6491)\tAcc: 67.19%(43/64) \tsec/iter: 2.3280\n",
      "Train Epoch: 8 [704/1002 (69%)]\tLoss: 0.6462(avg: 0.6578)\tAcc: 63.78%(449/704) \tsec/iter: 2.0856\n",
      "Train Epoch: 8 [1002/1002 (100%)]\tLoss: 0.7641(avg: 0.6620)\tAcc: 64.27%(644/1002) \tsec/iter: 2.0484\n",
      "Test set (epoch 9): Average loss: 0.9300, Accuracy: 46/111 (41.44%)\n",
      "\n",
      "Train Epoch: 9 [64/1002 (6%)]\tLoss: 0.9125(avg: 0.9125)\tAcc: 53.12%(34/64) \tsec/iter: 2.0931\n",
      "Train Epoch: 9 [704/1002 (69%)]\tLoss: 0.6812(avg: 0.6795)\tAcc: 63.78%(449/704) \tsec/iter: 2.1512\n",
      "Train Epoch: 9 [1002/1002 (100%)]\tLoss: 0.6209(avg: 0.6740)\tAcc: 63.77%(639/1002) \tsec/iter: 2.0797\n",
      "Test set (epoch 10): Average loss: 0.6236, Accuracy: 74/111 (66.67%)\n",
      "\n",
      "Train Epoch: 10 [64/1002 (6%)]\tLoss: 0.6289(avg: 0.6289)\tAcc: 71.88%(46/64) \tsec/iter: 2.0150\n",
      "Train Epoch: 10 [704/1002 (69%)]\tLoss: 0.8666(avg: 0.6367)\tAcc: 67.33%(474/704) \tsec/iter: 2.1581\n",
      "Train Epoch: 10 [1002/1002 (100%)]\tLoss: 0.6589(avg: 0.6381)\tAcc: 67.37%(675/1002) \tsec/iter: 2.1028\n",
      "Test set (epoch 11): Average loss: 0.6372, Accuracy: 75/111 (67.57%)\n",
      "\n",
      "Train Epoch: 11 [64/1002 (6%)]\tLoss: 0.6689(avg: 0.6689)\tAcc: 70.31%(45/64) \tsec/iter: 2.0581\n",
      "Train Epoch: 11 [704/1002 (69%)]\tLoss: 0.6074(avg: 0.6193)\tAcc: 66.62%(469/704) \tsec/iter: 2.1520\n",
      "Train Epoch: 11 [1002/1002 (100%)]\tLoss: 0.6794(avg: 0.6175)\tAcc: 65.97%(661/1002) \tsec/iter: 2.1077\n",
      "Test set (epoch 12): Average loss: 0.6698, Accuracy: 64/111 (57.66%)\n",
      "\n",
      "Train Epoch: 12 [64/1002 (6%)]\tLoss: 0.5730(avg: 0.5730)\tAcc: 70.31%(45/64) \tsec/iter: 2.2182\n",
      "Train Epoch: 12 [704/1002 (69%)]\tLoss: 0.7149(avg: 0.6402)\tAcc: 67.90%(478/704) \tsec/iter: 2.1380\n",
      "Train Epoch: 12 [1002/1002 (100%)]\tLoss: 0.6059(avg: 0.6384)\tAcc: 67.56%(677/1002) \tsec/iter: 2.1194\n",
      "Test set (epoch 13): Average loss: 0.6557, Accuracy: 68/111 (61.26%)\n",
      "\n",
      "Train Epoch: 13 [64/1002 (6%)]\tLoss: 0.6869(avg: 0.6869)\tAcc: 60.94%(39/64) \tsec/iter: 2.2124\n",
      "Train Epoch: 13 [704/1002 (69%)]\tLoss: 0.6851(avg: 0.6222)\tAcc: 66.48%(468/704) \tsec/iter: 2.0975\n",
      "Train Epoch: 13 [1002/1002 (100%)]\tLoss: 0.6309(avg: 0.6123)\tAcc: 68.46%(686/1002) \tsec/iter: 2.0761\n",
      "Test set (epoch 14): Average loss: 0.6210, Accuracy: 72/111 (64.86%)\n",
      "\n",
      "Train Epoch: 14 [64/1002 (6%)]\tLoss: 0.6721(avg: 0.6721)\tAcc: 62.50%(40/64) \tsec/iter: 2.0669\n",
      "Train Epoch: 14 [704/1002 (69%)]\tLoss: 0.6595(avg: 0.6594)\tAcc: 66.34%(467/704) \tsec/iter: 2.1350\n",
      "Train Epoch: 14 [1002/1002 (100%)]\tLoss: 0.4266(avg: 0.6460)\tAcc: 66.87%(670/1002) \tsec/iter: 2.1038\n",
      "Test set (epoch 15): Average loss: 0.6195, Accuracy: 75/111 (67.57%)\n",
      "\n",
      "Train Epoch: 15 [64/1002 (6%)]\tLoss: 0.7276(avg: 0.7276)\tAcc: 56.25%(36/64) \tsec/iter: 2.4276\n",
      "Train Epoch: 15 [704/1002 (69%)]\tLoss: 0.6208(avg: 0.6226)\tAcc: 68.04%(479/704) \tsec/iter: 2.0835\n",
      "Train Epoch: 15 [1002/1002 (100%)]\tLoss: 0.6629(avg: 0.6172)\tAcc: 69.06%(692/1002) \tsec/iter: 2.0647\n",
      "Test set (epoch 16): Average loss: 0.6182, Accuracy: 75/111 (67.57%)\n",
      "\n",
      "Train Epoch: 16 [64/1002 (6%)]\tLoss: 0.6581(avg: 0.6581)\tAcc: 68.75%(44/64) \tsec/iter: 1.8888\n",
      "Train Epoch: 16 [704/1002 (69%)]\tLoss: 0.5388(avg: 0.5824)\tAcc: 70.74%(498/704) \tsec/iter: 2.1176\n",
      "Train Epoch: 16 [1002/1002 (100%)]\tLoss: 0.5685(avg: 0.6046)\tAcc: 69.46%(696/1002) \tsec/iter: 2.0604\n",
      "Test set (epoch 17): Average loss: 0.6753, Accuracy: 63/111 (56.76%)\n",
      "\n",
      "Train Epoch: 17 [64/1002 (6%)]\tLoss: 0.6350(avg: 0.6350)\tAcc: 60.94%(39/64) \tsec/iter: 1.9774\n",
      "Train Epoch: 17 [704/1002 (69%)]\tLoss: 0.6422(avg: 0.6171)\tAcc: 65.77%(463/704) \tsec/iter: 2.0407\n",
      "Train Epoch: 17 [1002/1002 (100%)]\tLoss: 0.4637(avg: 0.6154)\tAcc: 66.07%(662/1002) \tsec/iter: 2.0359\n",
      "Test set (epoch 18): Average loss: 0.6181, Accuracy: 74/111 (66.67%)\n",
      "\n",
      "Train Epoch: 18 [64/1002 (6%)]\tLoss: 0.6166(avg: 0.6166)\tAcc: 76.56%(49/64) \tsec/iter: 1.8475\n",
      "Train Epoch: 18 [704/1002 (69%)]\tLoss: 0.5343(avg: 0.6025)\tAcc: 68.89%(485/704) \tsec/iter: 2.1233\n",
      "Train Epoch: 18 [1002/1002 (100%)]\tLoss: 0.6034(avg: 0.6140)\tAcc: 68.36%(685/1002) \tsec/iter: 2.0741\n",
      "Test set (epoch 19): Average loss: 0.6298, Accuracy: 72/111 (64.86%)\n",
      "\n",
      "Train Epoch: 19 [64/1002 (6%)]\tLoss: 0.6595(avg: 0.6595)\tAcc: 64.06%(41/64) \tsec/iter: 1.8912\n",
      "Train Epoch: 19 [704/1002 (69%)]\tLoss: 0.6629(avg: 0.5990)\tAcc: 69.18%(487/704) \tsec/iter: 2.0223\n",
      "Train Epoch: 19 [1002/1002 (100%)]\tLoss: 0.5730(avg: 0.5833)\tAcc: 69.16%(693/1002) \tsec/iter: 2.0116\n",
      "Test set (epoch 20): Average loss: 0.6194, Accuracy: 73/111 (65.77%)\n",
      "\n",
      "Train Epoch: 20 [64/1002 (6%)]\tLoss: 0.7080(avg: 0.7080)\tAcc: 59.38%(38/64) \tsec/iter: 2.3595\n",
      "Train Epoch: 20 [704/1002 (69%)]\tLoss: 0.6979(avg: 0.5789)\tAcc: 70.74%(498/704) \tsec/iter: 2.1194\n",
      "Train Epoch: 20 [1002/1002 (100%)]\tLoss: 0.5817(avg: 0.5779)\tAcc: 71.06%(712/1002) \tsec/iter: 2.0742\n",
      "Test set (epoch 21): Average loss: 0.6170, Accuracy: 72/111 (64.86%)\n",
      "\n",
      "Train Epoch: 21 [64/1002 (6%)]\tLoss: 0.6516(avg: 0.6516)\tAcc: 64.06%(41/64) \tsec/iter: 1.9851\n",
      "Train Epoch: 21 [704/1002 (69%)]\tLoss: 0.6484(avg: 0.5887)\tAcc: 70.31%(495/704) \tsec/iter: 2.1152\n",
      "Train Epoch: 21 [1002/1002 (100%)]\tLoss: 0.6531(avg: 0.5873)\tAcc: 69.76%(699/1002) \tsec/iter: 2.0795\n",
      "Test set (epoch 22): Average loss: 0.6221, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 22 [64/1002 (6%)]\tLoss: 0.5233(avg: 0.5233)\tAcc: 75.00%(48/64) \tsec/iter: 2.1228\n",
      "Train Epoch: 22 [704/1002 (69%)]\tLoss: 0.6830(avg: 0.6006)\tAcc: 69.03%(486/704) \tsec/iter: 2.1197\n",
      "Train Epoch: 22 [1002/1002 (100%)]\tLoss: 0.6158(avg: 0.5907)\tAcc: 69.66%(698/1002) \tsec/iter: 2.0941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set (epoch 23): Average loss: 0.6155, Accuracy: 73/111 (65.77%)\n",
      "\n",
      "Train Epoch: 23 [64/1002 (6%)]\tLoss: 0.6488(avg: 0.6488)\tAcc: 62.50%(40/64) \tsec/iter: 2.2908\n",
      "Train Epoch: 23 [704/1002 (69%)]\tLoss: 0.6490(avg: 0.5809)\tAcc: 72.44%(510/704) \tsec/iter: 2.1969\n",
      "Train Epoch: 23 [1002/1002 (100%)]\tLoss: 0.5561(avg: 0.5818)\tAcc: 72.06%(722/1002) \tsec/iter: 2.1286\n",
      "Test set (epoch 24): Average loss: 0.6206, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 24 [64/1002 (6%)]\tLoss: 0.5375(avg: 0.5375)\tAcc: 70.31%(45/64) \tsec/iter: 1.9123\n",
      "Train Epoch: 24 [704/1002 (69%)]\tLoss: 0.6630(avg: 0.5661)\tAcc: 70.60%(497/704) \tsec/iter: 2.0560\n",
      "Train Epoch: 24 [1002/1002 (100%)]\tLoss: 0.5607(avg: 0.5733)\tAcc: 70.86%(710/1002) \tsec/iter: 2.0475\n",
      "Test set (epoch 25): Average loss: 0.6251, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 25 [64/1002 (6%)]\tLoss: 0.4817(avg: 0.4817)\tAcc: 71.88%(46/64) \tsec/iter: 2.1616\n",
      "Train Epoch: 25 [704/1002 (69%)]\tLoss: 0.6241(avg: 0.5734)\tAcc: 71.59%(504/704) \tsec/iter: 2.1515\n",
      "Train Epoch: 25 [1002/1002 (100%)]\tLoss: 0.5529(avg: 0.5794)\tAcc: 70.36%(705/1002) \tsec/iter: 2.1704\n",
      "Test set (epoch 26): Average loss: 0.6221, Accuracy: 70/111 (63.06%)\n",
      "\n",
      "Train Epoch: 26 [64/1002 (6%)]\tLoss: 0.6572(avg: 0.6572)\tAcc: 59.38%(38/64) \tsec/iter: 2.0383\n",
      "Train Epoch: 26 [704/1002 (69%)]\tLoss: 0.5425(avg: 0.5729)\tAcc: 70.17%(494/704) \tsec/iter: 2.0855\n",
      "Train Epoch: 26 [1002/1002 (100%)]\tLoss: 0.5536(avg: 0.5823)\tAcc: 70.56%(707/1002) \tsec/iter: 2.0707\n",
      "Test set (epoch 27): Average loss: 0.6225, Accuracy: 70/111 (63.06%)\n",
      "\n",
      "Train Epoch: 27 [64/1002 (6%)]\tLoss: 0.5426(avg: 0.5426)\tAcc: 78.12%(50/64) \tsec/iter: 2.2622\n",
      "Train Epoch: 27 [704/1002 (69%)]\tLoss: 0.5522(avg: 0.5755)\tAcc: 72.73%(512/704) \tsec/iter: 2.1889\n",
      "Train Epoch: 27 [1002/1002 (100%)]\tLoss: 0.5011(avg: 0.5747)\tAcc: 72.46%(726/1002) \tsec/iter: 2.1357\n",
      "Test set (epoch 28): Average loss: 0.6169, Accuracy: 72/111 (64.86%)\n",
      "\n",
      "Train Epoch: 28 [64/1002 (6%)]\tLoss: 0.5740(avg: 0.5740)\tAcc: 70.31%(45/64) \tsec/iter: 2.1619\n",
      "Train Epoch: 28 [704/1002 (69%)]\tLoss: 0.6195(avg: 0.5822)\tAcc: 70.31%(495/704) \tsec/iter: 2.1594\n",
      "Train Epoch: 28 [1002/1002 (100%)]\tLoss: 0.5209(avg: 0.5900)\tAcc: 69.66%(698/1002) \tsec/iter: 2.1009\n",
      "Test set (epoch 29): Average loss: 0.6175, Accuracy: 72/111 (64.86%)\n",
      "\n",
      "Train Epoch: 29 [64/1002 (6%)]\tLoss: 0.6948(avg: 0.6948)\tAcc: 59.38%(38/64) \tsec/iter: 2.1645\n",
      "Train Epoch: 29 [704/1002 (69%)]\tLoss: 0.5830(avg: 0.5932)\tAcc: 70.03%(493/704) \tsec/iter: 2.1697\n",
      "Train Epoch: 29 [1002/1002 (100%)]\tLoss: 0.5129(avg: 0.5767)\tAcc: 71.36%(715/1002) \tsec/iter: 2.1478\n",
      "Test set (epoch 30): Average loss: 0.6177, Accuracy: 72/111 (64.86%)\n",
      "\n",
      "Train Epoch: 30 [64/1002 (6%)]\tLoss: 0.6957(avg: 0.6957)\tAcc: 64.06%(41/64) \tsec/iter: 2.1839\n",
      "Train Epoch: 30 [704/1002 (69%)]\tLoss: 0.6914(avg: 0.5944)\tAcc: 69.89%(492/704) \tsec/iter: 2.1757\n",
      "Train Epoch: 30 [1002/1002 (100%)]\tLoss: 0.7517(avg: 0.5918)\tAcc: 69.96%(701/1002) \tsec/iter: 2.1220\n",
      "Test set (epoch 31): Average loss: 0.6179, Accuracy: 72/111 (64.86%)\n",
      "\n",
      "Train Epoch: 31 [64/1002 (6%)]\tLoss: 0.6460(avg: 0.6460)\tAcc: 67.19%(43/64) \tsec/iter: 2.1910\n",
      "Train Epoch: 31 [704/1002 (69%)]\tLoss: 0.6817(avg: 0.5790)\tAcc: 71.73%(505/704) \tsec/iter: 2.1438\n",
      "Train Epoch: 31 [1002/1002 (100%)]\tLoss: 0.6411(avg: 0.5848)\tAcc: 71.06%(712/1002) \tsec/iter: 2.0926\n",
      "Test set (epoch 32): Average loss: 0.6187, Accuracy: 72/111 (64.86%)\n",
      "\n",
      "Train Epoch: 32 [64/1002 (6%)]\tLoss: 0.5601(avg: 0.5601)\tAcc: 70.31%(45/64) \tsec/iter: 2.0532\n",
      "Train Epoch: 32 [704/1002 (69%)]\tLoss: 0.5664(avg: 0.5811)\tAcc: 69.03%(486/704) \tsec/iter: 2.0622\n",
      "Train Epoch: 32 [1002/1002 (100%)]\tLoss: 0.6276(avg: 0.5861)\tAcc: 69.46%(696/1002) \tsec/iter: 2.0792\n",
      "Test set (epoch 33): Average loss: 0.6194, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 33 [64/1002 (6%)]\tLoss: 0.6283(avg: 0.6283)\tAcc: 68.75%(44/64) \tsec/iter: 1.9132\n",
      "Train Epoch: 33 [704/1002 (69%)]\tLoss: 0.4833(avg: 0.5783)\tAcc: 69.89%(492/704) \tsec/iter: 2.1209\n",
      "Train Epoch: 33 [1002/1002 (100%)]\tLoss: 0.6158(avg: 0.5755)\tAcc: 70.16%(703/1002) \tsec/iter: 2.0639\n",
      "Test set (epoch 34): Average loss: 0.6190, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 34 [64/1002 (6%)]\tLoss: 0.5705(avg: 0.5705)\tAcc: 76.56%(49/64) \tsec/iter: 2.1904\n",
      "Train Epoch: 34 [704/1002 (69%)]\tLoss: 0.5442(avg: 0.5628)\tAcc: 71.73%(505/704) \tsec/iter: 2.0692\n",
      "Train Epoch: 34 [1002/1002 (100%)]\tLoss: 0.6664(avg: 0.5669)\tAcc: 70.66%(708/1002) \tsec/iter: 2.0516\n",
      "Test set (epoch 35): Average loss: 0.6190, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 35 [64/1002 (6%)]\tLoss: 0.6365(avg: 0.6365)\tAcc: 70.31%(45/64) \tsec/iter: 1.9360\n",
      "Train Epoch: 35 [704/1002 (69%)]\tLoss: 0.5843(avg: 0.5845)\tAcc: 70.45%(496/704) \tsec/iter: 2.1161\n",
      "Train Epoch: 35 [1002/1002 (100%)]\tLoss: 0.6203(avg: 0.5822)\tAcc: 69.96%(701/1002) \tsec/iter: 2.0797\n",
      "Test set (epoch 36): Average loss: 0.6195, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 36 [64/1002 (6%)]\tLoss: 0.6717(avg: 0.6717)\tAcc: 65.62%(42/64) \tsec/iter: 2.2372\n",
      "Train Epoch: 36 [704/1002 (69%)]\tLoss: 0.6405(avg: 0.5974)\tAcc: 69.03%(486/704) \tsec/iter: 2.0846\n",
      "Train Epoch: 36 [1002/1002 (100%)]\tLoss: 0.5975(avg: 0.5935)\tAcc: 69.36%(695/1002) \tsec/iter: 2.0671\n",
      "Test set (epoch 37): Average loss: 0.6196, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 37 [64/1002 (6%)]\tLoss: 0.6048(avg: 0.6048)\tAcc: 64.06%(41/64) \tsec/iter: 2.0918\n",
      "Train Epoch: 37 [704/1002 (69%)]\tLoss: 0.5760(avg: 0.5751)\tAcc: 71.16%(501/704) \tsec/iter: 2.1021\n",
      "Train Epoch: 37 [1002/1002 (100%)]\tLoss: 0.6115(avg: 0.5777)\tAcc: 70.56%(707/1002) \tsec/iter: 2.1024\n",
      "Test set (epoch 38): Average loss: 0.6197, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 38 [64/1002 (6%)]\tLoss: 0.5474(avg: 0.5474)\tAcc: 71.88%(46/64) \tsec/iter: 1.9274\n",
      "Train Epoch: 38 [704/1002 (69%)]\tLoss: 0.6898(avg: 0.5971)\tAcc: 69.60%(490/704) \tsec/iter: 2.0597\n",
      "Train Epoch: 38 [1002/1002 (100%)]\tLoss: 0.7016(avg: 0.5942)\tAcc: 69.46%(696/1002) \tsec/iter: 2.0638\n",
      "Test set (epoch 39): Average loss: 0.6200, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 39 [64/1002 (6%)]\tLoss: 0.5706(avg: 0.5706)\tAcc: 68.75%(44/64) \tsec/iter: 2.0308\n",
      "Train Epoch: 39 [704/1002 (69%)]\tLoss: 0.7190(avg: 0.5740)\tAcc: 70.03%(493/704) \tsec/iter: 2.0819\n",
      "Train Epoch: 39 [1002/1002 (100%)]\tLoss: 0.4971(avg: 0.5809)\tAcc: 69.46%(696/1002) \tsec/iter: 2.0807\n",
      "Test set (epoch 40): Average loss: 0.6199, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 40 [64/1002 (6%)]\tLoss: 0.6364(avg: 0.6364)\tAcc: 62.50%(40/64) \tsec/iter: 2.2224\n",
      "Train Epoch: 40 [704/1002 (69%)]\tLoss: 0.5409(avg: 0.5525)\tAcc: 72.87%(513/704) \tsec/iter: 2.1654\n",
      "Train Epoch: 40 [1002/1002 (100%)]\tLoss: 0.5638(avg: 0.5745)\tAcc: 71.16%(713/1002) \tsec/iter: 2.1122\n",
      "Test set (epoch 41): Average loss: 0.6199, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 41 [64/1002 (6%)]\tLoss: 0.5149(avg: 0.5149)\tAcc: 67.19%(43/64) \tsec/iter: 1.9322\n",
      "Train Epoch: 41 [704/1002 (69%)]\tLoss: 0.5867(avg: 0.5889)\tAcc: 67.19%(473/704) \tsec/iter: 2.1036\n",
      "Train Epoch: 41 [1002/1002 (100%)]\tLoss: 0.5276(avg: 0.5828)\tAcc: 68.76%(689/1002) \tsec/iter: 2.0533\n",
      "Test set (epoch 42): Average loss: 0.6201, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 42 [64/1002 (6%)]\tLoss: 0.4668(avg: 0.4668)\tAcc: 75.00%(48/64) \tsec/iter: 2.1220\n",
      "Train Epoch: 42 [704/1002 (69%)]\tLoss: 0.5502(avg: 0.5801)\tAcc: 69.89%(492/704) \tsec/iter: 2.1746\n",
      "Train Epoch: 42 [1002/1002 (100%)]\tLoss: 0.6498(avg: 0.5839)\tAcc: 70.16%(703/1002) \tsec/iter: 2.1398\n",
      "Test set (epoch 43): Average loss: 0.6199, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 43 [64/1002 (6%)]\tLoss: 0.6027(avg: 0.6027)\tAcc: 67.19%(43/64) \tsec/iter: 2.1987\n",
      "Train Epoch: 43 [704/1002 (69%)]\tLoss: 0.5165(avg: 0.5443)\tAcc: 69.89%(492/704) \tsec/iter: 2.0886\n",
      "Train Epoch: 43 [1002/1002 (100%)]\tLoss: 0.6105(avg: 0.5602)\tAcc: 69.96%(701/1002) \tsec/iter: 2.0539\n",
      "Test set (epoch 44): Average loss: 0.6201, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 44 [64/1002 (6%)]\tLoss: 0.5546(avg: 0.5546)\tAcc: 67.19%(43/64) \tsec/iter: 1.8850\n",
      "Train Epoch: 44 [704/1002 (69%)]\tLoss: 0.4225(avg: 0.5639)\tAcc: 70.60%(497/704) \tsec/iter: 2.1338\n",
      "Train Epoch: 44 [1002/1002 (100%)]\tLoss: 0.5776(avg: 0.5713)\tAcc: 69.86%(700/1002) \tsec/iter: 2.0741\n",
      "Test set (epoch 45): Average loss: 0.6205, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 45 [64/1002 (6%)]\tLoss: 0.4790(avg: 0.4790)\tAcc: 76.56%(49/64) \tsec/iter: 2.0373\n",
      "Train Epoch: 45 [704/1002 (69%)]\tLoss: 0.5764(avg: 0.5585)\tAcc: 72.30%(509/704) \tsec/iter: 2.0673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 45 [1002/1002 (100%)]\tLoss: 0.5849(avg: 0.5693)\tAcc: 70.96%(711/1002) \tsec/iter: 2.0177\n",
      "Test set (epoch 46): Average loss: 0.6197, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 46 [64/1002 (6%)]\tLoss: 0.5969(avg: 0.5969)\tAcc: 70.31%(45/64) \tsec/iter: 1.9203\n",
      "Train Epoch: 46 [704/1002 (69%)]\tLoss: 0.6887(avg: 0.5740)\tAcc: 71.31%(502/704) \tsec/iter: 2.0970\n",
      "Train Epoch: 46 [1002/1002 (100%)]\tLoss: 0.5524(avg: 0.5732)\tAcc: 70.76%(709/1002) \tsec/iter: 2.0483\n",
      "Test set (epoch 47): Average loss: 0.6193, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 47 [64/1002 (6%)]\tLoss: 0.5786(avg: 0.5786)\tAcc: 76.56%(49/64) \tsec/iter: 2.4263\n",
      "Train Epoch: 47 [704/1002 (69%)]\tLoss: 0.5779(avg: 0.5783)\tAcc: 70.45%(496/704) \tsec/iter: 2.1451\n",
      "Train Epoch: 47 [1002/1002 (100%)]\tLoss: 0.6302(avg: 0.5669)\tAcc: 71.36%(715/1002) \tsec/iter: 2.1238\n",
      "Test set (epoch 48): Average loss: 0.6193, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 48 [64/1002 (6%)]\tLoss: 0.6579(avg: 0.6579)\tAcc: 68.75%(44/64) \tsec/iter: 2.1381\n",
      "Train Epoch: 48 [704/1002 (69%)]\tLoss: 0.6140(avg: 0.5698)\tAcc: 71.16%(501/704) \tsec/iter: 2.1723\n",
      "Train Epoch: 48 [1002/1002 (100%)]\tLoss: 0.6444(avg: 0.5684)\tAcc: 71.46%(716/1002) \tsec/iter: 2.0930\n",
      "Test set (epoch 49): Average loss: 0.6189, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "Train Epoch: 49 [64/1002 (6%)]\tLoss: 0.5559(avg: 0.5559)\tAcc: 78.12%(50/64) \tsec/iter: 1.9462\n",
      "Train Epoch: 49 [704/1002 (69%)]\tLoss: 0.6479(avg: 0.5824)\tAcc: 70.45%(496/704) \tsec/iter: 2.0556\n",
      "Train Epoch: 49 [1002/1002 (100%)]\tLoss: 0.5347(avg: 0.5767)\tAcc: 70.66%(708/1002) \tsec/iter: 2.0417\n",
      "Test set (epoch 50): Average loss: 0.6190, Accuracy: 71/111 (63.96%)\n",
      "\n",
      "[Trained Model]\n",
      "Training Set: Accuracy=73.25%(734/1002)\n",
      "Testing Set: Accuracy=63.96%(71/111)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Loading data')\n",
    "\n",
    "datareader = DataReader(data_dir=dataset, \n",
    "                        rnd_state=seed,training_size_p=training_size_p,folds=n_folds,balance=balance)\n",
    "\n",
    "train_acc_folds = []\n",
    "test_acc_folds = []\n",
    "for fold_id in range(n_folds):\n",
    "    print('\\nFOLD', fold_id+1)\n",
    "    loaders = []\n",
    "    for split in ['train', 'test']:\n",
    "        #製作\"train\"或\"test\" graph data\n",
    "        gdata = GraphData(fold_id=fold_id, datareader=datareader, split=split)\n",
    "        loader = torch.utils.data.DataLoader(gdata, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=split.find('train') >= 0,\n",
    "                                             num_workers=threads)\n",
    "        loaders.append(loader)\n",
    "        if split == 'train':\n",
    "            training_size = len(gdata.idx)\n",
    "    if model_name == 'GCN':\n",
    "        model = GCN(in_features=loaders[0].dataset.features_dim, \n",
    "                    out_features=loaders[0].dataset.n_classes,\n",
    "                    filters_gcn=filters_gcn,\n",
    "                    gcn_bias=gcn_bias,\n",
    "                    gcn_activation=gcn_activation,\n",
    "                    dropout_gcn=dropout_gcn,\n",
    "                    fc_bias=fc_bias,\n",
    "                    n_hidden_fc=n_hidden_fc,\n",
    "                    dropout_fc=dropout_fc).to(device)    \n",
    "\n",
    "    elif model_name == 'Graph-U-Net':\n",
    "        model = GraphUnet(in_features=loaders[0].dataset.features_dim,\n",
    "                          out_features=loaders[0].dataset.n_classes,\n",
    "                          filters_gcn=filters_gcn,\n",
    "                          gcn_bias=gcn_bias,\n",
    "                          gcn_activation=gcn_activation,\n",
    "                          dropout_gcn=dropout_gcn,\n",
    "                          pooling_ratios=pooling_ratios,\n",
    "                          n_hidden_fc=n_hidden_fc,\n",
    "                          fc_bias=fc_bias,\n",
    "                          fc_activation=fc_activation,\n",
    "                          dropout_fc=dropout_fc,\n",
    "                          dropout_gPool=dropout_gPool).to(device)    \n",
    "    print('\\nInitialize model')\n",
    "    print(model)\n",
    "    c = 0\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        c += p.numel()\n",
    "    print('N trainable parameters:', c)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=lr,\n",
    "                weight_decay=wdecay,\n",
    "                betas=(0.5, 0.999))\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.1)\n",
    "\n",
    "\n",
    "    def train(train_loader):\n",
    "        scheduler.step()#每個batch就會改變學習率\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        train_loss, correct, n_samples = 0, 0, 0\n",
    "        train_loss_batch_ls = []\n",
    "        train_acc_batch_ls = []\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)     \n",
    "            loss = loss_fn(output, data[4])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            time_iter = time.time() - start\n",
    "            train_loss += loss.item() * len(output)\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "            acc = 100. * correct / n_samples\n",
    "            train_loss_batch_ls.append(train_loss/n_samples)\n",
    "            train_acc_batch_ls.append(acc/100)\n",
    "            if batch_idx % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}(avg: {:.4f})\\tAcc: {:.2f}%({}/{}) \\tsec/iter: {:.4f}'.format(\n",
    "                    epoch, n_samples, len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, \n",
    "                    acc, correct, n_samples, time_iter / (batch_idx + 1) ))    \n",
    "        return train_loss_batch_ls, train_acc_batch_ls\n",
    "    def test(test_loader):\n",
    "        model.eval()\n",
    "        start = time.time()\n",
    "        test_loss, correct, n_samples = 0, 0, 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4], reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "        time_iter = time.time() - start\n",
    "        test_loss /= n_samples\n",
    "        acc = 100. * correct / n_samples\n",
    "        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch+1, \n",
    "                                                                                              test_loss, \n",
    "                                                                                              correct, \n",
    "                                                                                              n_samples, acc))\n",
    "        return test_loss,acc/100\n",
    "\n",
    "    def predict(loader_full):\n",
    "        idx_ls = []\n",
    "        pred_ls = [] \n",
    "        label_ls = []\n",
    "        length_ls = []\n",
    "        print('[Trained Model]')\n",
    "        for i in [0,1]:\n",
    "            model.eval()     \n",
    "            pred_tmp = []\n",
    "            label_tmp = []\n",
    "            for batch_idx, data in enumerate(loader_full[i]):\n",
    "                for j in range(len(data)):\n",
    "                    data[j] = data[j].to(device)\n",
    "                output = model(data)\n",
    "                idx_ls.extend(data[5].tolist())\n",
    "                pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "                pred_ls.extend(pred.reshape(pred.shape[0]).tolist())\n",
    "                label_ls.extend(data[4].tolist())\n",
    "\n",
    "                pred_tmp.extend(pred.reshape(pred.shape[0]).tolist())\n",
    "                label_tmp.extend(data[4].tolist())\n",
    "            total = len(pred_tmp)\n",
    "            c = sum(np.array(pred_tmp)==np.array(label_tmp)) \n",
    "            if i==0:\n",
    "                print('Training Set: Accuracy=%.2f%%(%s/%s)'%(c*100/total,c,total))\n",
    "                train_acc_folds.append(c*100/total)\n",
    "            elif i==1:\n",
    "                print('Testing Set: Accuracy=%.2f%%(%s/%s)'%(c*100/total,c,total))  \n",
    "                test_acc_folds.append(c*100/total)\n",
    "            length_ls.append(total)\n",
    "        return idx_ls, pred_ls, label_ls, length_ls\n",
    "\n",
    "    train_loss_ls = []\n",
    "    train_acc_ls = []\n",
    "    test_loss_ls = []\n",
    "    test_acc_ls = []\n",
    "    loss_fn = F.nll_loss\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(loaders[0])\n",
    "        test_loss, test_acc = test(loaders[1])\n",
    "        train_loss_ls.extend(train_loss)\n",
    "        train_acc_ls.extend(train_acc)\n",
    "        test_loss_ls.append(test_loss)\n",
    "        test_acc_ls.append(test_acc)   \n",
    "\n",
    "    idx_ls, pred_ls, label_ls, length_ls = predict(loaders)\n",
    "    \n",
    "    #plot\n",
    "    length_train = range(len(train_loss_ls))\n",
    "    length_test = range(int(np.ceil(training_size/batch_size))-1,len(train_loss_ls),int(np.ceil(training_size/batch_size)))\n",
    "    plt.plot(length_train,train_acc_ls,label='training accuracy')\n",
    "    plt.plot(length_test,test_acc_ls,label='validation accuracy')\n",
    "    x_ticks = [0]+list(length_test)\n",
    "    plt.xticks(x_ticks,list(range(0,epochs+1)))\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('accuracy',fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_folder+'/acc_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(length_train,train_loss_ls,label='training loss')\n",
    "    plt.plot(length_test,test_loss_ls,label='validation loss')\n",
    "    plt.xticks(x_ticks,list(range(0,epochs+1)))\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('loss',fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_folder+'/loss_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()    \n",
    "    \n",
    "    #xlsx\n",
    "    writer = pd.ExcelWriter(output_folder+'/results_fold%s.xlsx'%(fold_id+1), engine = 'xlsxwriter')\n",
    "    ################################################\n",
    "    train_loss_acc_df = pd.DataFrame(zip(range(1,epochs+1),np.array(train_loss_ls)[length_test],np.array(train_acc_ls)[length_test]))\n",
    "    train_loss_acc_df.columns = ['epoch','training loss','training accuracy']\n",
    "    train_loss_acc_df.to_excel(writer,sheet_name='training',header=True,index=False) \n",
    "    test_loss_acc_df = pd.DataFrame(zip(range(1,epochs+1),test_loss_ls,test_acc_ls))\n",
    "    test_loss_acc_df.columns = ['epoch','testing loss','testing accuracy']\n",
    "    test_loss_acc_df.to_excel(writer,sheet_name='testing',header=True,index=False) \n",
    "    ################################################\n",
    "    for i in range(len(model.gconv)):\n",
    "        f_GCN_weight = pd.DataFrame(np.matrix(model.gconv[i].fc.weight.tolist()).T)\n",
    "        f_GCN_weight.to_excel(writer,sheet_name='GCN%s_weight'%i,header=False,index=False)\n",
    "        if gcn_bias:\n",
    "            f_GCN_bias = pd.DataFrame(np.matrix(model.gconv[i].fc.bias.tolist()).T)\n",
    "            f_GCN_bias.to_excel(writer,sheet_name='GCN%s_bias'%i,header=False,index=False)    \n",
    "        if model_name == 'Graph-U-Net':\n",
    "            if i < len(model.proj):\n",
    "                f_gPool_weight = pd.DataFrame(pd.DataFrame(np.matrix(model.proj[i].tolist())))\n",
    "                f_gPool_weight.to_excel(writer,sheet_name='gPool%s_weight'%i,header=False,index=False)    \n",
    "    if gUnpool != 'None':\n",
    "        for i in range(len(model.gUnpool_gcn)):\n",
    "            f_gUnpool_weight = pd.DataFrame(np.matrix(model.gUnpool_gcn[i].fc.weight.tolist()).T) \n",
    "            f_gUnpool_weight.to_excel(writer,sheet_name='gUnpool_GCN%s_weight'%i,header=False,index=False)\n",
    "            if gcn_bias:\n",
    "                f_gUnpool_bias = pd.DataFrame(np.matrix(model.gUnpool_gcn[i].fc.bias.tolist()).T)\n",
    "                f_gUnpool_bias.to_excel(writer,sheet_name='gUnpool_GCN%s_bias'%i,header=False,index=False)            \n",
    "    fc_layer = 0\n",
    "    for i in range(len(model.fc)):\n",
    "        if i % 3 == 1:\n",
    "            f_fc_weight = pd.DataFrame(np.matrix(model.fc[i].weight.tolist()).T)\n",
    "            f_fc_weight.to_excel(writer,sheet_name='FC%s_weight'%fc_layer,header=False,index=False)    \n",
    "            if fc_bias:\n",
    "                f_fc_bias = pd.DataFrame(np.matrix(model.fc[i].bias.tolist()).T)\n",
    "                f_fc_bias.to_excel(writer,sheet_name='FC%s_bias'%fc_layer,header=False,index=False)\n",
    "            fc_layer += 1\n",
    "    #################################################################\n",
    "    label_target_ls = [loaders[0].dataset.label_to_target[i] for i in label_ls]\n",
    "    pred_target_ls = [loaders[0].dataset.label_to_target[i] for i in pred_ls]\n",
    "    id_ls = [loaders[0].dataset.node_idx_to_id[i] for i in idx_ls]\n",
    "    splits = ['training']*length_ls[0]+['testing']*length_ls[1]\n",
    "    prediction = pd.DataFrame(zip(idx_ls,id_ls,pred_target_ls,label_target_ls,splits))\n",
    "    prediction.columns = ['IDX','ID','prediction','label','splits']\n",
    "    prediction = prediction.sort_values(by=['IDX'])\n",
    "    prediction = prediction.loc[:,['ID','prediction','label','splits']]\n",
    "    prediction.to_excel(writer,sheet_name='Prediction',header=True,index=False)  \n",
    "    #######################################\n",
    "    writer.save()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "summary.append('traing set:')\n",
    "for i in range(len(train_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,train_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(train_acc_folds),np.std(train_acc_folds)))\n",
    "summary.append('testing set:')\n",
    "for i in range(len(test_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,test_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(test_acc_folds),np.std(test_acc_folds)))\n",
    "f1 = open(output_folder+'/summary_acc.txt','w')\n",
    "f1.write('\\n'.join(summary))\n",
    "f1.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = 'A'\n",
    "identity = 'I'\n",
    "\n",
    "if graph_kernel == 'Normalized-Laplacian':\n",
    "    kernel_description = 'tilde(D)^(-1/2)tilde(A)tilde(D)^(-1/2)'\n",
    "elif graph_kernel == 'Normalization':\n",
    "    kernel_description = 'tilde(D)^(-1)tilde(A)'\n",
    "model_structure_full = []\n",
    "\n",
    "for j in range(len(model.gconv)):\n",
    "    model_structure = str(model.gconv[j]).split('\\n')\n",
    "    for i in range(len(model_structure)):\n",
    "        if i != len(model_structure)-1:\n",
    "            model_structure_full.append(model_structure[i])\n",
    "            \n",
    "    model_structure_full.append('  (adjacency matrix): tilde(A) = %s+%s'%(adjacency_matrix,identity))\n",
    "    model_structure_full.append('  (graph kernel): '+kernel_description)\n",
    "    model_structure_full.append(')')\n",
    "    if (j != len(filters_gcn)-1) & (model_name=='Graph-U-Net'):\n",
    "        model_structure_full.append('gPool(')               \n",
    "        model_structure_full.append('  (top k): %s'%pooling_ratios[j])\n",
    "        model_structure_full.append('  (activation of top k): %s'%topk_activation)\n",
    "        model_structure_full.append(')')\n",
    "\n",
    "        \n",
    "if gUnpool != 'None':\n",
    "    for i in range(len(model.gUnpool_gcn)):\n",
    "        model_structure_full.append('gUnpool()')\n",
    "        model_structure = str(model.gUnpool_gcn[i]).split('\\n')\n",
    "        for j in range(len(model_structure)):\n",
    "            if j != len(model_structure)-1:\n",
    "                model_structure_full.append(model_structure[j])\n",
    "        model_structure_full.append('  (adjacency matrix): tilde(A) = %s+%s'%(adjacency_matrix,identity))\n",
    "        model_structure_full.append('  (graph kernel): '+kernel_description)    \n",
    "        if 'residual_connection' in gUnpool:\n",
    "            model_structure_full.append('  (residual connection): True')    \n",
    "        else:\n",
    "            model_structure_full.append('  (residual connection): False')    \n",
    "        model_structure_full.append(')')\n",
    "\n",
    "\n",
    "if gUnpool != 'None':\n",
    "    if pooing_by_layer_concatenation == True:\n",
    "        layer_info = ','.join(list(map(str,range(len(gUnpool_gcn)))))\n",
    "    else:\n",
    "        layer_info = len(gUnpool_gcn)-1\n",
    "    pooling_description = 'concatenate(%s pooling) over nodes by gUnpool layer%s'%(pooling_method,layer_info)\n",
    "elif gUnpool == 'None':\n",
    "    if pooing_by_layer_concatenation == True:\n",
    "        layer_info = ','.join(list(map(str,range(len(filters_gcn)))))\n",
    "    else:\n",
    "        layer_info = len(filters_gcn)-1\n",
    "    pooling_description = 'concatenate(%s pooling) over nodes by gPool layer%s'%(pooling_method,layer_info)\n",
    "\n",
    "    \n",
    "model_structure_full.append(' '*int(len(pooling_description)/2) +'|||')\n",
    "model_structure_full.append(' '*int(len(pooling_description)/2) +'vvv')\n",
    "model_structure_full.append(pooling_description)  \n",
    "model_structure_full.append(' '*int(len(pooling_description)/2) +'|||')\n",
    "model_structure_full.append(' '*int(len(pooling_description)/2) +'vvv')\n",
    "\n",
    "\n",
    "fully_connected = []\n",
    "for i in range(len(model.fc)):\n",
    "    if i%3==1:\n",
    "        fully_connected.append('FullyConnected(')\n",
    "        fully_connected.append('  (fc): %s'%model.fc[i])\n",
    "        fully_connected.append('  (dropout): %s'%model.fc[i-1])\n",
    "        if i != len(model.fc)-1:\n",
    "            fully_connected.append('  (activation): %s'%model.fc[i+1])\n",
    "        else:\n",
    "            fully_connected.append('  (activation): softmax')\n",
    "        fully_connected.append(')')\n",
    "model_structure_full.extend(fully_connected)\n",
    "\n",
    "\n",
    "f1 = open(output_folder+'/model.txt','w')\n",
    "f1.write('\\n'.join(model_structure_full))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cora.t import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |x|^n + |y|^n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chienhua\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in power\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\chienhua\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in power\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "x = np.arange(-1,1+0.001,0.001)\n",
    "x_n = abs(x)**n\n",
    "y_n_pos = (1-x_n)**(1/n)\n",
    "y_n_neg = -(1-x_n)**(1/n)\n",
    "plt.plot(x,y_n_neg)\n",
    "plt.plot(x,y_n_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n_neg[-1] = 0\n",
    "y_n_pos[-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e9a1356908>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,y_n_neg)\n",
    "plt.plot(x,y_n_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
