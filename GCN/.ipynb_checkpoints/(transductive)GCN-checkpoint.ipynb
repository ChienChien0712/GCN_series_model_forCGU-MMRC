{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Cora'\n",
    "n_folds = 1\n",
    "training_size = 0.8\n",
    "epochs = 300\n",
    "lr = 0.01\n",
    "wdecay = 5e-4\n",
    "filters_gcn = [64,64]\n",
    "A_normalized_method = 'Normalized-Laplacian' #'Normalization'\n",
    "gcn_activation = eval('nn.'+'ReLU'+'(inplace=True)')#ELU #Identity\n",
    "dropout_gcn = 0.3\n",
    "gcn_bias = True\n",
    "\n",
    "seed = 'Random'\n",
    "if seed in ['Random','random'] :\n",
    "    seed = random.randrange(0, 10000, 1)\n",
    "else:\n",
    "    seed = int(seed)\n",
    "log_interval = 50\n",
    "  \n",
    "output_url = './Cora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = sorted(list(set(labels)))\n",
    "    if '9999' in classes:\n",
    "        classes.remove('9999')\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    onehot_to_class = {np.where(np.identity(len(classes))[i, :])[0][0]:c for i, c in enumerate(classes)}\n",
    "    classes_dict['9999'] = [0]+[0]*(len(classes)-1)\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),dtype=np.int32)\n",
    "    return labels_onehot, onehot_to_class\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    #csr matrix轉回coo matrix\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    #indices:儲存row和col\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    #values:儲存非0數值\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def laplacian(mx):\n",
    "    \"\"\"compute L=D^-0.5 * (mx) * D^-0.5\"\"\"\n",
    "    degree = np.array(mx.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    laplacian_mx = d_hat.dot(mx).dot(d_hat)\n",
    "    return laplacian_mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct*100 / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"\",n_folds=None,training_size=None,A_normalized_method=None):\n",
    "    print('loading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    #idx_features\n",
    "    idx_features_url = path+'/'+list(filter(lambda f: f.find('_node_features') >= 0, files))[0]\n",
    "    f1 = open(idx_features_url)\n",
    "    if ',' in f1.readline():\n",
    "        idx_features = np.genfromtxt(idx_features_url,delimiter=',',dtype=np.dtype(str))\n",
    "    else:\n",
    "        idx_features = np.genfromtxt(idx_features_url,dtype=np.dtype(str))\n",
    "\n",
    "    #features (normalized)\n",
    "    features = sp.csr_matrix(idx_features[:, 1:], dtype=np.float32)\n",
    "    #features = normalize(features)\n",
    "    \n",
    "    #labels (one-hot)\n",
    "    labels_url = path+'/'+list(filter(lambda f: f.find('_node_label') >= 0, files))[0]\n",
    "    labels, onehot_to_class = encode_onehot(np.genfromtxt(labels_url,dtype=np.dtype(str)))\n",
    "    labels_name = np.genfromtxt(labels_url,dtype=np.dtype(str))\n",
    "    #ID\n",
    "    ID = idx_features[:, 0]\n",
    "    ID_to_idx = {j: i for i, j in enumerate(ID)}\n",
    "    idx_to_ID = {i: j for i, j in enumerate(ID)}\n",
    "\n",
    "    #edges\n",
    "    files = os.listdir(path)\n",
    "    edges_url = path+'/'+list(filter(lambda f: f.find('_A') >= 0, files))[0]\n",
    "    f1 = open(edges_url)\n",
    "    if ',' in f1.readline():\n",
    "        edges_unordered = np.genfromtxt(edges_url,delimiter=',',dtype=np.dtype(str))\n",
    "    else:\n",
    "        edges_unordered = np.genfromtxt(edges_url,dtype=np.dtype(str))\n",
    "    f1.close()\n",
    "    edges = np.array(list(map(ID_to_idx.get, edges_unordered.flatten())),dtype=np.int32).reshape(edges_unordered.shape)\n",
    "\n",
    "    #A (symmetric)\n",
    "    edges = edges[edges[:,0]!=edges[:,1]]\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    \n",
    "    if A_normalized_method == 'Normalization':\n",
    "        \"\"\"compute D^-1 * (A+I)\"\"\"\n",
    "        adj = normalize(adj)\n",
    "    #對稱標準化(Laplacian)\n",
    "    if A_normalized_method == 'Normalized-Laplacian':\n",
    "        \"\"\"compute D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
    "        adj = laplacian(adj)\n",
    "    \n",
    "    \n",
    "    #setting training set & val set\n",
    "    labels_df = pd.DataFrame(labels)\n",
    "    gp = set(labels_df[labels_df.sum(1)!=0].index)\n",
    "    idx_unknown = list(labels_df[labels_df.sum(1)==0].index)\n",
    "\n",
    "    assert n_folds >= 1, \"'n_folds' should >= 1\"\n",
    "    if n_folds == 1:\n",
    "        #size\n",
    "        known_size = len(gp)\n",
    "        train_size = round(known_size*training_size)\n",
    "        #sampling\n",
    "        idx_train = set(random.sample(gp,k=train_size))\n",
    "        idx_val = gp - idx_train\n",
    "        #tolist\n",
    "        idx_train = torch.LongTensor(sorted(list(idx_train)))\n",
    "        idx_val = torch.LongTensor(sorted(list(idx_val)))\n",
    "        #to Multiple list\n",
    "        idx_train_ls = [idx_train]\n",
    "        idx_val_ls = [idx_val]\n",
    "    else:\n",
    "        #size\n",
    "        gp_full = set(labels_df[labels_df.sum(1)!=0].index)\n",
    "        known_size = len(gp)\n",
    "        val_size = round(known_size*(1/n_folds))\n",
    "\n",
    "        idx_val_ls = []\n",
    "        idx_train_ls = []\n",
    "        for i in range(n_folds-1):\n",
    "            #sampling\n",
    "            idx_val = set(random.sample(gp,k=val_size))\n",
    "            idx_train = gp_full - idx_val\n",
    "            idx_val_ls.append(torch.LongTensor(sorted(list(idx_val))))\n",
    "            idx_train_ls.append(torch.LongTensor(sorted(list(idx_train))))\n",
    "            gp = gp - idx_val\n",
    "        idx_val_ls.append(torch.LongTensor(sorted(list(gp))))\n",
    "        idx_train_ls.append(torch.LongTensor(sorted(list(gp_full-gp))))\n",
    "\n",
    "    #轉Tensor\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels_location = []\n",
    "    for i in labels:\n",
    "        if sum(i) == 0:\n",
    "            labels_location.append(9999)\n",
    "        else:\n",
    "            labels_location.append(np.where(i)[0][0])\n",
    "    labels = torch.LongTensor(np.array(labels_location))\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    \n",
    "    #n_class\n",
    "    n_class = len(onehot_to_class)\n",
    "\n",
    "    return adj, features, labels, idx_train_ls, idx_val_ls, ID, labels_name, n_class, onehot_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train_ls, idx_val_ls, ID, labels_name, n_class, onehot_to_class = load_data(path=path, n_folds=n_folds, training_size=training_size,A_normalized_method=A_normalized_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN layers and models\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_features,\n",
    "                out_features,\n",
    "                activation=None,\n",
    "                dropout_gcn=0,\n",
    "                gcn_bias=None):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=in_features, out_features=out_features,bias=gcn_bias)\n",
    "        self.activation = activation\n",
    "        self.drop = nn.Dropout(p=dropout_gcn) if dropout_gcn > 0.0 else nn.Identity()\n",
    "    def forward(self, data):\n",
    "        x, A = data[:2]\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.spmm(A, x)\n",
    "        x = self.activation(x) \n",
    "        return (x, A)\n",
    "    \n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features, #n_class\n",
    "                 filters_gcn=None,\n",
    "                 dropout_gcn=0,\n",
    "                 gcn_bias=True,\n",
    "                 gcn_activation=None):\n",
    "        super(GCN, self).__init__()\n",
    "        self.hidden = filters_gcn.copy()\n",
    "        self.hidden.append(out_features)\n",
    "        # GCN\n",
    "        self.gconv = nn.Sequential(*([GraphConv(in_features=in_features if layer == 0 else self.hidden[layer - 1], \n",
    "                                                out_features=f, \n",
    "                                                activation=gcn_activation if layer != len(self.hidden)-1 else nn.Identity(),\n",
    "                                                dropout_gcn=dropout_gcn,\n",
    "                                                gcn_bias=gcn_bias) for layer, f in enumerate(self.hidden)]))       \n",
    "    def forward(self, data):\n",
    "        x = self.gconv(data)[0]\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gconv): Sequential(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=1433, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=7, bias=True)\n",
      "      (activation): Identity()\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "FOLD 1\n",
      "training:2166/2707\n",
      "validation:541/2707\n",
      "Epoch: 0001 loss_train: 1.9545 acc_train: 8.13% loss_val: 1.9206 acc_val: 28.84% time: 0.0530s\n",
      "Epoch: 0050 loss_train: 0.2131 acc_train: 92.29% loss_val: 0.4553 acc_val: 86.69% time: 0.0520s\n",
      "Epoch: 0100 loss_train: 0.1786 acc_train: 94.04% loss_val: 0.4765 acc_val: 87.80% time: 0.0520s\n",
      "Epoch: 0150 loss_train: 0.1533 acc_train: 94.14% loss_val: 0.5171 acc_val: 86.69% time: 0.0530s\n",
      "Epoch: 0200 loss_train: 0.1346 acc_train: 94.74% loss_val: 0.5229 acc_val: 86.69% time: 0.0510s\n",
      "Epoch: 0250 loss_train: 0.1351 acc_train: 94.97% loss_val: 0.5370 acc_val: 87.25% time: 0.0540s\n",
      "Epoch: 0300 loss_train: 0.1450 acc_train: 94.41% loss_val: 0.5679 acc_val: 85.95% time: 0.0520s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 15.7334s\n",
      "Validation set results: loss= 0.5679 accuracy= 85.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chienhua\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc_folds = []\n",
    "val_acc_folds = []\n",
    "for fold_id in range(n_folds):\n",
    "    model = GCN(in_features=features.shape[1],\n",
    "                out_features=n_class,\n",
    "                filters_gcn=filters_gcn,\n",
    "                dropout_gcn=dropout_gcn,\n",
    "                gcn_bias=gcn_bias,\n",
    "                gcn_activation=gcn_activation)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=wdecay)    \n",
    "    #if fold_id == 0:\n",
    "    print(model)\n",
    "    print('\\nFOLD', fold_id+1)\n",
    "    print('training:%s/%s'%(len(idx_train_ls[fold_id]),len(idx_train_ls[fold_id])+len(idx_val_ls[fold_id])))\n",
    "    print('validation:%s/%s'%(len(idx_val_ls[fold_id]),len(idx_train_ls[fold_id])+len(idx_val_ls[fold_id])))\n",
    "    Loss_Train = []\n",
    "    Acc_Train = []\n",
    "    Loss_Val = []\n",
    "    Acc_Val = []\n",
    "    def train(epoch):\n",
    "        t = time.time()\n",
    "        #training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        data = (features,adj)\n",
    "        output = model(data)\n",
    "        loss_train = F.nll_loss(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]]) \n",
    "        acc_train = accuracy(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]])\n",
    "        Loss_Train.append(loss_train.tolist())\n",
    "        Acc_Train.append(acc_train.tolist())\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        data = (features,adj)\n",
    "        output = model(data)\n",
    "        loss_val = F.nll_loss(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]]) \n",
    "        acc_val = accuracy(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        Loss_Val.append(loss_val.tolist())\n",
    "        Acc_Val.append(acc_val.tolist())\n",
    "        if ((epoch+1) % log_interval == 0) or (epoch+1==epochs)or (epoch==0):\n",
    "            print('Epoch: {:04d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'acc_train: {:.2f}%'.format(acc_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_val: {:.2f}%'.format(acc_val.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    prediction = []\n",
    "    prob = []\n",
    "    def test():\n",
    "        model.eval()\n",
    "        data = (features,adj)\n",
    "        output = model(data)\n",
    "        prob.append(output)\n",
    "        loss_val = F.nll_loss(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        acc_train = accuracy(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]])\n",
    "        acc_val = accuracy(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        train_acc_folds.append(acc_train)\n",
    "        val_acc_folds.append(acc_val)\n",
    "        preds = output.max(1)[1].type_as(labels)\n",
    "        preds = preds.tolist()\n",
    "        preds = [onehot_to_class[i] for i in preds]        \n",
    "        prediction.extend(preds)\n",
    "        print(\"Validation set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_val.item()),\n",
    "              \"accuracy= {:.2f}%\".format(acc_val.item()))\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    test()\n",
    "    \n",
    "    #########################################################################\n",
    "    writer = pd.ExcelWriter(output_url+'/result_fold%s.xlsx'%(fold_id+1), engine = 'xlsxwriter')\n",
    "    training_df = pd.DataFrame(zip(list(range(1,epochs+1)),Loss_Train,Acc_Train,Loss_Val,Acc_Val))\n",
    "    training_df.columns = ['epoch','training loss','training accuracy','validation loss','validation accuracy']\n",
    "    training_df.to_excel(writer, sheet_name = 'training&validation',index=False)\n",
    "    #########################################################################\n",
    "    for i in range(len(model.gconv)):\n",
    "        f_GCN_weight = pd.DataFrame(np.matrix(model.gconv[i].fc.weight.tolist()).T)\n",
    "        f_GCN_weight.to_excel(writer,sheet_name='GCN%s_weight'%i,header=False,index=False)\n",
    "        if gcn_bias:\n",
    "            f_GCN_bias = pd.DataFrame(np.matrix(model.gconv[i].fc.bias.tolist()).T)\n",
    "            f_GCN_bias.to_excel(writer,sheet_name='GCN%s_bias'%i,header=False,index=False) \n",
    "    ################################################################################\n",
    "    split = np.empty((len(labels_name)),dtype=np.object)\n",
    "    for i in idx_train_ls[fold_id].tolist():\n",
    "        split[i] = 'training'\n",
    "    for i in idx_val_ls[fold_id].tolist():\n",
    "        split[i] = 'validation'\n",
    "    corrects = []\n",
    "    for i,j in zip(prediction,labels_name):\n",
    "        if j == '9999':\n",
    "            corrects.append('')\n",
    "        elif i == j:\n",
    "            corrects.append(1)\n",
    "        elif i != j:\n",
    "            corrects.append(0)\n",
    "    preds_df = pd.DataFrame(zip(ID,prediction,labels_name,split,corrects))\n",
    "    preds_df.columns = ['ID','prediction','label','splits','correct']\n",
    "    \n",
    "    class_prob = pd.DataFrame(np.exp(prob[0].tolist())*100)\n",
    "    columns = []\n",
    "    for c in class_prob.columns:\n",
    "        class_prob[c] = class_prob[c].map('{:,.2f}%'.format)\n",
    "        columns.append('p('+onehot_to_class[c]+')')\n",
    "    class_prob.columns = columns\n",
    "    preds_df = pd.concat([preds_df,class_prob], axis=1)\n",
    "    \n",
    "    preds_df.to_excel(writer, sheet_name = 'prediction',index=False)\n",
    "    ################################################################################\n",
    "    #sampling info.\n",
    "    class_number_train = []\n",
    "    class_proportion_train = []\n",
    "    class_number_val = []\n",
    "    class_proportion_val = []    \n",
    "    \n",
    "    label_set = sorted(list(set(labels_name)))\n",
    "    if '9999' in label_set:\n",
    "        label_set.remove('9999')\n",
    "    for i in label_set:\n",
    "        class_number_train.append(sum(labels_name[idx_train_ls[fold_id]]== i))\n",
    "        class_proportion_train.append('%.1f%%'%(sum(labels_name[idx_train_ls[fold_id]]== i)*100/len(idx_train_ls[fold_id])))\n",
    "        class_number_val.append(sum(labels_name[idx_val_ls[fold_id]]== i))\n",
    "        class_proportion_val.append('%.1f%%'%(sum(labels_name[idx_val_ls[fold_id]]== i)*100/len(idx_val_ls[fold_id])))  \n",
    "    sampling_info = pd.DataFrame(zip(label_set,class_number_train,class_proportion_train,class_number_val,class_proportion_val))\n",
    "    sampling_info.columns = ['label','training set','proportion of training set','validation set','proportion of validation set']\n",
    "    sampling_info.to_excel(writer, sheet_name = 'sampling_info',index=False)    \n",
    "    #################################################################################################\n",
    "    #class_acc\n",
    "    Number_Train = []\n",
    "    Correct_Train = []\n",
    "    Number_Val = []\n",
    "    Correct_Val = []\n",
    "    Values = onehot_to_class.values()\n",
    "    for i in Values:\n",
    "        Correct_Train.append(len(preds_df[(preds_df['correct']==1)&(preds_df['label']==i)&(preds_df['splits']=='training')]))\n",
    "        Number_Train.append(len(preds_df[(preds_df['label']==i)&(preds_df['splits']=='training')]))\n",
    "        Correct_Val.append(len(preds_df[(preds_df['correct']==1)&(preds_df['label']==i)&(preds_df['splits']=='validation')]))\n",
    "        Number_Val.append(len(preds_df[(preds_df['label']==i)&(preds_df['splits']=='validation')]))\n",
    "        Rate_Train = np.array(Correct_Train)*100/np.array(Number_Train)\n",
    "        Rate_Val = np.array(Correct_Val)*100/np.array(Number_Val)\n",
    "    Class_Prob = pd.DataFrame(zip(Values,Correct_Train,Number_Train,Rate_Train,Correct_Val,Number_Val,Rate_Val))\n",
    "    Class_Prob.columns = ['class','train_correct','train_number','train_accuracy','validation_correct','validation_number','validation_accuracy']\n",
    "    Class_Prob['train_accuracy'] = Class_Prob['train_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob['validation_accuracy'] = Class_Prob['validation_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob.to_excel(writer, sheet_name = 'class_acc',index=False)      \n",
    "    #################################################################################################\n",
    "    #label_pred_mx_train\n",
    "    #label_pred_mx_val\n",
    "    class_to_onehot = dict(zip([onehot_to_class[i] for i in range(len(onehot_to_class))],range(len(onehot_to_class))))\n",
    "    label_pred_mx_train = np.zeros([len(onehot_to_class),len(onehot_to_class)])\n",
    "    label_pred_mx_val = np.zeros([len(onehot_to_class),len(onehot_to_class)])\n",
    "    for i in range(len(preds_df)):\n",
    "        if preds_df.at[i,'label'] == '9999':\n",
    "            continue\n",
    "        if preds_df.at[i,'splits'] == 'training':\n",
    "            label_pred_mx_train[class_to_onehot[preds_df.at[i,'label']]][class_to_onehot[preds_df.at[i,'prediction']]] += 1\n",
    "        elif preds_df.at[i,'splits'] == 'validation':\n",
    "            label_pred_mx_val[class_to_onehot[preds_df.at[i,'label']]][class_to_onehot[preds_df.at[i,'prediction']]] += 1\n",
    "    label_pred_mx_train = pd.DataFrame(label_pred_mx_train,dtype=int)\n",
    "    label_pred_mx_val = pd.DataFrame(label_pred_mx_val,dtype=int)        \n",
    "    label_pred_mx_train.columns = [onehot_to_class[i] for i in range(len(onehot_to_class))]\n",
    "    label_pred_mx_val.columns = [onehot_to_class[i] for i in range(len(onehot_to_class))]       \n",
    "    label_pred_mx_train['label\\pred'] = [onehot_to_class[i] for i in range(len(onehot_to_class))]\n",
    "    label_pred_mx_val['label\\pred'] = [onehot_to_class[i] for i in range(len(onehot_to_class))]        \n",
    "    label_pred_mx_train = pd.concat([label_pred_mx_train['label\\pred'],label_pred_mx_train[[onehot_to_class[i] for i in range(len(onehot_to_class))]]],axis=1)\n",
    "    label_pred_mx_val = pd.concat([label_pred_mx_val['label\\pred'],label_pred_mx_val[[onehot_to_class[i] for i in range(len(onehot_to_class))]]],axis=1)\n",
    "    label_pred_mx_train.to_excel(writer, sheet_name = 'train_label_pred',index=False)         \n",
    "    label_pred_mx_val.to_excel(writer, sheet_name = 'val_label_pred',index=False)          \n",
    "    writer.save()\n",
    "\n",
    "    Epoch = range(1,epochs+1)\n",
    "    plt.plot(Epoch,Loss_Train,label='training loss')\n",
    "    plt.plot(Epoch,Loss_Val,label='validation loss')\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('loss',fontsize=18)    \n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_url+'/loss_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.plot(Epoch,np.array(Acc_Train)/100,label='training accuracy')\n",
    "    plt.plot(Epoch,np.array(Acc_Val)/100,label='validation accuracy')\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('accuracy',fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_url+'/acc_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "summary.append('traing set:')\n",
    "for i in range(len(train_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,train_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(train_acc_folds),np.std(train_acc_folds)))\n",
    "summary.append('validation set:')\n",
    "for i in range(len(val_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,val_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(val_acc_folds),np.std(val_acc_folds)))\n",
    "f1 = open(output_url+'/summary_acc.txt','w')\n",
    "f1.write('\\n'.join(summary))\n",
    "f1.close()    \n",
    "\n",
    "f1 = open(output_url+'/model.txt','w')\n",
    "model_structure = str(model).split('\\n')\n",
    "model_structure[-5] = '      (activation): Softmax()'\n",
    "f1.write('\\n'.join(model_structure))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415.5055555555556"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29.9164/0.0720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
