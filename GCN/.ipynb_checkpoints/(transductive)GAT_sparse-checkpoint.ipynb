{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cora'\n",
    "n_folds = 10\n",
    "training_size = 0.05\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "wdecay = 5e-4\n",
    "model_name = 'GAT'\n",
    "#GAT\n",
    "n_hidden = 8\n",
    "dropout_gat = 0.6\n",
    "alpha_leakyReLU = 0.2\n",
    "n_att = 8\n",
    "\n",
    "#\n",
    "seed = 'Random'\n",
    "if seed in ['Random','random']:\n",
    "    seed = random.randrange(0, 1000000, 1)\n",
    "else:\n",
    "    seed = int(seed)\n",
    "log_interval = 1\n",
    "  \n",
    "output_url = path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = sorted(list(set(labels)))\n",
    "    if '9999' in classes:\n",
    "        classes.remove('9999')\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    onehot_to_class = {np.where(np.identity(len(classes))[i, :])[0][0]:c for i, c in enumerate(classes)}\n",
    "    classes_dict['9999'] = [0]+[0]*(len(classes)-1)\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),dtype=np.int32)\n",
    "    return labels_onehot, onehot_to_class\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    #csr matrix轉回coo matrix\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    #indices:儲存row和col\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    #values:儲存非0數值\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    if mx.toarray()[0][0] == 0:\n",
    "        mx = mx + sp.eye(mx.shape[0])\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "def laplacian(mx):\n",
    "    \"\"\"compute L=D^-0.5 * (mx) * D^-0.5\"\"\"\n",
    "    if mx.toarray()[0][0] == 0:\n",
    "        mx = mx + sp.eye(mx.shape[0])    \n",
    "    degree = np.array(mx.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    laplacian_mx = d_hat.dot(mx).dot(d_hat)\n",
    "    return laplacian_mx\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct*100 / len(labels)\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"\",n_folds=None,training_size=None):\n",
    "    print('loading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    #idx_features\n",
    "    idx_features_url = path+'/'+list(filter(lambda f: f.find('_node_features') >= 0, files))[0]\n",
    "    f1 = open(idx_features_url)\n",
    "    if ',' in f1.readline():\n",
    "        idx_features = np.genfromtxt(idx_features_url,delimiter=',',dtype=np.dtype(str))\n",
    "    else:\n",
    "        idx_features = np.genfromtxt(idx_features_url,dtype=np.dtype(str))\n",
    "\n",
    "    #features (normalized)\n",
    "    features = sp.csr_matrix(idx_features[:, 1:], dtype=np.float32)\n",
    "    #features = normalize_features(features)\n",
    "\n",
    "    #labels (one-hot)\n",
    "    labels_url = path+'/'+list(filter(lambda f: f.find('_node_label') >= 0, files))[0]\n",
    "    labels, onehot_to_class = encode_onehot(np.genfromtxt(labels_url,dtype=np.dtype(str)))\n",
    "    labels_name = np.genfromtxt(labels_url,dtype=np.dtype(str))\n",
    "    #ID\n",
    "    ID = idx_features[:, 0]\n",
    "    ID_to_idx = {j: i for i, j in enumerate(ID)}\n",
    "    idx_to_ID = {i: j for i, j in enumerate(ID)}\n",
    "\n",
    "    #edges\n",
    "    files = os.listdir(path)\n",
    "    edges_url = path+'/'+list(filter(lambda f: f.find('_A') >= 0, files))[0]\n",
    "    f1 = open(edges_url)\n",
    "    if ',' in f1.readline():\n",
    "        edges_unordered = np.genfromtxt(edges_url,delimiter=',',dtype=np.dtype(str))\n",
    "    else:\n",
    "        edges_unordered = np.genfromtxt(edges_url,dtype=np.dtype(str))\n",
    "    f1.close()\n",
    "    edges = np.array(list(map(ID_to_idx.get, edges_unordered.flatten())),dtype=np.int32).reshape(edges_unordered.shape)\n",
    "\n",
    "    #A (symmetric)\n",
    "    edges = edges[edges[:,0]!=edges[:,1]]\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    \n",
    "    #setting training set & val set\n",
    "    labels_df = pd.DataFrame(labels)\n",
    "    gp = set(labels_df[labels_df.sum(1)!=0].index)\n",
    "    idx_unknown = list(labels_df[labels_df.sum(1)==0].index)\n",
    "\n",
    "    assert n_folds >= 1, \"'n_folds' should >= 1\"\n",
    "    if n_folds == 1:\n",
    "        #size\n",
    "        known_size = len(gp)\n",
    "        train_size = round(known_size*training_size)\n",
    "        #sampling\n",
    "        idx_train = set(random.sample(gp,k=train_size))\n",
    "        idx_val = gp - idx_train\n",
    "        #tolist\n",
    "        idx_train = torch.LongTensor(sorted(list(idx_train)))\n",
    "        idx_val = torch.LongTensor(sorted(list(idx_val)))\n",
    "        #to Multiple list\n",
    "        idx_train_ls = [idx_train]\n",
    "        idx_val_ls = [idx_val]\n",
    "    else:\n",
    "        #size\n",
    "        gp_full = set(labels_df[labels_df.sum(1)!=0].index)\n",
    "        known_size = len(gp)\n",
    "        val_size = round(known_size*(1/n_folds))\n",
    "\n",
    "        idx_val_ls = []\n",
    "        idx_train_ls = []\n",
    "        for i in range(n_folds-1):\n",
    "            #sampling\n",
    "            idx_val = set(random.sample(gp,k=val_size))\n",
    "            idx_train = gp_full - idx_val\n",
    "            idx_val_ls.append(torch.LongTensor(sorted(list(idx_val))))\n",
    "            idx_train_ls.append(torch.LongTensor(sorted(list(idx_train))))\n",
    "            gp = gp - idx_val\n",
    "        idx_val_ls.append(torch.LongTensor(sorted(list(gp))))\n",
    "        idx_train_ls.append(torch.LongTensor(sorted(list(gp_full-gp))))\n",
    "\n",
    "    #轉Tensor\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels_location = []\n",
    "    for i in labels:\n",
    "        if sum(i) == 0:\n",
    "            labels_location.append(9999)\n",
    "        else:\n",
    "            labels_location.append(np.where(i)[0][0])\n",
    "    labels = torch.LongTensor(np.array(labels_location))\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    #adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    \n",
    "    #n_class\n",
    "    n_class = len(onehot_to_class)\n",
    "\n",
    "    return adj, features, labels, idx_train_ls, idx_val_ls, ID, labels_name, n_class, onehot_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train_ls, idx_val_ls, ID, labels_name, n_class, onehot_to_class = load_data(path=path, n_folds=n_folds, training_size=training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "    def extra_repr(self):\n",
    "        lines = []\n",
    "        lines.append('(hidden_features): Linear(in_features=%s, out_features=%s, bias=False)'%(self.in_features,self.out_features))\n",
    "        lines.append('(attetion): Attetion(')\n",
    "        lines.append('  (concat_ij): Concat(in_features=%s, out_features=%s*2)'%(self.out_features,self.out_features))\n",
    "        lines.append('  (a): Linear(in_features=%s, out_features=1, bias=False)'%(self.out_features*2))\n",
    "        lines.append('  (leakyrelu): LeakyReLU(negative_slope=%s)'%(self.alpha))\n",
    "        lines.append('  (concat_edges): Concat(in_features=1, out_features=1-hop edges)')\n",
    "        lines.append('  (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)')\n",
    "        lines.append('  (dropout_attetion): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append(')')\n",
    "        lines.append('(weighted_hidden_features): Matmul(attention, hidden_features)')\n",
    "        if self.concat:\n",
    "            lines.append('(elu): ELU(alpha=1.0)')        \n",
    "        lines = '\\n'.join(lines)\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.nhid = nhid\n",
    "        self.nheads = nheads\n",
    "        self.nclass = nclass\n",
    "        \n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
    "                                             nclass, \n",
    "                                             dropout=dropout, \n",
    "                                             alpha=alpha, \n",
    "                                             concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    def extra_repr(self):\n",
    "        lines = []\n",
    "        lines.append('GAT(')\n",
    "        lines.append('  (dropout): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append('->')\n",
    "        for i, att in enumerate(self.attentions):\n",
    "            lines.append('  (attention_%d): GraphAttentionLayer('%(i))\n",
    "            for j in att.extra_repr().split('\\n'):\n",
    "                lines.append('    '+j)\n",
    "            lines.append('  )')\n",
    "        lines.append('->')             \n",
    "        lines.append('  (concat_attention_0-%s): Concat(in_features=%s, out_features=%s*%slayers)'%(self.nheads-1,\n",
    "                                                                                    self.nhid, self.nhid, self.nheads))\n",
    "        lines.append('->')               \n",
    "        lines.append('  (dropout): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append('->')  \n",
    "        lines.append('  (attention_out): GraphAttentionLayer(')\n",
    "        for j in self.out_att.extra_repr().split('\\n'):\n",
    "            lines.append('    '+j)\n",
    "        lines.append('  )')\n",
    "        lines.append('->')\n",
    "        lines.append('  (elu): ELU(alpha=1.0)')     \n",
    "        lines.append('->')\n",
    "        lines.append('  (softmax): Softmax(in_features=%s, out_features=%s)'%(self.nclass,self.nclass))\n",
    "        lines.append(')')\n",
    "        lines='\\n'.join(lines)\n",
    "        \n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (dropout): Dropout(p=0.6)\n",
      "->\n",
      "  (attention_0): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_1): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_2): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_3): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_4): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_5): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_6): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_7): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "->\n",
      "  (concat_attention_0-7): Concat(in_features=8, out_features=8*8layers)\n",
      "->\n",
      "  (dropout): Dropout(p=0.6)\n",
      "->\n",
      "  (attention_out): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=64, out_features=7, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=7, out_features=7*2)\n",
      "      (a): Linear(in_features=14, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "  )\n",
      "->\n",
      "  (elu): ELU(alpha=1.0)\n",
      "->\n",
      "  (softmax): Softmax(in_features=7, out_features=7)\n",
      ")\n",
      "seed: 624416\n",
      "\n",
      "FOLD 1\n",
      "training:2436/2707\n",
      "validation:271/2707\n",
      "Epoch: 0001 loss_train: 2.0597 acc_train: 15.27% loss_val: 1.5909 acc_val: 39.85% time: 1.5592s\n",
      "Epoch: 0002 loss_train: 1.7938 acc_train: 30.87% loss_val: 1.3497 acc_val: 56.46% time: 1.1922s\n",
      "Epoch: 0003 loss_train: 1.5895 acc_train: 42.53% loss_val: 1.1407 acc_val: 76.01% time: 1.0887s\n",
      "Epoch: 0004 loss_train: 1.4397 acc_train: 51.85% loss_val: 0.9689 acc_val: 85.24% time: 1.2581s\n",
      "Epoch: 0005 loss_train: 1.3326 acc_train: 58.09% loss_val: 0.8346 acc_val: 87.08% time: 1.2517s\n",
      "Epoch: 0006 loss_train: 1.2357 acc_train: 61.58% loss_val: 0.7258 acc_val: 90.77% time: 1.2548s\n",
      "Epoch: 0007 loss_train: 1.1552 acc_train: 64.04% loss_val: 0.6387 acc_val: 91.14% time: 1.5163s\n",
      "Epoch: 0008 loss_train: 1.0963 acc_train: 66.54% loss_val: 0.5697 acc_val: 91.88% time: 1.1267s\n",
      "Epoch: 0009 loss_train: 1.0581 acc_train: 66.42% loss_val: 0.5169 acc_val: 91.51% time: 1.2503s\n",
      "Epoch: 0010 loss_train: 0.9940 acc_train: 66.91% loss_val: 0.4744 acc_val: 91.51% time: 1.3250s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 12.8891s\n",
      "Validation set results: loss= 0.4744 accuracy= 91.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chienhua\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:96: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (dropout): Dropout(p=0.6)\n",
      "->\n",
      "  (attention_0): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_1): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_2): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_3): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_4): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_5): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_6): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_7): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "->\n",
      "  (concat_attention_0-7): Concat(in_features=8, out_features=8*8layers)\n",
      "->\n",
      "  (dropout): Dropout(p=0.6)\n",
      "->\n",
      "  (attention_out): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=64, out_features=7, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=7, out_features=7*2)\n",
      "      (a): Linear(in_features=14, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=Dropout(p=0.6, inplace=False))\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "  )\n",
      "->\n",
      "  (elu): ELU(alpha=1.0)\n",
      "->\n",
      "  (softmax): Softmax(in_features=7, out_features=7)\n",
      ")\n",
      "seed: 624416\n",
      "\n",
      "FOLD 2\n",
      "training:2436/2707\n",
      "validation:271/2707\n",
      "Epoch: 0001 loss_train: 2.1360 acc_train: 14.16% loss_val: 1.6422 acc_val: 39.85% time: 1.4157s\n",
      "Epoch: 0002 loss_train: 1.8363 acc_train: 29.39% loss_val: 1.4494 acc_val: 49.45% time: 1.4783s\n",
      "Epoch: 0003 loss_train: 1.6463 acc_train: 38.92% loss_val: 1.2747 acc_val: 67.53% time: 1.2083s\n",
      "Epoch: 0004 loss_train: 1.4825 acc_train: 47.91% loss_val: 1.1335 acc_val: 74.17% time: 1.1610s\n"
     ]
    }
   ],
   "source": [
    "train_acc_folds = []\n",
    "val_acc_folds = []\n",
    "for fold_id in range(n_folds):\n",
    "    model = SpGAT(nfeat=features.shape[1],\n",
    "                nhid=n_hidden,\n",
    "                nclass=n_class,\n",
    "                dropout=dropout_gat,\n",
    "                alpha=alpha_leakyReLU,\n",
    "                nheads=n_att)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=wdecay)    \n",
    "    #if fold_id == 0:\n",
    "    print(model.extra_repr())\n",
    "    print('seed:',seed)\n",
    "    print('\\nFOLD', fold_id+1)\n",
    "    print('training:%s/%s'%(len(idx_train_ls[fold_id]),len(idx_train_ls[fold_id])+len(idx_val_ls[fold_id])))\n",
    "    print('validation:%s/%s'%(len(idx_val_ls[fold_id]),len(idx_train_ls[fold_id])+len(idx_val_ls[fold_id])))\n",
    "    Loss_Train = []\n",
    "    Acc_Train = []\n",
    "    Loss_Val = []\n",
    "    Acc_Val = []\n",
    "    def train(epoch):\n",
    "        t = time.time()\n",
    "        #training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features,adj)\n",
    "        loss_train = F.nll_loss(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]]) \n",
    "        acc_train = accuracy(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]])\n",
    "        Loss_Train.append(loss_train.tolist())\n",
    "        Acc_Train.append(acc_train.tolist())\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        output = model(features,adj)\n",
    "        loss_val = F.nll_loss(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]]) \n",
    "        acc_val = accuracy(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        Loss_Val.append(loss_val.tolist())\n",
    "        Acc_Val.append(acc_val.tolist())\n",
    "        if ((epoch+1) % log_interval == 0) or (epoch+1==epochs)or (epoch==0):\n",
    "            print('Epoch: {:04d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'acc_train: {:.2f}%'.format(acc_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_val: {:.2f}%'.format(acc_val.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    prediction = []\n",
    "    prob = []\n",
    "    def test():\n",
    "        model.eval()\n",
    "        output = model(features,adj)\n",
    "        prob.append(output)\n",
    "        loss_val = F.nll_loss(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        acc_train = accuracy(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]])\n",
    "        acc_val = accuracy(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        train_acc_folds.append(acc_train)\n",
    "        val_acc_folds.append(acc_val)\n",
    "        preds = output.max(1)[1].type_as(labels)\n",
    "        preds = preds.tolist()\n",
    "        preds = [onehot_to_class[i] for i in preds]        \n",
    "        prediction.extend(preds)\n",
    "        print(\"Validation set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_val.item()),\n",
    "              \"accuracy= {:.2f}%\".format(acc_val.item()))\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    test()\n",
    "    \n",
    "    #########################################################################\n",
    "    writer = pd.ExcelWriter(output_url+'/result_fold%s.xlsx'%(fold_id+1), engine = 'xlsxwriter')\n",
    "    training_df = pd.DataFrame(zip(list(range(1,epochs+1)),Loss_Train,Acc_Train,Loss_Val,Acc_Val))\n",
    "    training_df.columns = ['epoch','training loss','training accuracy','validation loss','validation accuracy']\n",
    "    training_df.to_excel(writer, sheet_name = 'training&validation',index=False)\n",
    "    #########################################################################   \n",
    "    for i in range(len(model.attentions)):\n",
    "        W_weight = pd.DataFrame(np.matrix(model.attentions[i].W.data.tolist()))\n",
    "        W_weight.to_excel(writer,sheet_name='W%s_weight'%i,header=False,index=False)  \n",
    "        a_weight = pd.DataFrame(np.matrix(model.attentions[i].a.data.tolist()).T)\n",
    "        a_weight.to_excel(writer,sheet_name='a%s_weight'%i,header=False,index=False)    \n",
    "        \n",
    "    W_weight = pd.DataFrame(np.matrix(model.out_att.W.data.tolist()))\n",
    "    W_weight.to_excel(writer,sheet_name='W_out_weight',header=False,index=False)  \n",
    "    a_weight = pd.DataFrame(np.matrix(model.out_att.a.data.tolist()).T)\n",
    "    a_weight.to_excel(writer,sheet_name='a_out_weight',header=False,index=False)   \n",
    "    ################################################################################\n",
    "    split = np.empty((len(labels_name)),dtype=np.object)\n",
    "    for i in idx_train_ls[fold_id].tolist():\n",
    "        split[i] = 'training'\n",
    "    for i in idx_val_ls[fold_id].tolist():\n",
    "        split[i] = 'validation'\n",
    "    corrects = []\n",
    "    for i,j in zip(prediction,labels_name):\n",
    "        if j == '9999':\n",
    "            corrects.append('')\n",
    "        elif i == j:\n",
    "            corrects.append(1)\n",
    "        elif i != j:\n",
    "            corrects.append(0)\n",
    "    preds_df = pd.DataFrame(zip(ID,prediction,labels_name,split,corrects))\n",
    "    preds_df.columns = ['ID','prediction','label','splits','correct']\n",
    "    \n",
    "    class_prob = pd.DataFrame(np.exp(prob[0].tolist())*100)\n",
    "    columns = []\n",
    "    for c in class_prob.columns:\n",
    "        class_prob[c] = class_prob[c].map('{:,.2f}%'.format)\n",
    "        columns.append('p('+onehot_to_class[c]+')')\n",
    "    class_prob.columns = columns\n",
    "    preds_df = pd.concat([preds_df,class_prob], axis=1)\n",
    "    \n",
    "    preds_df.to_excel(writer, sheet_name = 'prediction',index=False)\n",
    "    ################################################################################\n",
    "    #sampling info.\n",
    "    class_number_train = []\n",
    "    class_proportion_train = []\n",
    "    class_number_val = []\n",
    "    class_proportion_val = []    \n",
    "    \n",
    "    label_set = sorted(list(set(labels_name)))\n",
    "    if '9999' in label_set:\n",
    "        label_set.remove('9999')\n",
    "    for i in label_set:\n",
    "        class_number_train.append(sum(labels_name[idx_train_ls[fold_id]]== i))\n",
    "        class_proportion_train.append('%.1f%%'%(sum(labels_name[idx_train_ls[fold_id]]== i)*100/len(idx_train_ls[fold_id])))\n",
    "        class_number_val.append(sum(labels_name[idx_val_ls[fold_id]]== i))\n",
    "        class_proportion_val.append('%.1f%%'%(sum(labels_name[idx_val_ls[fold_id]]== i)*100/len(idx_val_ls[fold_id])))  \n",
    "    sampling_info = pd.DataFrame(zip(label_set,class_number_train,class_proportion_train,class_number_val,class_proportion_val))\n",
    "    sampling_info.columns = ['label','training set','proportion of training set','validation set','proportion of validation set']\n",
    "    sampling_info.to_excel(writer, sheet_name = 'sampling_info',index=False)    \n",
    "    #################################################################################################\n",
    "    #class_acc\n",
    "    Number_Train = []\n",
    "    Correct_Train = []\n",
    "    Number_Val = []\n",
    "    Correct_Val = []\n",
    "    Values = onehot_to_class.values()\n",
    "    for i in Values:\n",
    "        Correct_Train.append(len(preds_df[(preds_df['correct']==1)&(preds_df['label']==i)&(preds_df['splits']=='training')]))\n",
    "        Number_Train.append(len(preds_df[(preds_df['label']==i)&(preds_df['splits']=='training')]))\n",
    "        Correct_Val.append(len(preds_df[(preds_df['correct']==1)&(preds_df['label']==i)&(preds_df['splits']=='validation')]))\n",
    "        Number_Val.append(len(preds_df[(preds_df['label']==i)&(preds_df['splits']=='validation')]))\n",
    "        Rate_Train = np.array(Correct_Train)*100/np.array(Number_Train)\n",
    "        Rate_Val = np.array(Correct_Val)*100/np.array(Number_Val)\n",
    "    Class_Prob = pd.DataFrame(zip(Values,Correct_Train,Number_Train,Rate_Train,Correct_Val,Number_Val,Rate_Val))\n",
    "    Class_Prob.columns = ['class','train_correct','train_number','train_accuracy','validation_correct','validation_number','validation_accuracy']\n",
    "    Class_Prob['train_accuracy'] = Class_Prob['train_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob['validation_accuracy'] = Class_Prob['validation_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob.to_excel(writer, sheet_name = 'class_acc',index=False)      \n",
    "    #################################################################################################\n",
    "    #label_pred_mx_train\n",
    "    #label_pred_mx_val\n",
    "    class_to_onehot = dict(zip([onehot_to_class[i] for i in range(len(onehot_to_class))],range(len(onehot_to_class))))\n",
    "    label_pred_mx_train = np.zeros([len(onehot_to_class),len(onehot_to_class)])\n",
    "    label_pred_mx_val = np.zeros([len(onehot_to_class),len(onehot_to_class)])\n",
    "    for i in range(len(preds_df)):\n",
    "        if preds_df.at[i,'label'] == '9999':\n",
    "            continue\n",
    "        if preds_df.at[i,'splits'] == 'training':\n",
    "            label_pred_mx_train[class_to_onehot[preds_df.at[i,'label']]][class_to_onehot[preds_df.at[i,'prediction']]] += 1\n",
    "        elif preds_df.at[i,'splits'] == 'validation':\n",
    "            label_pred_mx_val[class_to_onehot[preds_df.at[i,'label']]][class_to_onehot[preds_df.at[i,'prediction']]] += 1\n",
    "    label_pred_mx_train = pd.DataFrame(label_pred_mx_train,dtype=int)\n",
    "    label_pred_mx_val = pd.DataFrame(label_pred_mx_val,dtype=int)        \n",
    "    label_pred_mx_train.columns = [onehot_to_class[i] for i in range(len(onehot_to_class))]\n",
    "    label_pred_mx_val.columns = [onehot_to_class[i] for i in range(len(onehot_to_class))]       \n",
    "    label_pred_mx_train['label\\pred'] = [onehot_to_class[i] for i in range(len(onehot_to_class))]\n",
    "    label_pred_mx_val['label\\pred'] = [onehot_to_class[i] for i in range(len(onehot_to_class))]        \n",
    "    label_pred_mx_train = pd.concat([label_pred_mx_train['label\\pred'],label_pred_mx_train[[onehot_to_class[i] for i in range(len(onehot_to_class))]]],axis=1)\n",
    "    label_pred_mx_val = pd.concat([label_pred_mx_val['label\\pred'],label_pred_mx_val[[onehot_to_class[i] for i in range(len(onehot_to_class))]]],axis=1)\n",
    "    label_pred_mx_train.to_excel(writer, sheet_name = 'train_label_pred',index=False)         \n",
    "    label_pred_mx_val.to_excel(writer, sheet_name = 'val_label_pred',index=False)          \n",
    "    writer.save()\n",
    "\n",
    "    Epoch = range(1,epochs+1)\n",
    "    plt.plot(Epoch,Loss_Train,label='training loss')\n",
    "    plt.plot(Epoch,Loss_Val,label='validation loss')\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('loss',fontsize=18)    \n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_url+'/loss_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.plot(Epoch,np.array(Acc_Train)/100,label='training accuracy')\n",
    "    plt.plot(Epoch,np.array(Acc_Val)/100,label='validation accuracy')\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('accuracy',fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_url+'/acc_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "summary.append('traing set:')\n",
    "for i in range(len(train_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,train_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(train_acc_folds),np.std(train_acc_folds)))\n",
    "summary.append('validation set:')\n",
    "for i in range(len(val_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,val_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(val_acc_folds),np.std(val_acc_folds)))\n",
    "f1 = open(output_url+'/summary_acc.txt','w')\n",
    "f1.write('\\n'.join(summary))\n",
    "f1.close()    \n",
    "\n",
    "\n",
    "f1 = open(output_url+'/model.txt','w')\n",
    "f1.write(model.extra_repr())\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
