{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.nn.parameter import Parameter\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "dataset = 'PROTEINS2'\n",
    "dataset = './graph_nn-master/graph_nn-master/data/%s'%dataset\n",
    "n_folds = 1  \n",
    "training_size_p = 0.8\n",
    "balance = False\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "lr = 0.001\n",
    "wdecay = 1e-4\n",
    "model_name = 'GAT'\n",
    "#GAT\n",
    "n_hidden = 8\n",
    "dropout_gat = 0.6\n",
    "alpha_leakyReLU = 0.2\n",
    "n_att = 8\n",
    "gat_out_dim = 8\n",
    "#information pooling\n",
    "pooling_method = 'max,sum,mean' #max,sum,mean\n",
    "#fc\n",
    "n_hidden_fc = '32,16' # 'None' or '32,16'\n",
    "if n_hidden_fc != 'None':\n",
    "    n_hidden_fc = list(map(int,n_hidden_fc.strip().split(',')))\n",
    "fc_activation = eval('nn.'+'ReLU'+'(inplace=True)')#'ELU' #'ReLU' #'Identity'    \n",
    "dropout_fc = 0.3\n",
    "fc_bias = True\n",
    "\n",
    "#device\n",
    "device = 'cpu'  # 'cuda', 'cpu'\n",
    "seed = 'Random'\n",
    "threads = 0 #線程數目\n",
    "log_interval = 1\n",
    "\n",
    "#output folder\n",
    "output_folder = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader and reader\n",
    "class GraphData(torch.utils.data.Dataset):\n",
    "    def __init__(self,datareader,fold_id,split):\n",
    "        self.fold_id = fold_id #預設0，只執行一次\n",
    "        self.split = split #\"train\" or \"test\"\n",
    "        self.rnd_state = datareader.rnd_state\n",
    "        self.set_fold(datareader.data, fold_id) #利用方法，建立屬性。set_fold()在下面\n",
    "        \n",
    "        \n",
    "    def set_fold(self, data, fold_id):\n",
    "        self.total = len(data['labels']) #graph數目\n",
    "        self.N_nodes_max = data['N_nodes_max'] #最多node的graph之node數目\n",
    "        self.n_classes = data['n_classes'] #graph分類的種類數目\n",
    "        self.features_dim = data['features_dim'] #node的feature數目\n",
    "        self.idx = data['splits'][fold_id][self.split]#train或test的index\n",
    "        self.labels = copy.deepcopy([data['labels'][i] for i in self.idx])#特定index(train or test)下的graph labels\n",
    "        self.adj_list = copy.deepcopy([data['adj_list'][i] for i in self.idx])#特定index(train or test)下的A矩陣\n",
    "        self.features_onehot = copy.deepcopy([data['features_onehot'][i] for i in self.idx])#特定index(train or test)下的node feature\n",
    "        print('%s: %d/%d' % (self.split.upper(), len(self.labels), len(data['labels'])))\n",
    "        self.indices = np.arange(len(self.idx))  # sample indices for this epoch(for這次epoch，index從新編碼)\n",
    "        self.label_to_target = data['label_to_target']\n",
    "        self.node_idx_to_id = data['node_idx_to_id']\n",
    "        self.targets = data['targets']\n",
    "        \n",
    "    def pad(self, mtx, desired_dim1, desired_dim2=None, value=0):\n",
    "        sz = mtx.shape\n",
    "        assert len(sz) == 2, ('only 2d arrays are supported', sz)\n",
    "        if desired_dim2 is not None:\n",
    "            mtx = np.pad(mtx, ((0, desired_dim1 - sz[0]), (0, desired_dim2 - sz[1])), 'constant', constant_values=value)\n",
    "        else:\n",
    "            mtx = np.pad(mtx, ((0, desired_dim1 - sz[0]), (0, 0)), 'constant', constant_values=value)\n",
    "        return mtx\n",
    "    \n",
    "    def nested_list_to_torch(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            keys = list(data.keys())           \n",
    "        for i in range(len(data)):\n",
    "            if isinstance(data, dict):\n",
    "                i = keys[i]\n",
    "            if isinstance(data[i], np.ndarray):\n",
    "                data[i] = torch.from_numpy(data[i]).float()\n",
    "            elif isinstance(data[i], list):\n",
    "                data[i] = list_to_torch(data[i])\n",
    "        return data\n",
    "        \n",
    "    def __len__(self): #__len__:未來可以len(類別)，呼叫下面code\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):#__getitem__:未來這個類別可以使用[]索引，來完成下面code\n",
    "        index = self.indices[index]\n",
    "        N_nodes_max = self.N_nodes_max\n",
    "        N_nodes = self.adj_list[index].shape[0]\n",
    "        graph_support = np.zeros(self.N_nodes_max)\n",
    "        graph_support[:N_nodes] = 1\n",
    "        #1.把features捕到620,預設補0\n",
    "        #2.把adj補到620*620,預設補0\n",
    "        #3.graph_support: mask\n",
    "        #4.每個圖的真正nodes數\n",
    "        return self.nested_list_to_torch([self.pad(self.features_onehot[index].copy(), self.N_nodes_max),  # node_features\n",
    "                                          self.pad(self.adj_list[index], self.N_nodes_max, self.N_nodes_max),  # adjacency matrix\n",
    "                                          graph_support,  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1 \n",
    "                                          N_nodes,\n",
    "                                          int(self.labels[index]),\n",
    "                                          self.idx[index]])  # convert to torch\n",
    "\n",
    "class DataReader():\n",
    "    def __init__(self,\n",
    "                 data_dir, \n",
    "                 rnd_state=None, \n",
    "                 training_size_p=None,\n",
    "                 folds=None,\n",
    "                 balance=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.rnd_state = np.random.RandomState() if rnd_state == 'Random' else np.random.RandomState(int(rnd_state))\n",
    "        \n",
    "        files = os.listdir(self.data_dir)\n",
    "        \n",
    "        #data starage!\n",
    "        data = {}\n",
    "        #1. nodes:為dict，{node_id:graph_id}\n",
    "        #2. graphs:為dict,{graph_id:np.array([node_id 1,node_id 2,...])}\n",
    "        nodes, graphs = self.read_graph_nodes_relations(list(filter(lambda f: f.find('graph_indicator') >= 0, files))[0])\n",
    "        #3. data['node_id_to_idx']\n",
    "        node_id_to_idx, node_idx_to_id= self.read_node_ID(list(filter(lambda f: f.find('node_features') >= 0, files))[0])\n",
    "        data['node_id_to_idx'] = node_id_to_idx\n",
    "        data['node_idx_to_id'] = node_idx_to_id\n",
    "        #4. data['features_onehot']\n",
    "        data['features_onehot'] = self.read_node_features(list(filter(lambda f: f.find('node_features') >= 0, files))[0], nodes, graphs)  \n",
    "        #data['adj_list']\n",
    "        data['adj_list'] = self.read_graph_adj(list(filter(lambda f: f.find('_A') >= 0, files))[0], nodes, graphs,node_id_to_idx) \n",
    "        #data['labels'] 0開始\n",
    "        target_to_label = {}\n",
    "        label_to_target = {}\n",
    "        targets = np.array(self.parse_txt_file(list(filter(lambda f: f.find('graph_labels') >= 0, files))[0], \n",
    "                                      line_parse_fn=lambda s: s.strip()))\n",
    "        data['targets'] = targets\n",
    "        target_category = sorted(list(set(targets)))\n",
    "        for l, t in enumerate(target_category): \n",
    "            target_to_label[t] = l\n",
    "            label_to_target[l] = t\n",
    "        data['labels'] = np.array([target_to_label[t] for t in targets])\n",
    "        data['target_to_label'] = target_to_label\n",
    "        data['label_to_target'] = label_to_target\n",
    "        n_edges, degrees = [], []\n",
    "        for sample_id, adj in enumerate(data['adj_list']):\n",
    "            N = len(adj)  # number of nodes\n",
    "            n = np.sum(adj)  # total sum of edges\n",
    "            n_edges.append(int(n/2))  # undirected edges, so need to divide by 2\n",
    "            degrees.extend(list(np.sum(adj, 1)))\n",
    "        features_dim = len(data['features_onehot'][0][0])\n",
    "        shapes = [len(adj) for adj in data['adj_list']]\n",
    "        N_nodes_max = np.max(shapes)\n",
    "        classes = target_category\n",
    "        n_classes = len(target_category)\n",
    "\n",
    "        print('N nodes avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(shapes), np.std(shapes), np.min(shapes), np.max(shapes)))\n",
    "        print('N edges avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(n_edges), np.std(n_edges), np.min(n_edges), np.max(n_edges)))\n",
    "        print('Node degree avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(degrees), np.std(degrees), np.min(degrees), np.max(degrees)))\n",
    "        print('Node features dim: \\t\\t%d' % features_dim)\n",
    "        print('N classes: \\t\\t\\t%d' % n_classes)\n",
    "        print('Classes: \\t\\t\\t%s' %(', '.join(classes)))\n",
    "        for lbl in classes:\n",
    "            print('Class %s: \\t\\t\\t%s samples' % (lbl, np.sum(targets == lbl)))\n",
    "        #判斷每個資料中，graph數量是否相等\n",
    "        N_graphs = len(data['labels']) \n",
    "        assert N_graphs == len(data['adj_list']) == len(data['features_onehot']), 'invalid data'\n",
    "\n",
    "        train_ids, test_ids = self.split_ids(data['labels'], rnd_state=self.rnd_state, training_size_p=training_size_p,\n",
    "                                             folds=n_folds, balance=balance)\n",
    "        splits = [] #塞入dict('train':[index...],'test':[index...])\n",
    "        for fold in range(folds):\n",
    "            splits.append({'train': train_ids[fold],\n",
    "                           'test': test_ids[fold]})\n",
    "        \n",
    "        data['splits'] = splits #folds份的train和test之index\n",
    "        data['N_nodes_max'] = N_nodes_max\n",
    "        data['features_dim'] = features_dim\n",
    "        data['n_classes'] = n_classes #graph label種類數目\n",
    "        \n",
    "        self.data = data # data為一個dict()\n",
    "\n",
    "    def split_ids(self, labels_all, rnd_state=None,folds=1, training_size_p=None, balance=False):\n",
    "        if folds == 1:\n",
    "            if balance == True:\n",
    "                classes = list(set(labels_all))\n",
    "                classes_dict = dict()\n",
    "                for i in classes:\n",
    "                    classes_dict[i] = []\n",
    "                for idx,l in enumerate(labels_all):\n",
    "                    classes_dict[l].append(idx)\n",
    "                min_classes_n = len(labels_all)\n",
    "                for i in classes:\n",
    "                    if len(classes_dict[i]) < min_classes_n:\n",
    "                        min_classes_n = len(classes_dict[i])\n",
    "                training_size_per_class = int(np.round(min_classes_n*training_size_p))\n",
    "                ids_all = np.arange(len(labels_all))\n",
    "                ids = ids_all[rnd_state.permutation(len(ids_all))]\n",
    "                train_ids = []\n",
    "                for i in classes:\n",
    "                    class_ls = np.array(classes_dict[i])\n",
    "                    sampling = class_ls[rnd_state.permutation(len(class_ls))][0:training_size_per_class]\n",
    "\n",
    "                    train_ids.extend(sampling)\n",
    "                test_ids = [np.array([e for e in ids if e not in train_ids])]    \n",
    "                train_ids = [np.array(train_ids)]\n",
    "            else:\n",
    "                ids_all = np.arange(len(labels_all))\n",
    "                n = len(ids_all) #n:graph的數目\n",
    "                ids = ids_all[rnd_state.permutation(n)]\n",
    "                testing_size = int(np.round(n*(1-training_size_p)))\n",
    "                test_ids = ids[0:testing_size] # 包著np.array()\n",
    "                train_ids = [np.array([e for e in ids if e not in test_ids])] # 包著np.array()\n",
    "                test_ids = [test_ids]\n",
    "        elif folds > 1:\n",
    "            ids_all = np.arange(len(labels_all))\n",
    "            n = len(ids_all)\n",
    "            ids = ids_all[rnd_state.permutation(n)]\n",
    "            stride = int(np.ceil(n / float(folds)))\n",
    "            test_ids = [ids[i: i + stride] for i in range(0, n, stride)]\n",
    "            assert np.all(np.unique(np.concatenate(test_ids)) == sorted(ids_all)), 'some graphs are missing in the test sets'\n",
    "            assert len(test_ids) == folds, 'invalid test sets'\n",
    "            train_ids = []\n",
    "            for fold in range(folds):\n",
    "                train_ids.append(np.array([e for e in ids if e not in test_ids[fold]]))\n",
    "                assert len(train_ids[fold]) + len(test_ids[fold]) == len(np.unique(list(train_ids[fold]) + list(test_ids[fold]))) == n, 'invalid splits'\n",
    "\n",
    "        return train_ids, test_ids\n",
    "\n",
    "    def parse_txt_file(self, fpath, line_parse_fn=None):\n",
    "        #pjoin=os.path.join:路徑拼接\n",
    "        #os.path.join([PATH_1], [PATH_2], [PATH_3], ...)-->return:[PATH_1]/[PATH_2]/[PATH_3]\n",
    "        with open(pjoin(self.data_dir, fpath), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        #if line_parse_fn is not None else s:代表如果有處理字串函數就執行，否則就保留原本的樣子\n",
    "        data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]\n",
    "        return data\n",
    "    \n",
    "    def read_graph_adj(self, fpath, nodes, graphs, node_id_to_idx):\n",
    "        def fn_read_graph_adj(s):\n",
    "            if ',' in s:\n",
    "                return s.strip().split(',')\n",
    "            else:\n",
    "                return s.strip().split()\n",
    "        edges = self.parse_txt_file(fpath, line_parse_fn=fn_read_graph_adj)\n",
    "        adj_dict = {}\n",
    "        for edge in edges:\n",
    "            node1 = node_id_to_idx[edge[0].strip()]\n",
    "            node2 = node_id_to_idx[edge[1].strip()]\n",
    "            graph_id = nodes[node1]\n",
    "            assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])\n",
    "            \n",
    "            if graph_id not in adj_dict:\n",
    "                n = len(graphs[graph_id])\n",
    "                adj_dict[graph_id] = np.zeros((n, n))\n",
    "            ind1 = np.where(graphs[graph_id] == node1)[0]\n",
    "            ind2 = np.where(graphs[graph_id] == node2)[0]\n",
    "            assert len(ind1) == len(ind2) == 1, (ind1, ind2)\n",
    "            adj_dict[graph_id][ind1, ind2] = 1\n",
    "            adj_dict[graph_id][ind2, ind1] = 1\n",
    "        adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]        \n",
    "        return adj_list\n",
    "        \n",
    "    #graph_indicator\n",
    "    def read_graph_nodes_relations(self, fpath):\n",
    "        #node從0開始\n",
    "        #graph沒限定，但要是整數\n",
    "        graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))\n",
    "        nodes, graphs = {}, {}\n",
    "        for node_id, graph_id in enumerate(graph_ids):\n",
    "            if graph_id not in graphs:\n",
    "                graphs[graph_id] = []\n",
    "            graphs[graph_id].append(node_id)\n",
    "            nodes[node_id] = graph_id\n",
    "        graph_ids = np.unique(list(graphs.keys()))\n",
    "        for graph_id in graphs:\n",
    "            graphs[graph_id] = np.array(graphs[graph_id])\n",
    "        return nodes, graphs\n",
    "\n",
    "    def read_node_features(self, fpath, nodes, graphs):\n",
    "        def fn_read_node_features(s):\n",
    "            if ',' in s:\n",
    "                return list(map(float,(s.strip().split(',')[1:])))\n",
    "            else:\n",
    "                return list(map(float,(s.strip().split()[1:])))\n",
    "        node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn_read_node_features)\n",
    "        node_features = {}\n",
    "        #node_features:資料格式和graphs相似\n",
    "        for node_id, x in enumerate(node_features_all):\n",
    "            graph_id = nodes[node_id]\n",
    "            if graph_id not in node_features:\n",
    "                node_features[graph_id] = [ None ] * len(graphs[graph_id])\n",
    "            ind = np.where(graphs[graph_id] == node_id)[0]\n",
    "            #assert 判斷式, 如果有誤回傳的內容\n",
    "            assert len(ind) == 1, ind\n",
    "            assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]\n",
    "            node_features[graph_id][ind[0]] = x\n",
    "        node_features_lst = [np.array(node_features[graph_id]) for graph_id in sorted(list(graphs.keys()))]\n",
    "        return node_features_lst\n",
    "    \n",
    "    def read_node_ID(self, fpath):\n",
    "        def fn_read_node_ID(s):\n",
    "            if ',' in s:\n",
    "                return s.strip().split(',')[0]\n",
    "            else:\n",
    "                return s.strip().split()[0]\n",
    "        node_ID_all = self.parse_txt_file(fpath, line_parse_fn=fn_read_node_ID)\n",
    "        assert len(node_ID_all) == len(set(node_ID_all))\n",
    "        \n",
    "        node_id_to_idx = {}#str:int\n",
    "        node_idx_to_id = {}\n",
    "        for node_idx, node_id in enumerate(node_ID_all):\n",
    "            node_id_to_idx[node_id] = node_idx\n",
    "            node_idx_to_id[node_idx] = node_id\n",
    "        return node_id_to_idx, node_idx_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.W = nn.Linear(in_features=in_features, out_features=out_features,bias=False)\n",
    "        self.a = nn.Linear(in_features=2*out_features, out_features=1,bias=False)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.concatenate = torch.cat\n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "    def forward(self, data):\n",
    "        inp, adj = data[:2]\n",
    "        h = self.W(inp)\n",
    "        n_batch = h.size()[0]\n",
    "        N = h.size()[1] #node size\n",
    "        a_inp = self.concatenate([h.repeat(1,1,N).view(n_batch,N*N,-1),h.repeat(1,N,1)],dim=1).view(n_batch,N,-1,2*self.out_features)\n",
    "        e = self.leakyrelu(self.a(a_inp)).squeeze(3) #shape=[batch size, node size, node size]\n",
    "        \n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        I = torch.eye(N).unsqueeze(0).to(device)\n",
    "        adj_I = adj+I\n",
    "        attention = torch.where(adj_I>0, e, zero_vec) #沒鄰接:負很大，有鄰接:e\n",
    "        attention = F.softmax(attention, dim=2) #負很大->softmax->0\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        if self.concat:\n",
    "            return self.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime       \n",
    "    def extra_repr(self):\n",
    "        lines = []\n",
    "        lines.append('(hidden_features): Linear(in_features=%s, out_features=%s, bias=False)'%(self.in_features,self.out_features))\n",
    "        lines.append('(attetion): Attetion(')\n",
    "        lines.append('  (concat_ij): Concat(in_features=%s, out_features=%s*2)'%(self.out_features,self.out_features))\n",
    "        lines.append('  (a): Linear(in_features=%s, out_features=1, bias=False)'%(self.out_features*2))\n",
    "        lines.append('  (leakyrelu): LeakyReLU(negative_slope=%s)'%(self.alpha))\n",
    "        lines.append('  (concat_edges): Concat(in_features=1, out_features=1-hop edges)')\n",
    "        lines.append('  (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)')\n",
    "        lines.append('  (dropout_attetion): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append(')')\n",
    "        lines.append('(weighted_hidden_features): Matmul(attention, hidden_features)')\n",
    "        if self.concat:\n",
    "            lines.append('(elu): ELU(alpha=1.0)')        \n",
    "        lines = '\\n'.join(lines)\n",
    "        return lines\n",
    "        \n",
    "\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 nfeat, \n",
    "                 nhid, \n",
    "                 nclass, \n",
    "                 dropout, \n",
    "                 alpha, \n",
    "                 nheads,\n",
    "                 gat_out_dim,\n",
    "                 pooling_method,\n",
    "                 n_hidden_fc, \n",
    "                 dropout_fc,\n",
    "                 fc_bias,\n",
    "                 fc_activation):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.nhid = nhid\n",
    "        self.nheads = nheads\n",
    "        self.nclass = nclass\n",
    "        self.pooling_method = pooling_method\n",
    "        self.n_hidden_fc = n_hidden_fc\n",
    "        self.dropout_fc = dropout_fc\n",
    "        self.fc_bias = fc_bias\n",
    "        self.fc_activation = fc_activation\n",
    "        \n",
    "        #GAT\n",
    "        self.attentions = [GraphAttentionLayer(in_features=nfeat,\n",
    "                                               out_features=nhid, \n",
    "                                               dropout=dropout, \n",
    "                                               alpha=alpha, \n",
    "                                               concat=True) for _ in range(nheads)]     \n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, gat_out_dim, dropout=dropout, alpha=alpha, concat=False)\n",
    "        \n",
    "        #fc\n",
    "        fc = []\n",
    "        pooling_noumber = len(pooling_method.split(','))      \n",
    "        if n_hidden_fc != 'None':\n",
    "            for layer, f in enumerate(n_hidden_fc):\n",
    "                if dropout_fc > 0:\n",
    "                    fc.append(nn.Dropout(p=dropout_fc))\n",
    "                else:\n",
    "                    fc.append(nn.Identity())\n",
    "                if layer == 0:\n",
    "                    fc.append(nn.Linear(gat_out_dim*pooling_noumber, n_hidden_fc[layer], bias=fc_bias)) \n",
    "                    fc.append(fc_activation)\n",
    "                else:\n",
    "                    fc.append(nn.Linear(n_hidden_fc[layer-1], n_hidden_fc[layer], bias=fc_bias))   \n",
    "                    fc.append(fc_activation)\n",
    "            n_last = n_hidden_fc[-1]\n",
    "        else:\n",
    "            n_last = gat_out_dim*pooling_noumber\n",
    "            \n",
    "        #last layer\n",
    "        if dropout_fc > 0:\n",
    "            fc.append(nn.Dropout(p=dropout_fc))\n",
    "        else:\n",
    "            fc.append(nn.Identity())            \n",
    "        fc.append(nn.Linear(n_last, nclass, bias=fc_bias))\n",
    "        self.fc = nn.Sequential(*fc) \n",
    "                \n",
    "        \n",
    "    def forward(self, data):\n",
    "        mask = data[2].clone()\n",
    "        N_nodes = torch.sum(mask, dim=1).reshape(len(torch.sum(mask, dim=1)),1)        \n",
    "        x, adj = data[:2]\n",
    "                      \n",
    "        #GAT\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att((x, adj)) for att in self.attentions], dim=2)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att((x, adj)))\n",
    "        \n",
    "        #pooling\n",
    "        pooling_ls = []\n",
    "        if 'max' in pooling_method:\n",
    "            max_pooling = torch.max(x, 1)[0]\n",
    "            pooling_ls.append(max_pooling)\n",
    "        if 'sum' in pooling_method:\n",
    "            sum_pooling = torch.sum(x, 1)\n",
    "            pooling_ls.append(sum_pooling)\n",
    "        if 'mean' in pooling_method:\n",
    "            mean_pooling = torch.sum(x, 1)/N_nodes\n",
    "            pooling_ls.append(mean_pooling)\n",
    "        x = torch.cat(pooling_ls,1)  \n",
    "        \n",
    "        #fc\n",
    "        x = self.fc(x) \n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x    \n",
    "    \n",
    "    def extra_repr(self):\n",
    "        lines = []\n",
    "        lines.append('GAT(')\n",
    "        lines.append('  (dropout): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append('  ->')\n",
    "        for i, att in enumerate(self.attentions):\n",
    "            lines.append('  (attention_%d): GraphAttentionLayer('%(i))\n",
    "            for j in att.extra_repr().split('\\n'):\n",
    "                lines.append('    '+j)\n",
    "            lines.append('  )')\n",
    "        lines.append('  ->')             \n",
    "        lines.append('  (concat_attention_0-%s): Concat(in_features=%s, out_features=%s*%slayers)'%(self.nheads-1,\n",
    "                                                                                    self.nhid, self.nhid, self.nheads))\n",
    "        lines.append('  ->')               \n",
    "        lines.append('  (dropout): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append('  ->')  \n",
    "        lines.append('  (attention_out): GraphAttentionLayer(')\n",
    "        for j in self.out_att.extra_repr().split('\\n'):\n",
    "            lines.append('    '+j)\n",
    "        lines.append('  )')\n",
    "        lines.append('  ->')\n",
    "        lines.append('  (elu): ELU(alpha=1.0)')  \n",
    "        lines.append(')')\n",
    "        lines.append('->')\n",
    "        lines.append('Pooling(')\n",
    "        lines.append('  (concat): Concat(%s)'%self.pooling_method)\n",
    "        lines.append(')')\n",
    "        lines.append('->')\n",
    "        lines.append('FullyConnected(')\n",
    "        for i in model.fc:\n",
    "            i = str(i)\n",
    "            if i[0]=='D':\n",
    "                i = '  (dropout): ' + i\n",
    "            elif i[0]=='L':\n",
    "                i = '  (fc): ' + i\n",
    "            elif i[0]=='R':\n",
    "                i = '  (activation): ' + i     \n",
    "            lines.append(i)    \n",
    "        lines.append('  (softmax): Softmax(in_features=%s, out_features=%s)'%(self.nclass,self.nclass))\n",
    "        lines.append(')')    \n",
    "        lines='\\n'.join(lines)\n",
    "        \n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "N nodes avg/std/min/max: \t39.06/45.76/4/620\n",
      "N edges avg/std/min/max: \t72.82/84.60/5/1049\n",
      "Node degree avg/std/min/max: \t3.73/1.15/0/25\n",
      "Node features dim: \t\t3\n",
      "N classes: \t\t\t2\n",
      "Classes: \t\t\t1, 2\n",
      "Class 1: \t\t\t663 samples\n",
      "Class 2: \t\t\t450 samples\n",
      "\n",
      "FOLD 1\n",
      "TRAIN: 890/1113\n",
      "TEST: 223/1113\n",
      "\n",
      "Initialize model\n",
      "GAT(\n",
      "  (dropout): Dropout(p=0.6)\n",
      "  ->\n",
      "  (attention_0): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_1): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_2): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_3): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_4): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_5): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_6): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_7): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=3, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  ->\n",
      "  (concat_attention_0-7): Concat(in_features=8, out_features=8*8layers)\n",
      "  ->\n",
      "  (dropout): Dropout(p=0.6)\n",
      "  ->\n",
      "  (attention_out): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=64, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "  )\n",
      "  ->\n",
      "  (elu): ELU(alpha=1.0)\n",
      ")\n",
      "->\n",
      "Pooling(\n",
      "  (concat): Concat(max,sum,mean)\n",
      ")\n",
      "->\n",
      "FullyConnected(\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=24, out_features=32, bias=True)\n",
      "  (activation): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (activation): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (softmax): Softmax(in_features=2, out_features=2)\n",
      ")\n",
      "N trainable parameters: 2210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chienhua\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [64/890 (7%)]\tLoss: 0.6393(avg: 0.6393)\tAcc: 62.50%(40/64) \tsec/iter: 64.7780\n",
      "Train Epoch: 0 [128/890 (14%)]\tLoss: 0.6599(avg: 0.6496)\tAcc: 60.94%(78/128) \tsec/iter: 65.1682\n",
      "Train Epoch: 0 [192/890 (21%)]\tLoss: 0.6500(avg: 0.6497)\tAcc: 58.33%(112/192) \tsec/iter: 64.7179\n",
      "Train Epoch: 0 [256/890 (29%)]\tLoss: 0.6864(avg: 0.6589)\tAcc: 59.77%(153/256) \tsec/iter: 64.6353\n",
      "Train Epoch: 0 [320/890 (36%)]\tLoss: 0.7546(avg: 0.6780)\tAcc: 57.81%(185/320) \tsec/iter: 64.3954\n",
      "Train Epoch: 0 [384/890 (43%)]\tLoss: 0.6941(avg: 0.6807)\tAcc: 57.81%(222/384) \tsec/iter: 64.2101\n"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "\n",
    "datareader = DataReader(data_dir=dataset, \n",
    "                        rnd_state=seed,training_size_p=training_size_p,folds=n_folds,balance=balance)\n",
    "\n",
    "train_acc_folds = []\n",
    "test_acc_folds = []\n",
    "for fold_id in range(n_folds):\n",
    "    print('\\nFOLD', fold_id+1)\n",
    "    loaders = []\n",
    "    for split in ['train', 'test']:\n",
    "        #製作\"train\"或\"test\" graph data\n",
    "        gdata = GraphData(fold_id=fold_id, datareader=datareader, split=split)\n",
    "        loader = torch.utils.data.DataLoader(gdata, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=split.find('train') >= 0,\n",
    "                                             num_workers=threads)\n",
    "        loaders.append(loader)\n",
    "        if split == 'train':\n",
    "            training_size = len(gdata.idx)\n",
    "    if model_name == 'GAT':\n",
    "        model = GAT(nfeat=loaders[0].dataset.features_dim,\n",
    "                    nhid=n_hidden,\n",
    "                    nclass=loaders[0].dataset.n_classes,\n",
    "                    dropout=dropout_gat,\n",
    "                    alpha=alpha_leakyReLU,\n",
    "                    nheads=n_att,\n",
    "                    gat_out_dim=gat_out_dim,\n",
    "                    pooling_method=pooling_method,\n",
    "                    n_hidden_fc=n_hidden_fc, \n",
    "                    dropout_fc=dropout_fc,\n",
    "                    fc_bias=fc_bias,\n",
    "                    fc_activation=fc_activation).to(device)    \n",
    "\n",
    "    print('\\nInitialize model')\n",
    "    print(model.extra_repr())\n",
    "    c = 0\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        c += p.numel()\n",
    "    print('N trainable parameters:', c)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=lr,\n",
    "                weight_decay=wdecay,\n",
    "                betas=(0.5, 0.999))\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.1)\n",
    "\n",
    "\n",
    "    def train(train_loader):\n",
    "        scheduler.step()#每個batch就會改變學習率\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        train_loss, correct, n_samples = 0, 0, 0\n",
    "        train_loss_batch_ls = []\n",
    "        train_acc_batch_ls = []\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)     \n",
    "            loss = loss_fn(output, data[4])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            time_iter = time.time() - start\n",
    "            train_loss += loss.item() * len(output)\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "            acc = 100. * correct / n_samples\n",
    "            train_loss_batch_ls.append(train_loss/n_samples)\n",
    "            train_acc_batch_ls.append(acc/100)\n",
    "            if batch_idx % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}(avg: {:.4f})\\tAcc: {:.2f}%({}/{}) \\tsec/iter: {:.4f}'.format(\n",
    "                    epoch, n_samples, len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, \n",
    "                    acc, correct, n_samples, time_iter / (batch_idx + 1) ))    \n",
    "        return train_loss_batch_ls, train_acc_batch_ls\n",
    "    def test(test_loader):\n",
    "        model.eval()\n",
    "        start = time.time()\n",
    "        test_loss, correct, n_samples = 0, 0, 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4], reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "        time_iter = time.time() - start\n",
    "        test_loss /= n_samples\n",
    "        acc = 100. * correct / n_samples\n",
    "        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch+1, \n",
    "                                                                                              test_loss, \n",
    "                                                                                              correct, \n",
    "                                                                                              n_samples, acc))\n",
    "        return test_loss,acc/100\n",
    "\n",
    "    def predict(loader_full):\n",
    "        idx_ls = []\n",
    "        pred_ls = [] \n",
    "        label_ls = []\n",
    "        length_ls = []\n",
    "        output_ls = []\n",
    "        print('[Trained Model]')\n",
    "        for i in [0,1]:\n",
    "            model.eval()     \n",
    "            pred_tmp = []\n",
    "            label_tmp = []\n",
    "            for batch_idx, data in enumerate(loader_full[i]):\n",
    "                for j in range(len(data)):\n",
    "                    data[j] = data[j].to(device)\n",
    "                output = model(data)\n",
    "                idx_ls.extend(data[5].tolist())\n",
    "                pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "                pred_ls.extend(pred.reshape(pred.shape[0]).tolist())\n",
    "                label_ls.extend(data[4].tolist())\n",
    "                output_ls.extend(output)\n",
    "                pred_tmp.extend(pred.reshape(pred.shape[0]).tolist())\n",
    "                label_tmp.extend(data[4].tolist())\n",
    "            total = len(pred_tmp)\n",
    "            c = sum(np.array(pred_tmp)==np.array(label_tmp)) \n",
    "            if i==0:\n",
    "                print('Training Set: Accuracy=%.2f%%(%s/%s)'%(c*100/total,c,total))\n",
    "                train_acc_folds.append(c*100/total)\n",
    "            elif i==1:\n",
    "                print('Testing Set: Accuracy=%.2f%%(%s/%s)'%(c*100/total,c,total))  \n",
    "                test_acc_folds.append(c*100/total)\n",
    "            length_ls.append(total)\n",
    "        return idx_ls, pred_ls, label_ls, length_ls, output_ls\n",
    "\n",
    "    train_loss_ls = []\n",
    "    train_acc_ls = []\n",
    "    test_loss_ls = []\n",
    "    test_acc_ls = []\n",
    "    loss_fn = F.nll_loss\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(loaders[0])\n",
    "        test_loss, test_acc = test(loaders[1])\n",
    "        train_loss_ls.extend(train_loss)\n",
    "        train_acc_ls.extend(train_acc)\n",
    "        test_loss_ls.append(test_loss)\n",
    "        test_acc_ls.append(test_acc)   \n",
    "\n",
    "    idx_ls, pred_ls, label_ls, length_ls, output_ls = predict(loaders)\n",
    "\n",
    "    #plot\n",
    "    length_train = range(len(train_loss_ls))\n",
    "    length_test = range(int(np.ceil(training_size/batch_size))-1,len(train_loss_ls),int(np.ceil(training_size/batch_size)))\n",
    "    plt.plot(length_train,train_acc_ls,label='training accuracy')\n",
    "    plt.plot(length_test,test_acc_ls,label='validation accuracy')\n",
    "    x_ticks = [0]+list(length_test)\n",
    "    plt.xticks(x_ticks,list(range(0,epochs+1)))\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('accuracy',fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_folder+'/acc_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(length_train,train_loss_ls,label='training loss')\n",
    "    plt.plot(length_test,test_loss_ls,label='validation loss')\n",
    "    plt.xticks(x_ticks,list(range(0,epochs+1)))\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('loss',fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_folder+'/loss_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
