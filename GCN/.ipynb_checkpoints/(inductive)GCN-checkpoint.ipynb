{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.nn.parameter import Parameter\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "dataset = 'PROTEINS2'\n",
    "dataset = './graph_nn-master/graph_nn-master/data/%s'%dataset\n",
    "n_folds = 1  \n",
    "training_size_p = 0.8\n",
    "balance = False\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "lr = 0.001\n",
    "wdecay = 1e-4\n",
    "#Graph Convolution Layer\n",
    "model_name = 'GCN'\n",
    "filters_gcn = [64,64,64]\n",
    "graph_kernel = 'Normalized-Laplacian' #'Normalized-Laplacian' #Normalization #Cluster-GCN\n",
    "connection = 'None' #'residual connection' #dense connection #'None'\n",
    "gcn_activation = eval('nn.'+'ReLU'+'(inplace=True)')#'ELU' #'ReLU' \n",
    "dropout_gcn = 0.3\n",
    "gcn_bias = True\n",
    "#information pooling\n",
    "pooling_method = 'max' #max,sum,mean\n",
    "#fc\n",
    "n_hidden_fc = 'None' # 'None' or '32,16'\n",
    "if n_hidden_fc != 'None':\n",
    "    n_hidden_fc = list(map(int,n_hidden_fc.strip().split(',')))\n",
    "fc_activation = eval('nn.'+'ReLU'+'(inplace=True)')#'ELU' #'ReLU' #'Identity'    \n",
    "dropout_fc = 0.3\n",
    "fc_bias = True\n",
    "\n",
    "#device\n",
    "device = 'cpu'  # 'cuda', 'cpu'\n",
    "seed = 'Random'\n",
    "threads = 0 #線程數目\n",
    "log_interval = 10\n",
    "\n",
    "#output folder\n",
    "output_folder = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN layers and models\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_features,\n",
    "                out_features,\n",
    "                activation=None,\n",
    "                dropout_gcn=0,\n",
    "                gcn_bias=True,\n",
    "                gconv_order=None):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=in_features, out_features=out_features,bias=gcn_bias)\n",
    "        self.activation = activation\n",
    "        self.drop = nn.Dropout(p=dropout_gcn) if dropout_gcn > 0.0 else nn.Identity()\n",
    "        self.gconv_order = gconv_order\n",
    "        \n",
    "    def L_batch(self, A):\n",
    "        batch, N = A.shape[:2]\n",
    "        I = torch.eye(N).unsqueeze(0).to(device)\n",
    "        A_hat = A + I \n",
    "        if graph_kernel == 'Normalized-Laplacian':\n",
    "            D_hat = (torch.sum(A_hat, 1)) ** (-0.5)\n",
    "            L = D_hat.view(batch, N, 1) * A_hat * D_hat.view(batch, 1, N)\n",
    "        elif graph_kernel == 'Normalization':\n",
    "            D_hat = (torch.sum(A_hat, 1)) ** (-1)\n",
    "            L = D_hat.view(batch, N, 1) * A_hat \n",
    "        elif graph_kernel == 'Cluster-GCN':\n",
    "            D_hat = torch.sum(A_hat, 1)**(-1)\n",
    "            A_hat = D_hat.view(batch, N, 1) * A_hat\n",
    "            for i in range(len(A_hat)):\n",
    "                A_hat[i] = A_hat[i]+torch.diagflat(torch.diag(A_hat[i]))\n",
    "            L = A_hat\n",
    "        return L\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, A = data[:2]\n",
    "        x_res = data[0]\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(torch.bmm(self.L_batch(A), x))\n",
    "        x = self.activation(x) \n",
    "        if (connection == 'residual connection')&(self.gconv_order != 0):\n",
    "            assert x.shape == x_res.shape, \"'residual connection' was set,\" +\\\n",
    "                                           \"hyper-parameter 'filters_gcn' should be set in same dimension.\\n\" +\\\n",
    "                                           \"filters_gcn you set is %s.\"%filters_gcn +\\\n",
    "                                           \"try to use %s.\"%' or '.join([str([i]*len(filters_gcn)) for i in set(filters_gcn)])\n",
    "            x = x + x_res\n",
    "        if (connection == 'dense connection')&(self.gconv_order != 0):\n",
    "            x = torch.cat([x,x_res],2)\n",
    "        return (x, A)\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 filters_gcn=[64,64,64],\n",
    "                 dropout_gcn=0,\n",
    "                 n_hidden_fc=[32,16,8],\n",
    "                 dropout_fc=0.2,\n",
    "                 gcn_bias=True,\n",
    "                 gcn_activation=nn.ReLU(inplace=True),\n",
    "                 fc_bias=True,\n",
    "                 fc_activation=None):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # GCN\n",
    "        if (connection == 'dense connection'):\n",
    "            filters_gcn_dense = [sum(filters_gcn[:i+1]) for i in range(len(filters_gcn))]\n",
    "            self.gconv = nn.Sequential(*([GraphConv(in_features=in_features if layer == 0 else filters_gcn_dense[layer - 1], \n",
    "                                                    out_features=f, \n",
    "                                                    activation=gcn_activation,\n",
    "                                                    dropout_gcn=dropout_gcn,\n",
    "                                                    gcn_bias=gcn_bias,\n",
    "                                                    gconv_order=layer) for layer, f in enumerate(filters_gcn)]))\n",
    "        else:\n",
    "            self.gconv = nn.Sequential(*([GraphConv(in_features=in_features if layer == 0 else filters_gcn[layer - 1], \n",
    "                                                    out_features=f, \n",
    "                                                    activation=gcn_activation,\n",
    "                                                    dropout_gcn=dropout_gcn,\n",
    "                                                    gcn_bias=gcn_bias,\n",
    "                                                    gconv_order=layer) for layer, f in enumerate(filters_gcn)]))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc = []\n",
    "        pooling_noumber = len(pooling_method.split(','))\n",
    "        if n_hidden_fc != 'None':\n",
    "            for layer, f in enumerate(n_hidden_fc):\n",
    "                if dropout_fc > 0:\n",
    "                    fc.append(nn.Dropout(p=dropout_fc))\n",
    "                else:\n",
    "                    fc.append(nn.Identity())\n",
    "                if layer == 0:\n",
    "                    if (connection == 'dense connection'):\n",
    "                        fc.append(nn.Linear(filters_gcn_dense[-1]*pooling_noumber, n_hidden_fc[layer], bias=fc_bias)) \n",
    "                    else:\n",
    "                        fc.append(nn.Linear(filters_gcn[-1]*pooling_noumber, n_hidden_fc[layer], bias=fc_bias)) \n",
    "                    fc.append(fc_activation)\n",
    "                else:\n",
    "                    fc.append(nn.Linear(n_hidden_fc[layer-1], n_hidden_fc[layer], bias=fc_bias))   \n",
    "                    fc.append(fc_activation)\n",
    "            n_last = n_hidden_fc[-1]\n",
    "        else:\n",
    "            if (connection == 'dense connection'):\n",
    "                n_last = filters_gcn_dense[-1]*pooling_noumber\n",
    "            else:\n",
    "                n_last = filters_gcn[-1]*pooling_noumber\n",
    "        #last layer\n",
    "        if dropout_fc > 0:\n",
    "            fc.append(nn.Dropout(p=dropout_fc))\n",
    "        else:\n",
    "            fc.append(nn.Identity())            \n",
    "        fc.append(nn.Linear(n_last, out_features, bias=fc_bias))\n",
    "        self.fc = nn.Sequential(*fc) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        mask = data[2].clone()\n",
    "        N_nodes = torch.sum(mask, dim=1).reshape(len(torch.sum(mask, dim=1)),1)\n",
    "        \n",
    "        x = self.gconv(data)[0]\n",
    "        \n",
    "        pooling_ls = []\n",
    "        if 'max' in pooling_method:\n",
    "            max_pooling = torch.max(x, 1)[0]\n",
    "            pooling_ls.append(max_pooling)\n",
    "        if 'sum' in pooling_method:\n",
    "            sum_pooling = torch.sum(x, 1)\n",
    "            pooling_ls.append(sum_pooling)\n",
    "        if 'mean' in pooling_method:\n",
    "            mean_pooling = torch.sum(x, 1)/N_nodes\n",
    "            pooling_ls.append(mean_pooling)\n",
    "        x = torch.cat(pooling_ls,1)  \n",
    "        \n",
    "        x = self.fc(x) \n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader and reader\n",
    "class GraphData(torch.utils.data.Dataset):\n",
    "    def __init__(self,datareader,fold_id,split):\n",
    "        self.fold_id = fold_id #預設0，只執行一次\n",
    "        self.split = split #\"train\" or \"test\"\n",
    "        self.rnd_state = datareader.rnd_state\n",
    "        self.set_fold(datareader.data, fold_id) #利用方法，建立屬性。set_fold()在下面\n",
    "        \n",
    "        \n",
    "    def set_fold(self, data, fold_id):\n",
    "        self.total = len(data['labels']) #graph數目\n",
    "        self.N_nodes_max = data['N_nodes_max'] #最多node的graph之node數目\n",
    "        self.n_classes = data['n_classes'] #graph分類的種類數目\n",
    "        self.features_dim = data['features_dim'] #node的feature數目\n",
    "        self.idx = data['splits'][fold_id][self.split]#train或test的index\n",
    "        self.labels = copy.deepcopy([data['labels'][i] for i in self.idx])#特定index(train or test)下的graph labels\n",
    "        self.adj_list = copy.deepcopy([data['adj_list'][i] for i in self.idx])#特定index(train or test)下的A矩陣\n",
    "        self.features_onehot = copy.deepcopy([data['features_onehot'][i] for i in self.idx])#特定index(train or test)下的node feature\n",
    "        print('%s: %d/%d' % (self.split.upper(), len(self.labels), len(data['labels'])))\n",
    "        self.indices = np.arange(len(self.idx))  # sample indices for this epoch(for這次epoch，index從新編碼)\n",
    "        self.label_to_target = data['label_to_target']\n",
    "        self.node_idx_to_id = data['node_idx_to_id']\n",
    "        self.targets = data['targets']\n",
    "        \n",
    "    def pad(self, mtx, desired_dim1, desired_dim2=None, value=0):\n",
    "        sz = mtx.shape\n",
    "        assert len(sz) == 2, ('only 2d arrays are supported', sz)\n",
    "        if desired_dim2 is not None:\n",
    "            mtx = np.pad(mtx, ((0, desired_dim1 - sz[0]), (0, desired_dim2 - sz[1])), 'constant', constant_values=value)\n",
    "        else:\n",
    "            mtx = np.pad(mtx, ((0, desired_dim1 - sz[0]), (0, 0)), 'constant', constant_values=value)\n",
    "        return mtx\n",
    "    \n",
    "    def nested_list_to_torch(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            keys = list(data.keys())           \n",
    "        for i in range(len(data)):\n",
    "            if isinstance(data, dict):\n",
    "                i = keys[i]\n",
    "            if isinstance(data[i], np.ndarray):\n",
    "                data[i] = torch.from_numpy(data[i]).float()\n",
    "            elif isinstance(data[i], list):\n",
    "                data[i] = list_to_torch(data[i])\n",
    "        return data\n",
    "        \n",
    "    def __len__(self): #__len__:未來可以len(類別)，呼叫下面code\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):#__getitem__:未來這個類別可以使用[]索引，來完成下面code\n",
    "        index = self.indices[index]\n",
    "        N_nodes_max = self.N_nodes_max\n",
    "        N_nodes = self.adj_list[index].shape[0]\n",
    "        graph_support = np.zeros(self.N_nodes_max)\n",
    "        graph_support[:N_nodes] = 1\n",
    "        #1.把features捕到620,預設補0\n",
    "        #2.把adj補到620*620,預設補0\n",
    "        #3.graph_support: mask\n",
    "        #4.每個圖的真正nodes數\n",
    "        return self.nested_list_to_torch([self.pad(self.features_onehot[index].copy(), self.N_nodes_max),  # node_features\n",
    "                                          self.pad(self.adj_list[index], self.N_nodes_max, self.N_nodes_max),  # adjacency matrix\n",
    "                                          graph_support,  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1 \n",
    "                                          N_nodes,\n",
    "                                          int(self.labels[index]),\n",
    "                                          self.idx[index]])  # convert to torch\n",
    "\n",
    "class DataReader():\n",
    "    def __init__(self,\n",
    "                 data_dir, \n",
    "                 rnd_state=None, \n",
    "                 training_size_p=None,\n",
    "                 folds=None,\n",
    "                 balance=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.rnd_state = np.random.RandomState() if rnd_state == 'Random' else np.random.RandomState(int(rnd_state))\n",
    "        \n",
    "        files = os.listdir(self.data_dir)\n",
    "        \n",
    "        #data starage!\n",
    "        data = {}\n",
    "        #1. nodes:為dict，{node_id:graph_id}\n",
    "        #2. graphs:為dict,{graph_id:np.array([node_id 1,node_id 2,...])}\n",
    "        nodes, graphs = self.read_graph_nodes_relations(list(filter(lambda f: f.find('graph_indicator') >= 0, files))[0])\n",
    "        #3. data['node_id_to_idx']\n",
    "        node_id_to_idx, node_idx_to_id= self.read_node_ID(list(filter(lambda f: f.find('node_features') >= 0, files))[0])\n",
    "        data['node_id_to_idx'] = node_id_to_idx\n",
    "        data['node_idx_to_id'] = node_idx_to_id\n",
    "        #4. data['features_onehot']\n",
    "        data['features_onehot'] = self.read_node_features(list(filter(lambda f: f.find('node_features') >= 0, files))[0], nodes, graphs)  \n",
    "        #data['adj_list']\n",
    "        data['adj_list'] = self.read_graph_adj(list(filter(lambda f: f.find('_A') >= 0, files))[0], nodes, graphs,node_id_to_idx) \n",
    "        #data['labels'] 0開始\n",
    "        target_to_label = {}\n",
    "        label_to_target = {}\n",
    "        targets = np.array(self.parse_txt_file(list(filter(lambda f: f.find('graph_labels') >= 0, files))[0], \n",
    "                                      line_parse_fn=lambda s: s.strip()))\n",
    "        data['targets'] = targets\n",
    "        target_category = sorted(list(set(targets)))\n",
    "        for l, t in enumerate(target_category): \n",
    "            target_to_label[t] = l\n",
    "            label_to_target[l] = t\n",
    "        data['labels'] = np.array([target_to_label[t] for t in targets])\n",
    "        data['target_to_label'] = target_to_label\n",
    "        data['label_to_target'] = label_to_target\n",
    "        n_edges, degrees = [], []\n",
    "        for sample_id, adj in enumerate(data['adj_list']):\n",
    "            N = len(adj)  # number of nodes\n",
    "            n = np.sum(adj)  # total sum of edges\n",
    "            n_edges.append(int(n/2))  # undirected edges, so need to divide by 2\n",
    "            degrees.extend(list(np.sum(adj, 1)))\n",
    "        features_dim = len(data['features_onehot'][0][0])\n",
    "        shapes = [len(adj) for adj in data['adj_list']]\n",
    "        N_nodes_max = np.max(shapes)\n",
    "        classes = target_category\n",
    "        n_classes = len(target_category)\n",
    "\n",
    "        print('N nodes avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(shapes), np.std(shapes), np.min(shapes), np.max(shapes)))\n",
    "        print('N edges avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(n_edges), np.std(n_edges), np.min(n_edges), np.max(n_edges)))\n",
    "        print('Node degree avg/std/min/max: \\t%.2f/%.2f/%d/%d' % (np.mean(degrees), np.std(degrees), np.min(degrees), np.max(degrees)))\n",
    "        print('Node features dim: \\t\\t%d' % features_dim)\n",
    "        print('N classes: \\t\\t\\t%d' % n_classes)\n",
    "        print('Classes: \\t\\t\\t%s' %(', '.join(classes)))\n",
    "        for lbl in classes:\n",
    "            print('Class %s: \\t\\t\\t%s samples' % (lbl, np.sum(targets == lbl)))\n",
    "        #判斷每個資料中，graph數量是否相等\n",
    "        N_graphs = len(data['labels']) \n",
    "        assert N_graphs == len(data['adj_list']) == len(data['features_onehot']), 'invalid data'\n",
    "\n",
    "        train_ids, test_ids = self.split_ids(data['labels'], rnd_state=self.rnd_state, training_size_p=training_size_p,\n",
    "                                             folds=n_folds, balance=balance)\n",
    "        splits = [] #塞入dict('train':[index...],'test':[index...])\n",
    "        for fold in range(folds):\n",
    "            splits.append({'train': train_ids[fold],\n",
    "                           'test': test_ids[fold]})\n",
    "        \n",
    "        data['splits'] = splits #folds份的train和test之index\n",
    "        data['N_nodes_max'] = N_nodes_max\n",
    "        data['features_dim'] = features_dim\n",
    "        data['n_classes'] = n_classes #graph label種類數目\n",
    "        \n",
    "        self.data = data # data為一個dict()\n",
    "\n",
    "    def split_ids(self, labels_all, rnd_state=None,folds=1, training_size_p=None, balance=False):\n",
    "        if folds == 1:\n",
    "            if balance == True:\n",
    "                classes = list(set(labels_all))\n",
    "                classes_dict = dict()\n",
    "                for i in classes:\n",
    "                    classes_dict[i] = []\n",
    "                for idx,l in enumerate(labels_all):\n",
    "                    classes_dict[l].append(idx)\n",
    "                min_classes_n = len(labels_all)\n",
    "                for i in classes:\n",
    "                    if len(classes_dict[i]) < min_classes_n:\n",
    "                        min_classes_n = len(classes_dict[i])\n",
    "                training_size_per_class = int(np.round(min_classes_n*training_size_p))\n",
    "                ids_all = np.arange(len(labels_all))\n",
    "                ids = ids_all[rnd_state.permutation(len(ids_all))]\n",
    "                train_ids = []\n",
    "                for i in classes:\n",
    "                    class_ls = np.array(classes_dict[i])\n",
    "                    sampling = class_ls[rnd_state.permutation(len(class_ls))][0:training_size_per_class]\n",
    "\n",
    "                    train_ids.extend(sampling)\n",
    "                test_ids = [np.array([e for e in ids if e not in train_ids])]    \n",
    "                train_ids = [np.array(train_ids)]\n",
    "            else:\n",
    "                ids_all = np.arange(len(labels_all))\n",
    "                n = len(ids_all) #n:graph的數目\n",
    "                ids = ids_all[rnd_state.permutation(n)]\n",
    "                testing_size = int(np.round(n*(1-training_size_p)))\n",
    "                test_ids = ids[0:testing_size] # 包著np.array()\n",
    "                train_ids = [np.array([e for e in ids if e not in test_ids])] # 包著np.array()\n",
    "                test_ids = [test_ids]\n",
    "        elif folds > 1:\n",
    "            ids_all = np.arange(len(labels_all))\n",
    "            n = len(ids_all)\n",
    "            ids = ids_all[rnd_state.permutation(n)]\n",
    "            stride = int(np.ceil(n / float(folds)))\n",
    "            test_ids = [ids[i: i + stride] for i in range(0, n, stride)]\n",
    "            assert np.all(np.unique(np.concatenate(test_ids)) == sorted(ids_all)), 'some graphs are missing in the test sets'\n",
    "            assert len(test_ids) == folds, 'invalid test sets'\n",
    "            train_ids = []\n",
    "            for fold in range(folds):\n",
    "                train_ids.append(np.array([e for e in ids if e not in test_ids[fold]]))\n",
    "                assert len(train_ids[fold]) + len(test_ids[fold]) == len(np.unique(list(train_ids[fold]) + list(test_ids[fold]))) == n, 'invalid splits'\n",
    "\n",
    "        return train_ids, test_ids\n",
    "\n",
    "    def parse_txt_file(self, fpath, line_parse_fn=None):\n",
    "        #pjoin=os.path.join:路徑拼接\n",
    "        #os.path.join([PATH_1], [PATH_2], [PATH_3], ...)-->return:[PATH_1]/[PATH_2]/[PATH_3]\n",
    "        with open(pjoin(self.data_dir, fpath), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        #if line_parse_fn is not None else s:代表如果有處理字串函數就執行，否則就保留原本的樣子\n",
    "        data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]\n",
    "        return data\n",
    "    \n",
    "    def read_graph_adj(self, fpath, nodes, graphs, node_id_to_idx):\n",
    "        def fn_read_graph_adj(s):\n",
    "            if ',' in s:\n",
    "                return s.strip().split(',')\n",
    "            else:\n",
    "                return s.strip().split()\n",
    "        edges = self.parse_txt_file(fpath, line_parse_fn=fn_read_graph_adj)\n",
    "        adj_dict = {}\n",
    "        for edge in edges:\n",
    "            node1 = node_id_to_idx[edge[0].strip()]\n",
    "            node2 = node_id_to_idx[edge[1].strip()]\n",
    "            graph_id = nodes[node1]\n",
    "            assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])\n",
    "            \n",
    "            if graph_id not in adj_dict:\n",
    "                n = len(graphs[graph_id])\n",
    "                adj_dict[graph_id] = np.zeros((n, n))\n",
    "            ind1 = np.where(graphs[graph_id] == node1)[0]\n",
    "            ind2 = np.where(graphs[graph_id] == node2)[0]\n",
    "            assert len(ind1) == len(ind2) == 1, (ind1, ind2)\n",
    "            adj_dict[graph_id][ind1, ind2] = 1\n",
    "            adj_dict[graph_id][ind2, ind1] = 1\n",
    "        adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]        \n",
    "        return adj_list\n",
    "        \n",
    "    #graph_indicator\n",
    "    def read_graph_nodes_relations(self, fpath):\n",
    "        #node從0開始\n",
    "        #graph沒限定，但要是整數\n",
    "        graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))\n",
    "        nodes, graphs = {}, {}\n",
    "        for node_id, graph_id in enumerate(graph_ids):\n",
    "            if graph_id not in graphs:\n",
    "                graphs[graph_id] = []\n",
    "            graphs[graph_id].append(node_id)\n",
    "            nodes[node_id] = graph_id\n",
    "        graph_ids = np.unique(list(graphs.keys()))\n",
    "        for graph_id in graphs:\n",
    "            graphs[graph_id] = np.array(graphs[graph_id])\n",
    "        return nodes, graphs\n",
    "\n",
    "    def read_node_features(self, fpath, nodes, graphs):\n",
    "        def fn_read_node_features(s):\n",
    "            if ',' in s:\n",
    "                return list(map(float,(s.strip().split(',')[1:])))\n",
    "            else:\n",
    "                return list(map(float,(s.strip().split()[1:])))\n",
    "        node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn_read_node_features)\n",
    "        node_features = {}\n",
    "        #node_features:資料格式和graphs相似\n",
    "        for node_id, x in enumerate(node_features_all):\n",
    "            graph_id = nodes[node_id]\n",
    "            if graph_id not in node_features:\n",
    "                node_features[graph_id] = [ None ] * len(graphs[graph_id])\n",
    "            ind = np.where(graphs[graph_id] == node_id)[0]\n",
    "            #assert 判斷式, 如果有誤回傳的內容\n",
    "            assert len(ind) == 1, ind\n",
    "            assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]\n",
    "            node_features[graph_id][ind[0]] = x\n",
    "        node_features_lst = [np.array(node_features[graph_id]) for graph_id in sorted(list(graphs.keys()))]\n",
    "        return node_features_lst\n",
    "    \n",
    "    def read_node_ID(self, fpath):\n",
    "        def fn_read_node_ID(s):\n",
    "            if ',' in s:\n",
    "                return s.strip().split(',')[0]\n",
    "            else:\n",
    "                return s.strip().split()[0]\n",
    "        node_ID_all = self.parse_txt_file(fpath, line_parse_fn=fn_read_node_ID)\n",
    "        assert len(node_ID_all) == len(set(node_ID_all))\n",
    "        \n",
    "        node_id_to_idx = {}#str:int\n",
    "        node_idx_to_id = {}\n",
    "        for node_idx, node_id in enumerate(node_ID_all):\n",
    "            node_id_to_idx[node_id] = node_idx\n",
    "            node_idx_to_id[node_idx] = node_id\n",
    "        return node_id_to_idx, node_idx_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "N nodes avg/std/min/max: \t39.06/45.76/4/620\n",
      "N edges avg/std/min/max: \t72.82/84.60/5/1049\n",
      "Node degree avg/std/min/max: \t3.73/1.15/0/25\n",
      "Node features dim: \t\t3\n",
      "N classes: \t\t\t2\n",
      "Classes: \t\t\t1, 2\n",
      "Class 1: \t\t\t663 samples\n",
      "Class 2: \t\t\t450 samples\n",
      "\n",
      "FOLD 1\n",
      "TRAIN: 890/1113\n",
      "TEST: 223/1113\n",
      "\n",
      "Initialize model\n",
      "GCN(\n",
      "  (gconv): Sequential(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=3, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "N trainable parameters: 8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chienhua\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [64/890 (7%)]\tLoss: 0.7053(avg: 0.7053)\tAcc: 32.81%(21/64) \tsec/iter: 0.8917\n",
      "Train Epoch: 0 [704/890 (79%)]\tLoss: 0.6811(avg: 0.6899)\tAcc: 50.14%(353/704) \tsec/iter: 0.8030\n",
      "Train Epoch: 0 [890/890 (100%)]\tLoss: 0.6580(avg: 0.6873)\tAcc: 52.36%(466/890) \tsec/iter: 0.7960\n",
      "Test set (epoch 1): Average loss: 0.6758, Accuracy: 138/223 (61.88%)\n",
      "\n",
      "Train Epoch: 1 [64/890 (7%)]\tLoss: 0.6854(avg: 0.6854)\tAcc: 62.50%(40/64) \tsec/iter: 0.7628\n",
      "Train Epoch: 1 [704/890 (79%)]\tLoss: 0.6930(avg: 0.6814)\tAcc: 59.38%(418/704) \tsec/iter: 0.7962\n",
      "Train Epoch: 1 [890/890 (100%)]\tLoss: 0.6726(avg: 0.6818)\tAcc: 58.88%(524/890) \tsec/iter: 0.7967\n",
      "Test set (epoch 2): Average loss: 0.6730, Accuracy: 138/223 (61.88%)\n",
      "\n",
      "Train Epoch: 2 [64/890 (7%)]\tLoss: 0.6868(avg: 0.6868)\tAcc: 54.69%(35/64) \tsec/iter: 0.7938\n",
      "Train Epoch: 2 [704/890 (79%)]\tLoss: 0.6793(avg: 0.6791)\tAcc: 58.24%(410/704) \tsec/iter: 0.7970\n",
      "Train Epoch: 2 [890/890 (100%)]\tLoss: 0.6876(avg: 0.6757)\tAcc: 58.88%(524/890) \tsec/iter: 0.7870\n",
      "Test set (epoch 3): Average loss: 0.6698, Accuracy: 138/223 (61.88%)\n",
      "\n",
      "Train Epoch: 3 [64/890 (7%)]\tLoss: 0.6776(avg: 0.6776)\tAcc: 57.81%(37/64) \tsec/iter: 0.6983\n",
      "Train Epoch: 3 [704/890 (79%)]\tLoss: 0.6798(avg: 0.6768)\tAcc: 59.94%(422/704) \tsec/iter: 0.7908\n",
      "Train Epoch: 3 [890/890 (100%)]\tLoss: 0.6889(avg: 0.6791)\tAcc: 58.99%(525/890) \tsec/iter: 0.7953\n",
      "Test set (epoch 4): Average loss: 0.6723, Accuracy: 138/223 (61.88%)\n",
      "\n",
      "Train Epoch: 4 [64/890 (7%)]\tLoss: 0.6653(avg: 0.6653)\tAcc: 60.94%(39/64) \tsec/iter: 0.7394\n",
      "Train Epoch: 4 [704/890 (79%)]\tLoss: 0.6748(avg: 0.6817)\tAcc: 58.52%(412/704) \tsec/iter: 0.7959\n",
      "Train Epoch: 4 [890/890 (100%)]\tLoss: 0.6405(avg: 0.6807)\tAcc: 58.99%(525/890) \tsec/iter: 0.7905\n",
      "Test set (epoch 5): Average loss: 0.6701, Accuracy: 138/223 (61.88%)\n",
      "\n",
      "Train Epoch: 5 [64/890 (7%)]\tLoss: 0.6809(avg: 0.6809)\tAcc: 56.25%(36/64) \tsec/iter: 0.8149\n",
      "Train Epoch: 5 [704/890 (79%)]\tLoss: 0.6659(avg: 0.6749)\tAcc: 59.52%(419/704) \tsec/iter: 0.7898\n",
      "Train Epoch: 5 [890/890 (100%)]\tLoss: 0.6814(avg: 0.6752)\tAcc: 58.99%(525/890) \tsec/iter: 0.7859\n",
      "Test set (epoch 6): Average loss: 0.6640, Accuracy: 138/223 (61.88%)\n",
      "\n",
      "Train Epoch: 6 [64/890 (7%)]\tLoss: 0.6789(avg: 0.6789)\tAcc: 56.25%(36/64) \tsec/iter: 0.7985\n",
      "Train Epoch: 6 [704/890 (79%)]\tLoss: 0.7387(avg: 0.6793)\tAcc: 57.10%(402/704) \tsec/iter: 0.7883\n",
      "Train Epoch: 6 [890/890 (100%)]\tLoss: 0.6693(avg: 0.6714)\tAcc: 58.99%(525/890) \tsec/iter: 0.7879\n",
      "Test set (epoch 7): Average loss: 0.6515, Accuracy: 138/223 (61.88%)\n",
      "\n",
      "Train Epoch: 7 [64/890 (7%)]\tLoss: 0.6748(avg: 0.6748)\tAcc: 57.81%(37/64) \tsec/iter: 0.7131\n",
      "Train Epoch: 7 [704/890 (79%)]\tLoss: 0.6560(avg: 0.6624)\tAcc: 59.23%(417/704) \tsec/iter: 0.7904\n",
      "Train Epoch: 7 [890/890 (100%)]\tLoss: 0.6751(avg: 0.6656)\tAcc: 58.99%(525/890) \tsec/iter: 0.7878\n",
      "Test set (epoch 8): Average loss: 0.6450, Accuracy: 138/223 (61.88%)\n",
      "\n",
      "Train Epoch: 8 [64/890 (7%)]\tLoss: 0.6447(avg: 0.6447)\tAcc: 56.25%(36/64) \tsec/iter: 0.7370\n",
      "Train Epoch: 8 [704/890 (79%)]\tLoss: 0.6730(avg: 0.6636)\tAcc: 58.95%(415/704) \tsec/iter: 0.7966\n",
      "Train Epoch: 8 [890/890 (100%)]\tLoss: 0.6411(avg: 0.6635)\tAcc: 58.88%(524/890) \tsec/iter: 0.7956\n",
      "Test set (epoch 9): Average loss: 0.6419, Accuracy: 139/223 (62.33%)\n",
      "\n",
      "Train Epoch: 9 [64/890 (7%)]\tLoss: 0.6418(avg: 0.6418)\tAcc: 64.06%(41/64) \tsec/iter: 0.7441\n",
      "Train Epoch: 9 [704/890 (79%)]\tLoss: 0.6583(avg: 0.6616)\tAcc: 59.80%(421/704) \tsec/iter: 0.7900\n",
      "Train Epoch: 9 [890/890 (100%)]\tLoss: 0.6331(avg: 0.6574)\tAcc: 59.44%(529/890) \tsec/iter: 0.7868\n",
      "Test set (epoch 10): Average loss: 0.6338, Accuracy: 147/223 (65.92%)\n",
      "\n",
      "Train Epoch: 10 [64/890 (7%)]\tLoss: 0.6285(avg: 0.6285)\tAcc: 71.88%(46/64) \tsec/iter: 0.7384\n",
      "Train Epoch: 10 [704/890 (79%)]\tLoss: 0.6198(avg: 0.6519)\tAcc: 65.20%(459/704) \tsec/iter: 0.8036\n",
      "Train Epoch: 10 [890/890 (100%)]\tLoss: 0.6281(avg: 0.6525)\tAcc: 64.94%(578/890) \tsec/iter: 0.8010\n",
      "Test set (epoch 11): Average loss: 0.6267, Accuracy: 159/223 (71.30%)\n",
      "\n",
      "Train Epoch: 11 [64/890 (7%)]\tLoss: 0.6670(avg: 0.6670)\tAcc: 62.50%(40/64) \tsec/iter: 0.7561\n",
      "Train Epoch: 11 [704/890 (79%)]\tLoss: 0.6275(avg: 0.6514)\tAcc: 64.20%(452/704) \tsec/iter: 0.8136\n",
      "Train Epoch: 11 [890/890 (100%)]\tLoss: 0.7336(avg: 0.6517)\tAcc: 64.49%(574/890) \tsec/iter: 0.7980\n",
      "Test set (epoch 12): Average loss: 0.6208, Accuracy: 157/223 (70.40%)\n",
      "\n",
      "Train Epoch: 12 [64/890 (7%)]\tLoss: 0.6537(avg: 0.6537)\tAcc: 67.19%(43/64) \tsec/iter: 0.6911\n",
      "Train Epoch: 12 [704/890 (79%)]\tLoss: 0.7040(avg: 0.6495)\tAcc: 63.64%(448/704) \tsec/iter: 0.7934\n",
      "Train Epoch: 12 [890/890 (100%)]\tLoss: 0.6543(avg: 0.6507)\tAcc: 64.04%(570/890) \tsec/iter: 0.7874\n",
      "Test set (epoch 13): Average loss: 0.6295, Accuracy: 166/223 (74.44%)\n",
      "\n",
      "Train Epoch: 13 [64/890 (7%)]\tLoss: 0.6859(avg: 0.6859)\tAcc: 64.06%(41/64) \tsec/iter: 0.7229\n",
      "Train Epoch: 13 [704/890 (79%)]\tLoss: 0.6435(avg: 0.6497)\tAcc: 64.49%(454/704) \tsec/iter: 0.7844\n",
      "Train Epoch: 13 [890/890 (100%)]\tLoss: 0.6097(avg: 0.6445)\tAcc: 65.28%(581/890) \tsec/iter: 0.7805\n",
      "Test set (epoch 14): Average loss: 0.6198, Accuracy: 160/223 (71.75%)\n",
      "\n",
      "Train Epoch: 14 [64/890 (7%)]\tLoss: 0.6256(avg: 0.6256)\tAcc: 60.94%(39/64) \tsec/iter: 0.8356\n",
      "Train Epoch: 14 [704/890 (79%)]\tLoss: 0.6605(avg: 0.6368)\tAcc: 65.20%(459/704) \tsec/iter: 0.8068\n",
      "Train Epoch: 14 [890/890 (100%)]\tLoss: 0.6703(avg: 0.6411)\tAcc: 65.39%(582/890) \tsec/iter: 0.7992\n",
      "Test set (epoch 15): Average loss: 0.6276, Accuracy: 163/223 (73.09%)\n",
      "\n",
      "Train Epoch: 15 [64/890 (7%)]\tLoss: 0.6722(avg: 0.6722)\tAcc: 56.25%(36/64) \tsec/iter: 0.6950\n",
      "Train Epoch: 15 [704/890 (79%)]\tLoss: 0.6414(avg: 0.6478)\tAcc: 63.78%(449/704) \tsec/iter: 0.7706\n",
      "Train Epoch: 15 [890/890 (100%)]\tLoss: 0.6035(avg: 0.6482)\tAcc: 64.49%(574/890) \tsec/iter: 0.7784\n",
      "Test set (epoch 16): Average loss: 0.6154, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "Train Epoch: 16 [64/890 (7%)]\tLoss: 0.6714(avg: 0.6714)\tAcc: 59.38%(38/64) \tsec/iter: 0.7940\n",
      "Train Epoch: 16 [704/890 (79%)]\tLoss: 0.6169(avg: 0.6314)\tAcc: 65.20%(459/704) \tsec/iter: 0.8038\n",
      "Train Epoch: 16 [890/890 (100%)]\tLoss: 0.6223(avg: 0.6373)\tAcc: 64.72%(576/890) \tsec/iter: 0.7947\n",
      "Test set (epoch 17): Average loss: 0.6180, Accuracy: 165/223 (73.99%)\n",
      "\n",
      "Train Epoch: 17 [64/890 (7%)]\tLoss: 0.6542(avg: 0.6542)\tAcc: 67.19%(43/64) \tsec/iter: 0.8162\n",
      "Train Epoch: 17 [704/890 (79%)]\tLoss: 0.6244(avg: 0.6430)\tAcc: 65.06%(458/704) \tsec/iter: 0.8003\n",
      "Train Epoch: 17 [890/890 (100%)]\tLoss: 0.7013(avg: 0.6480)\tAcc: 64.61%(575/890) \tsec/iter: 0.7924\n",
      "Test set (epoch 18): Average loss: 0.6152, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "Train Epoch: 18 [64/890 (7%)]\tLoss: 0.6300(avg: 0.6300)\tAcc: 64.06%(41/64) \tsec/iter: 0.7739\n",
      "Train Epoch: 18 [704/890 (79%)]\tLoss: 0.6416(avg: 0.6217)\tAcc: 68.32%(481/704) \tsec/iter: 0.7695\n",
      "Train Epoch: 18 [890/890 (100%)]\tLoss: 0.6733(avg: 0.6273)\tAcc: 67.42%(600/890) \tsec/iter: 0.7771\n",
      "Test set (epoch 19): Average loss: 0.6237, Accuracy: 163/223 (73.09%)\n",
      "\n",
      "Train Epoch: 19 [64/890 (7%)]\tLoss: 0.6105(avg: 0.6105)\tAcc: 67.19%(43/64) \tsec/iter: 0.7887\n",
      "Train Epoch: 19 [704/890 (79%)]\tLoss: 0.6757(avg: 0.6433)\tAcc: 64.77%(456/704) \tsec/iter: 0.7791\n",
      "Train Epoch: 19 [890/890 (100%)]\tLoss: 0.6204(avg: 0.6382)\tAcc: 65.73%(585/890) \tsec/iter: 0.7717\n",
      "Test set (epoch 20): Average loss: 0.6130, Accuracy: 165/223 (73.99%)\n",
      "\n",
      "Train Epoch: 20 [64/890 (7%)]\tLoss: 0.6962(avg: 0.6962)\tAcc: 60.94%(39/64) \tsec/iter: 0.7563\n",
      "Train Epoch: 20 [704/890 (79%)]\tLoss: 0.6411(avg: 0.6517)\tAcc: 63.49%(447/704) \tsec/iter: 0.8059\n",
      "Train Epoch: 20 [890/890 (100%)]\tLoss: 0.5864(avg: 0.6386)\tAcc: 65.62%(584/890) \tsec/iter: 0.8050\n",
      "Test set (epoch 21): Average loss: 0.6106, Accuracy: 164/223 (73.54%)\n",
      "\n",
      "Train Epoch: 21 [64/890 (7%)]\tLoss: 0.6426(avg: 0.6426)\tAcc: 64.06%(41/64) \tsec/iter: 0.6998\n",
      "Train Epoch: 21 [704/890 (79%)]\tLoss: 0.5915(avg: 0.6374)\tAcc: 65.06%(458/704) \tsec/iter: 0.7858\n",
      "Train Epoch: 21 [890/890 (100%)]\tLoss: 0.6193(avg: 0.6296)\tAcc: 65.96%(587/890) \tsec/iter: 0.7860\n",
      "Test set (epoch 22): Average loss: 0.6092, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "Train Epoch: 22 [64/890 (7%)]\tLoss: 0.6219(avg: 0.6219)\tAcc: 70.31%(45/64) \tsec/iter: 0.7480\n",
      "Train Epoch: 22 [704/890 (79%)]\tLoss: 0.6274(avg: 0.6341)\tAcc: 66.34%(467/704) \tsec/iter: 0.7867\n",
      "Train Epoch: 22 [890/890 (100%)]\tLoss: 0.5926(avg: 0.6341)\tAcc: 66.29%(590/890) \tsec/iter: 0.7837\n",
      "Test set (epoch 23): Average loss: 0.6087, Accuracy: 162/223 (72.65%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23 [64/890 (7%)]\tLoss: 0.6964(avg: 0.6964)\tAcc: 56.25%(36/64) \tsec/iter: 0.7537\n",
      "Train Epoch: 23 [704/890 (79%)]\tLoss: 0.5864(avg: 0.6342)\tAcc: 65.20%(459/704) \tsec/iter: 0.7794\n",
      "Train Epoch: 23 [890/890 (100%)]\tLoss: 0.5869(avg: 0.6325)\tAcc: 65.73%(585/890) \tsec/iter: 0.7745\n",
      "Test set (epoch 24): Average loss: 0.6081, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "Train Epoch: 24 [64/890 (7%)]\tLoss: 0.6185(avg: 0.6185)\tAcc: 64.06%(41/64) \tsec/iter: 0.7639\n",
      "Train Epoch: 24 [704/890 (79%)]\tLoss: 0.6147(avg: 0.6200)\tAcc: 68.04%(479/704) \tsec/iter: 0.7861\n",
      "Train Epoch: 24 [890/890 (100%)]\tLoss: 0.6441(avg: 0.6261)\tAcc: 67.30%(599/890) \tsec/iter: 0.7847\n",
      "Test set (epoch 25): Average loss: 0.6072, Accuracy: 161/223 (72.20%)\n",
      "\n",
      "Train Epoch: 25 [64/890 (7%)]\tLoss: 0.6938(avg: 0.6938)\tAcc: 60.94%(39/64) \tsec/iter: 0.7262\n",
      "Train Epoch: 25 [704/890 (79%)]\tLoss: 0.5760(avg: 0.6430)\tAcc: 66.34%(467/704) \tsec/iter: 0.7770\n",
      "Train Epoch: 25 [890/890 (100%)]\tLoss: 0.6694(avg: 0.6429)\tAcc: 65.62%(584/890) \tsec/iter: 0.7783\n",
      "Test set (epoch 26): Average loss: 0.6082, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "Train Epoch: 26 [64/890 (7%)]\tLoss: 0.5933(avg: 0.5933)\tAcc: 78.12%(50/64) \tsec/iter: 0.7670\n",
      "Train Epoch: 26 [704/890 (79%)]\tLoss: 0.6531(avg: 0.6215)\tAcc: 66.48%(468/704) \tsec/iter: 0.8033\n",
      "Train Epoch: 26 [890/890 (100%)]\tLoss: 0.6505(avg: 0.6351)\tAcc: 63.71%(567/890) \tsec/iter: 0.7925\n",
      "Test set (epoch 27): Average loss: 0.6076, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "Train Epoch: 27 [64/890 (7%)]\tLoss: 0.6245(avg: 0.6245)\tAcc: 71.88%(46/64) \tsec/iter: 0.7042\n",
      "Train Epoch: 27 [704/890 (79%)]\tLoss: 0.6201(avg: 0.6191)\tAcc: 67.90%(478/704) \tsec/iter: 0.7855\n",
      "Train Epoch: 27 [890/890 (100%)]\tLoss: 0.6186(avg: 0.6193)\tAcc: 67.53%(601/890) \tsec/iter: 0.7959\n",
      "Test set (epoch 28): Average loss: 0.6073, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "Train Epoch: 28 [64/890 (7%)]\tLoss: 0.6072(avg: 0.6072)\tAcc: 68.75%(44/64) \tsec/iter: 0.7429\n",
      "Train Epoch: 28 [704/890 (79%)]\tLoss: 0.7156(avg: 0.6239)\tAcc: 66.19%(466/704) \tsec/iter: 0.7850\n",
      "Train Epoch: 28 [890/890 (100%)]\tLoss: 0.5743(avg: 0.6258)\tAcc: 65.51%(583/890) \tsec/iter: 0.7830\n",
      "Test set (epoch 29): Average loss: 0.6072, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "Train Epoch: 29 [64/890 (7%)]\tLoss: 0.6302(avg: 0.6302)\tAcc: 68.75%(44/64) \tsec/iter: 0.7675\n",
      "Train Epoch: 29 [704/890 (79%)]\tLoss: 0.6358(avg: 0.6407)\tAcc: 66.62%(469/704) \tsec/iter: 0.7894\n",
      "Train Epoch: 29 [890/890 (100%)]\tLoss: 0.6071(avg: 0.6363)\tAcc: 67.64%(602/890) \tsec/iter: 0.7861\n",
      "Test set (epoch 30): Average loss: 0.6072, Accuracy: 162/223 (72.65%)\n",
      "\n",
      "[Trained Model]\n",
      "Training Set: Accuracy=64.38%(573/890)\n",
      "Testing Set: Accuracy=72.65%(162/223)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Loading data')\n",
    "\n",
    "datareader = DataReader(data_dir=dataset, \n",
    "                        rnd_state=seed,training_size_p=training_size_p,folds=n_folds,balance=balance)\n",
    "\n",
    "train_acc_folds = []\n",
    "test_acc_folds = []\n",
    "for fold_id in range(n_folds):\n",
    "    print('\\nFOLD', fold_id+1)\n",
    "    loaders = []\n",
    "    for split in ['train', 'test']:\n",
    "        #製作\"train\"或\"test\" graph data\n",
    "        gdata = GraphData(fold_id=fold_id, datareader=datareader, split=split)\n",
    "        loader = torch.utils.data.DataLoader(gdata, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=split.find('train') >= 0,\n",
    "                                             num_workers=threads)\n",
    "        loaders.append(loader)\n",
    "        if split == 'train':\n",
    "            training_size = len(gdata.idx)\n",
    "    if model_name == 'GCN':\n",
    "        model = GCN(in_features=loaders[0].dataset.features_dim, \n",
    "                    out_features=loaders[0].dataset.n_classes,\n",
    "                    filters_gcn=filters_gcn,\n",
    "                    gcn_bias=gcn_bias,\n",
    "                    gcn_activation=gcn_activation,\n",
    "                    dropout_gcn=dropout_gcn,\n",
    "                    fc_bias=fc_bias,\n",
    "                    n_hidden_fc=n_hidden_fc,\n",
    "                    dropout_fc=dropout_fc,\n",
    "                    fc_activation=fc_activation).to(device)    \n",
    "\n",
    "    print('\\nInitialize model')\n",
    "    print(model)\n",
    "    c = 0\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        c += p.numel()\n",
    "    print('N trainable parameters:', c)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=lr,\n",
    "                weight_decay=wdecay,\n",
    "                betas=(0.5, 0.999))\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.1)\n",
    "\n",
    "\n",
    "    def train(train_loader):\n",
    "        scheduler.step()#每個batch就會改變學習率\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        train_loss, correct, n_samples = 0, 0, 0\n",
    "        train_loss_batch_ls = []\n",
    "        train_acc_batch_ls = []\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)     \n",
    "            loss = loss_fn(output, data[4])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            time_iter = time.time() - start\n",
    "            train_loss += loss.item() * len(output)\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "            acc = 100. * correct / n_samples\n",
    "            train_loss_batch_ls.append(train_loss/n_samples)\n",
    "            train_acc_batch_ls.append(acc/100)\n",
    "            if batch_idx % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}(avg: {:.4f})\\tAcc: {:.2f}%({}/{}) \\tsec/iter: {:.4f}'.format(\n",
    "                    epoch, n_samples, len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, \n",
    "                    acc, correct, n_samples, time_iter / (batch_idx + 1) ))    \n",
    "        return train_loss_batch_ls, train_acc_batch_ls\n",
    "    def test(test_loader):\n",
    "        model.eval()\n",
    "        start = time.time()\n",
    "        test_loss, correct, n_samples = 0, 0, 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4], reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "        time_iter = time.time() - start\n",
    "        test_loss /= n_samples\n",
    "        acc = 100. * correct / n_samples\n",
    "        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch+1, \n",
    "                                                                                              test_loss, \n",
    "                                                                                              correct, \n",
    "                                                                                              n_samples, acc))\n",
    "        return test_loss,acc/100\n",
    "\n",
    "    def predict(loader_full):\n",
    "        idx_ls = []\n",
    "        pred_ls = [] \n",
    "        label_ls = []\n",
    "        length_ls = []\n",
    "        output_ls = []\n",
    "        print('[Trained Model]')\n",
    "        for i in [0,1]:\n",
    "            model.eval()     \n",
    "            pred_tmp = []\n",
    "            label_tmp = []\n",
    "            for batch_idx, data in enumerate(loader_full[i]):\n",
    "                for j in range(len(data)):\n",
    "                    data[j] = data[j].to(device)\n",
    "                output = model(data)\n",
    "                idx_ls.extend(data[5].tolist())\n",
    "                pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "                pred_ls.extend(pred.reshape(pred.shape[0]).tolist())\n",
    "                label_ls.extend(data[4].tolist())\n",
    "                output_ls.extend(output)\n",
    "                pred_tmp.extend(pred.reshape(pred.shape[0]).tolist())\n",
    "                label_tmp.extend(data[4].tolist())\n",
    "            total = len(pred_tmp)\n",
    "            c = sum(np.array(pred_tmp)==np.array(label_tmp)) \n",
    "            if i==0:\n",
    "                print('Training Set: Accuracy=%.2f%%(%s/%s)'%(c*100/total,c,total))\n",
    "                train_acc_folds.append(c*100/total)\n",
    "            elif i==1:\n",
    "                print('Testing Set: Accuracy=%.2f%%(%s/%s)'%(c*100/total,c,total))  \n",
    "                test_acc_folds.append(c*100/total)\n",
    "            length_ls.append(total)\n",
    "        return idx_ls, pred_ls, label_ls, length_ls, output_ls\n",
    "\n",
    "    train_loss_ls = []\n",
    "    train_acc_ls = []\n",
    "    test_loss_ls = []\n",
    "    test_acc_ls = []\n",
    "    loss_fn = F.nll_loss\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(loaders[0])\n",
    "        test_loss, test_acc = test(loaders[1])\n",
    "        train_loss_ls.extend(train_loss)\n",
    "        train_acc_ls.extend(train_acc)\n",
    "        test_loss_ls.append(test_loss)\n",
    "        test_acc_ls.append(test_acc)   \n",
    "\n",
    "    idx_ls, pred_ls, label_ls, length_ls, output_ls = predict(loaders)\n",
    "\n",
    "    #plot\n",
    "    length_train = range(len(train_loss_ls))\n",
    "    length_test = range(int(np.ceil(training_size/batch_size))-1,len(train_loss_ls),int(np.ceil(training_size/batch_size)))\n",
    "    plt.plot(length_train,train_acc_ls,label='training accuracy')\n",
    "    plt.plot(length_test,test_acc_ls,label='validation accuracy')\n",
    "    x_ticks = [0]+list(length_test)\n",
    "    plt.xticks(x_ticks,list(range(0,epochs+1)))\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('accuracy',fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_folder+'/acc_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(length_train,train_loss_ls,label='training loss')\n",
    "    plt.plot(length_test,test_loss_ls,label='validation loss')\n",
    "    plt.xticks(x_ticks,list(range(0,epochs+1)))\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('loss',fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_folder+'/loss_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()    \n",
    "    \n",
    "    #xlsx\n",
    "    writer = pd.ExcelWriter(output_folder+'/result_fold%s.xlsx'%(fold_id+1), engine = 'xlsxwriter')\n",
    "    ################################################\n",
    "    train_loss_acc_df = pd.DataFrame(zip(range(1,epochs+1),np.array(train_loss_ls)[length_test],np.array(train_acc_ls)[length_test]))\n",
    "    train_loss_acc_df.columns = ['epoch','training loss','training accuracy']\n",
    "    train_loss_acc_df.to_excel(writer,sheet_name='training',header=True,index=False) \n",
    "    test_loss_acc_df = pd.DataFrame(zip(range(1,epochs+1),test_loss_ls,test_acc_ls))\n",
    "    test_loss_acc_df.columns = ['epoch','testing loss','testing accuracy']\n",
    "    test_loss_acc_df.to_excel(writer,sheet_name='validation',header=True,index=False) \n",
    "    ################################################\n",
    "    for i in range(len(model.gconv)):\n",
    "        f_GCN_weight = pd.DataFrame(np.matrix(model.gconv[i].fc.weight.tolist()).T)\n",
    "        f_GCN_weight.to_excel(writer,sheet_name='GCN%s_weight'%i,header=False,index=False)\n",
    "        if gcn_bias:\n",
    "            f_GCN_bias = pd.DataFrame(np.matrix(model.gconv[i].fc.bias.tolist()).T)\n",
    "            f_GCN_bias.to_excel(writer,sheet_name='GCN%s_bias'%i,header=False,index=False)    \n",
    "            \n",
    "    fc_layer = 0\n",
    "    for i in range(len(model.fc)):\n",
    "        if i % 3 == 1:\n",
    "            f_fc_weight = pd.DataFrame(np.matrix(model.fc[i].weight.tolist()).T)\n",
    "            f_fc_weight.to_excel(writer,sheet_name='FC%s_weight'%fc_layer,header=False,index=False)    \n",
    "            if fc_bias:\n",
    "                f_fc_bias = pd.DataFrame(np.matrix(model.fc[i].bias.tolist()).T)\n",
    "                f_fc_bias.to_excel(writer,sheet_name='FC%s_bias'%fc_layer,header=False,index=False)\n",
    "            fc_layer += 1\n",
    "    #################################################################\n",
    "    label_target_ls = [loaders[0].dataset.label_to_target[i] for i in label_ls]\n",
    "    pred_target_ls = [loaders[0].dataset.label_to_target[i] for i in pred_ls]\n",
    "    id_ls = [loaders[0].dataset.node_idx_to_id[i] for i in idx_ls]\n",
    "    splits = ['training']*length_ls[0]+['validation']*length_ls[1]\n",
    "    prediction = pd.DataFrame(zip(idx_ls,id_ls,pred_target_ls,label_target_ls,splits))\n",
    "    prediction.columns = ['IDX','ID','prediction','label','splits']\n",
    "    \n",
    "    class_prob = pd.DataFrame([np.exp(i.tolist())*100 for i in output_ls])\n",
    "    columns = []\n",
    "    for c in class_prob.columns:\n",
    "        class_prob[c] = class_prob[c].map('{:,.2f}%'.format)\n",
    "        columns.append('p('+loaders[0].dataset.label_to_target[c]+')')\n",
    "    class_prob.columns = columns    \n",
    "    prediction = pd.concat([prediction,class_prob], axis=1)\n",
    "    \n",
    "    prediction = prediction.sort_values(by=['IDX'])\n",
    "    prediction = prediction.loc[:,'ID':]\n",
    "    corrects = []\n",
    "    for i in prediction['prediction'] == prediction['label']:\n",
    "        if i:\n",
    "            corrects.append(1)\n",
    "        else:\n",
    "            corrects.append(0)\n",
    "    prediction['correct'] = corrects\n",
    "    prediction.to_excel(writer,sheet_name='Prediction',header=True,index=False)  \n",
    "    #######################################################################################\n",
    "    #sampling info.\n",
    "    class_number_train = []\n",
    "    class_proportion_train = []\n",
    "    class_number_val = []\n",
    "    class_proportion_val = []    \n",
    "    \n",
    "    label_set = sorted(list(set(prediction['label'])))\n",
    "    for i in label_set:\n",
    "        train_n = len(prediction[(prediction['label']==i)&(prediction['splits']=='training')])\n",
    "        train_total = len(prediction[(prediction['splits']=='training')])\n",
    "        val_n = len(prediction[(prediction['label']==i)&(prediction['splits']=='validation')])\n",
    "        val_total = len(prediction[(prediction['splits']=='validation')])\n",
    "        class_number_train.append(train_n)\n",
    "        class_proportion_train.append('%.1f%%'%(train_n*100/train_total))\n",
    "        class_number_val.append(val_n)\n",
    "        class_proportion_val.append('%.1f%%'%(val_n*100/val_total))  \n",
    "    sampling_info = pd.DataFrame(zip(label_set,class_number_train,class_proportion_train,class_number_val,class_proportion_val))\n",
    "    sampling_info.columns = ['label','training set','proportion of training set','validation set','proportion of validation set']\n",
    "    sampling_info.to_excel(writer, sheet_name = 'sampling_info',index=False)    \n",
    "    ############################################################################################\n",
    "    #class_acc\n",
    "    Number_Train = []\n",
    "    Correct_Train = []\n",
    "    Number_Val = []\n",
    "    Correct_Val = []\n",
    "    Values = list(loaders[0].dataset.label_to_target.values())\n",
    "    for i in Values:\n",
    "        Correct_Train.append(len(prediction[(prediction['correct']==1)&(prediction['label']==i)&(prediction['splits']=='training')]))\n",
    "        Number_Train.append(len(prediction[(prediction['label']==i)&(prediction['splits']=='training')]))\n",
    "        Correct_Val.append(len(prediction[(prediction['correct']==1)&(prediction['label']==i)&(prediction['splits']=='validation')]))\n",
    "        Number_Val.append(len(prediction[(prediction['label']==i)&(prediction['splits']=='validation')]))\n",
    "        Rate_Train = np.array(Correct_Train)*100/np.array(Number_Train)\n",
    "        Rate_Val = np.array(Correct_Val)*100/np.array(Number_Val)\n",
    "    Class_Prob = pd.DataFrame(zip(Values,Correct_Train,Number_Train,Rate_Train,Correct_Val,Number_Val,Rate_Val))\n",
    "    Class_Prob.columns = ['class','train_correct','train_number','train_accuracy','validation_correct','validation_number','validation_accuracy']\n",
    "    Class_Prob['train_accuracy'] = Class_Prob['train_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob['validation_accuracy'] = Class_Prob['validation_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob.to_excel(writer, sheet_name = 'class_acc',index=False)          \n",
    "    #################################################################################################\n",
    "    #label_pred_mx_train\n",
    "    #label_pred_mx_val\n",
    "    class_to_onehot = dict(zip([loaders[0].dataset.label_to_target[i] for i in range(len(loaders[0].dataset.label_to_target))],range(len(loaders[0].dataset.label_to_target))))\n",
    "    label_pred_mx_train = np.zeros([len(class_to_onehot),len(class_to_onehot)])\n",
    "    label_pred_mx_val = np.zeros([len(class_to_onehot),len(class_to_onehot)])\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction.at[i,'splits'] == 'training':\n",
    "            label_pred_mx_train[class_to_onehot[prediction.at[i,'label']]][class_to_onehot[prediction.at[i,'prediction']]] += 1\n",
    "        elif prediction.at[i,'splits'] == 'validation':\n",
    "            label_pred_mx_val[class_to_onehot[prediction.at[i,'label']]][class_to_onehot[prediction.at[i,'prediction']]] += 1\n",
    "    label_pred_mx_train = pd.DataFrame(label_pred_mx_train,dtype=int)\n",
    "    label_pred_mx_val = pd.DataFrame(label_pred_mx_val,dtype=int)        \n",
    "    label_pred_mx_train.columns = [loaders[0].dataset.label_to_target[i] for i in range(len(loaders[0].dataset.label_to_target))]\n",
    "    label_pred_mx_val.columns = [loaders[0].dataset.label_to_target[i] for i in range(len(loaders[0].dataset.label_to_target))]       \n",
    "    label_pred_mx_train['label\\pred'] = [loaders[0].dataset.label_to_target[i] for i in range(len(loaders[0].dataset.label_to_target))]\n",
    "    label_pred_mx_val['label\\pred'] = [loaders[0].dataset.label_to_target[i] for i in range(len(loaders[0].dataset.label_to_target))]        \n",
    "    label_pred_mx_train = pd.concat([label_pred_mx_train['label\\pred'],label_pred_mx_train[[loaders[0].dataset.label_to_target[i] for i in range(len(loaders[0].dataset.label_to_target))]]],axis=1)\n",
    "    label_pred_mx_val = pd.concat([label_pred_mx_val['label\\pred'],label_pred_mx_val[[loaders[0].dataset.label_to_target[i] for i in range(len(loaders[0].dataset.label_to_target))]]],axis=1)\n",
    "    label_pred_mx_train.to_excel(writer, sheet_name = 'train_label_pred',index=False)         \n",
    "    label_pred_mx_val.to_excel(writer, sheet_name = 'val_label_pred',index=False)    \n",
    "    ############################################################################################\n",
    "    writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "summary.append('traing set:')\n",
    "for i in range(len(train_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,train_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(train_acc_folds),np.std(train_acc_folds)))\n",
    "summary.append('testing set:')\n",
    "for i in range(len(test_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,test_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(test_acc_folds),np.std(test_acc_folds)))\n",
    "f1 = open(output_folder+'/summary_acc.txt','w')\n",
    "f1.write('\\n'.join(summary))\n",
    "f1.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph_kernel == 'Cluster-GCN':\n",
    "    tilde_A = '(D+I)^(-1)(A+I)'\n",
    "else:\n",
    "    tilde_A = 'A+I'\n",
    "\n",
    "if graph_kernel == 'Normalized-Laplacian':\n",
    "    kernel_description = 'tilde(D)^(-1/2)tilde(A)tilde(D)^(-1/2)'\n",
    "elif graph_kernel == 'Normalization':\n",
    "    kernel_description = 'tilde(D)^(-1)tilde(A)'\n",
    "elif graph_kernel == 'Cluster-GCN':\n",
    "    kernel_description = 'tilde(A)+lambda*diag(tilde(A)); default lambda=1'\n",
    "model_structure_full = []\n",
    "\n",
    "for j in range(len(model.gconv)):\n",
    "    model_structure = str(model.gconv[j]).split('\\n')\n",
    "    for i in range(len(model_structure)):\n",
    "        if i != len(model_structure)-1:\n",
    "            model_structure_full.append(model_structure[i])\n",
    "            \n",
    "    model_structure_full.append('  (adjacency matrix): tilde(A) = %s'%(tilde_A))\n",
    "    model_structure_full.append('  (graph kernel): '+kernel_description)\n",
    "    model_structure_full.append('  (connection): '+connection)\n",
    "    model_structure_full.append(')')\n",
    "\n",
    "\n",
    "layer_info = len(filters_gcn)-1\n",
    "pooling_description = 'concatenate(%s pooling) over nodes by GCN layer%s'%(pooling_method,layer_info)\n",
    "\n",
    "    \n",
    "model_structure_full.append(' '*int(len(pooling_description)/2) +'|||')\n",
    "model_structure_full.append(' '*int(len(pooling_description)/2) +'vvv')\n",
    "model_structure_full.append(pooling_description)  \n",
    "model_structure_full.append(' '*int(len(pooling_description)/2) +'|||')\n",
    "model_structure_full.append(' '*int(len(pooling_description)/2) +'vvv')\n",
    "\n",
    "\n",
    "fully_connected = []\n",
    "for i in range(len(model.fc)):\n",
    "    if i%3==1:\n",
    "        fully_connected.append('FullyConnected(')\n",
    "        fully_connected.append('  (fc): %s'%model.fc[i])\n",
    "        fully_connected.append('  (dropout): %s'%model.fc[i-1])\n",
    "        if i != len(model.fc)-1:\n",
    "            fully_connected.append('  (activation): %s'%model.fc[i+1])\n",
    "        else:\n",
    "            fully_connected.append('  (activation): softmax')\n",
    "        fully_connected.append(')')\n",
    "model_structure_full.extend(fully_connected)\n",
    "\n",
    "\n",
    "f1 = open(output_folder+'/model.txt','w')\n",
    "f1.write('\\n'.join(model_structure_full))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
