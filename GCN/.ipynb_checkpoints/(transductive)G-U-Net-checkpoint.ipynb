{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cora'\n",
    "n_folds = 1\n",
    "training_size = 0.8\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "wdecay = 5e-4\n",
    "model_name = 'Graph-U-Net'\n",
    "#downGCN\n",
    "filters_gcn = [64,32,16]\n",
    "A_normalized_method = 'Normalized-Laplacian' #'Normalization'\n",
    "gcn_activation = eval('nn.'+'ReLU'+'(inplace=True)')#ELU #Identity\n",
    "dropout_gcn = 0.3\n",
    "gcn_bias = True\n",
    "#gPool\n",
    "pooling_ratios = [0.8,0.8]\n",
    "assert len(filters_gcn)==(len(pooling_ratios)+1), \"len(filters_gcn) should equal len(pooling_ratios)-1\"\n",
    "activation_gPool= eval('torch.'+'sigmoid')#tanh #sigmoid\n",
    "dropout_gPool=0.3\n",
    "bias_gPool=True\n",
    "#gUnpool+upGCN\n",
    "filters_up_gcn = [16,12] #same as gPool Layer \n",
    "if filters_up_gcn == 'same_as_gPool_layer':\n",
    "    filters_up_gcn = filters_gcn[:-1]\n",
    "    filters_up_gcn.reverse()\n",
    "\n",
    "#finalGCN\n",
    "final_gcn = True\n",
    "\n",
    "    \n",
    "seed = 1\n",
    "if seed in ['Random','random'] :\n",
    "    seed = random.randrange(0, 10000, 1)\n",
    "else:\n",
    "    seed = int(seed)\n",
    "log_interval = 50\n",
    "  \n",
    "output_url = './cora'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = sorted(list(set(labels)))\n",
    "    if '9999' in classes:\n",
    "        classes.remove('9999')\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    onehot_to_class = {np.where(np.identity(len(classes))[i, :])[0][0]:c for i, c in enumerate(classes)}\n",
    "    classes_dict['9999'] = [0]+[0]*(len(classes)-1)\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),dtype=np.int32)\n",
    "    return labels_onehot, onehot_to_class\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    #csr matrix轉回coo matrix\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    #indices:儲存row和col\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    #values:儲存非0數值\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    if mx.toarray()[0][0] == 0:\n",
    "        mx = mx + sp.eye(mx.shape[0])\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "def laplacian(mx):\n",
    "    \"\"\"compute L=D^-0.5 * (mx) * D^-0.5\"\"\"\n",
    "    if mx.toarray()[0][0] == 0:\n",
    "        mx = mx + sp.eye(mx.shape[0])    \n",
    "    degree = np.array(mx.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    laplacian_mx = d_hat.dot(mx).dot(d_hat)\n",
    "    return laplacian_mx\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct*100 / len(labels)\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"\",n_folds=None,training_size=None,A_normalized_method=None):\n",
    "    print('loading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    #idx_features\n",
    "    idx_features_url = path+'/'+list(filter(lambda f: f.find('_node_features') >= 0, files))[0]\n",
    "    f1 = open(idx_features_url)\n",
    "    if ',' in f1.readline():\n",
    "        idx_features = np.genfromtxt(idx_features_url,delimiter=',',dtype=np.dtype(str))\n",
    "    else:\n",
    "        idx_features = np.genfromtxt(idx_features_url,dtype=np.dtype(str))\n",
    "\n",
    "    #features (normalized)\n",
    "    features = sp.csr_matrix(idx_features[:, 1:], dtype=np.float32)\n",
    "    #features = normalize_features(features)\n",
    "\n",
    "    #labels (one-hot)\n",
    "    labels_url = path+'/'+list(filter(lambda f: f.find('_node_label') >= 0, files))[0]\n",
    "    labels, onehot_to_class = encode_onehot(np.genfromtxt(labels_url,dtype=np.dtype(str)))\n",
    "    labels_name = np.genfromtxt(labels_url,dtype=np.dtype(str))\n",
    "    #ID\n",
    "    ID = idx_features[:, 0]\n",
    "    ID_to_idx = {j: i for i, j in enumerate(ID)}\n",
    "    idx_to_ID = {i: j for i, j in enumerate(ID)}\n",
    "\n",
    "    #edges\n",
    "    files = os.listdir(path)\n",
    "    edges_url = path+'/'+list(filter(lambda f: f.find('_A') >= 0, files))[0]\n",
    "    f1 = open(edges_url)\n",
    "    if ',' in f1.readline():\n",
    "        edges_unordered = np.genfromtxt(edges_url,delimiter=',',dtype=np.dtype(str))\n",
    "    else:\n",
    "        edges_unordered = np.genfromtxt(edges_url,dtype=np.dtype(str))\n",
    "    f1.close()\n",
    "    edges = np.array(list(map(ID_to_idx.get, edges_unordered.flatten())),dtype=np.int32).reshape(edges_unordered.shape)\n",
    "\n",
    "    #A (symmetric)\n",
    "    edges = edges[edges[:,0]!=edges[:,1]]\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    \n",
    "    \n",
    "    #setting training set & val set\n",
    "    labels_df = pd.DataFrame(labels)\n",
    "    gp = set(labels_df[labels_df.sum(1)!=0].index)\n",
    "    idx_unknown = list(labels_df[labels_df.sum(1)==0].index)\n",
    "\n",
    "    assert n_folds >= 1, \"'n_folds' should >= 1\"\n",
    "    if n_folds == 1:\n",
    "        #size\n",
    "        known_size = len(gp)\n",
    "        train_size = round(known_size*training_size)\n",
    "        #sampling\n",
    "        idx_train = set(random.sample(gp,k=train_size))\n",
    "        idx_val = gp - idx_train\n",
    "        #tolist\n",
    "        idx_train = torch.LongTensor(sorted(list(idx_train)))\n",
    "        idx_val = torch.LongTensor(sorted(list(idx_val)))\n",
    "        #to Multiple list\n",
    "        idx_train_ls = [idx_train]\n",
    "        idx_val_ls = [idx_val]\n",
    "    else:\n",
    "        #size\n",
    "        gp_full = set(labels_df[labels_df.sum(1)!=0].index)\n",
    "        known_size = len(gp)\n",
    "        val_size = round(known_size*(1/n_folds))\n",
    "\n",
    "        idx_val_ls = []\n",
    "        idx_train_ls = []\n",
    "        for i in range(n_folds-1):\n",
    "            #sampling\n",
    "            idx_val = set(random.sample(gp,k=val_size))\n",
    "            idx_train = gp_full - idx_val\n",
    "            idx_val_ls.append(torch.LongTensor(sorted(list(idx_val))))\n",
    "            idx_train_ls.append(torch.LongTensor(sorted(list(idx_train))))\n",
    "            gp = gp - idx_val\n",
    "        idx_val_ls.append(torch.LongTensor(sorted(list(gp))))\n",
    "        idx_train_ls.append(torch.LongTensor(sorted(list(gp_full-gp))))\n",
    "\n",
    "    #轉Tensor\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels_location = []\n",
    "    for i in labels:\n",
    "        if sum(i) == 0:\n",
    "            labels_location.append(9999)\n",
    "        else:\n",
    "            labels_location.append(np.where(i)[0][0])\n",
    "    labels = torch.LongTensor(np.array(labels_location))\n",
    "    #adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    \n",
    "    #n_class\n",
    "    n_class = len(onehot_to_class)\n",
    "\n",
    "    return adj, features, labels, idx_train_ls, idx_val_ls, ID, labels_name, n_class, onehot_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train_ls, idx_val_ls, ID, labels_name, n_class, onehot_to_class = load_data(path=path, n_folds=n_folds, training_size=training_size,A_normalized_method=A_normalized_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN layers and models\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_features,\n",
    "                out_features,\n",
    "                activation=None,\n",
    "                dropout_gcn=0,\n",
    "                gcn_bias=None):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=in_features, out_features=out_features,bias=gcn_bias)\n",
    "        self.activation = activation\n",
    "        self.drop = nn.Dropout(p=dropout_gcn) if dropout_gcn > 0.0 else nn.Identity()\n",
    "        if A_normalized_method == 'Normalization':\n",
    "            self.kernel = normalize\n",
    "        elif A_normalized_method == 'Normalized-Laplacian':  \n",
    "            self.kernel = laplacian\n",
    "            \n",
    "    def forward(self, data):#要coo_matrix才能用self.kernel\n",
    "        x, A = data[:2]\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.spmm(sparse_mx_to_torch_sparse_tensor(self.kernel(A)),x)\n",
    "        x = self.activation(x) \n",
    "        return (x, A)\n",
    "    \n",
    "class gPool(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_features,\n",
    "                pooling_ratio=None,\n",
    "                dropout_gPool=0,\n",
    "                bias_gPool=None,\n",
    "                activation_gPool=None):\n",
    "        super(gPool,self).__init__()\n",
    "        self.fc = nn.Linear(in_features=in_features, out_features=1,bias=bias_gPool)\n",
    "        self.activation = activation_gPool\n",
    "        self.drop = nn.Dropout(p=dropout_gPool) if dropout_gPool > 0.0 else nn.Identity()\n",
    "        self.pooling_ratio = pooling_ratio\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, A = data[:2]\n",
    "        score = self.drop(x)\n",
    "        score = self.fc(score)\n",
    "        score = score / torch.sum(score**2).view(1, 1)**0.5\n",
    "        score = score.squeeze()\n",
    "        score = self.activation(score)\n",
    "        values, idx = torch.topk(score, max(2,int(self.pooling_ratio*x.shape[0])))\n",
    "        new_x = x[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_x = torch.mul(new_x, values)\n",
    "        new_A = A.todense()[idx,:]\n",
    "        new_A = new_A[:,idx]\n",
    "        new_A = sp.coo_matrix(new_A)\n",
    "        return (new_x, new_A), idx\n",
    "class gUnpool(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(gUnpool,self).__init__()\n",
    "        \n",
    "    def forward(self, data, up_A, idx):\n",
    "        x, A = data\n",
    "        new_x = x.new_zeros(up_A.shape[0], x.shape[1])\n",
    "        new_x[idx] = x\n",
    "        return (new_x, up_A)\n",
    "        \n",
    "class GraphUnet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features, #n_class\n",
    "                 filters_gcn=None,\n",
    "                 dropout_gcn=0,\n",
    "                 gcn_bias=True,\n",
    "                 gcn_activation=None,\n",
    "                 pooling_ratios=None,\n",
    "                 dropout_gPool=0,\n",
    "                 bias_gPool=None,\n",
    "                 activation_gPool=None,\n",
    "                 filters_up_gcn=None):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.hidden = filters_gcn\n",
    "\n",
    "        # down GCN\n",
    "        self.down_gconv = nn.ModuleList([GraphConv(in_features=in_features if layer == 0 else self.hidden[layer - 1], \n",
    "                                                   out_features=f, \n",
    "                                                   activation=gcn_activation,\n",
    "                                                   dropout_gcn=dropout_gcn,\n",
    "                                                   gcn_bias=gcn_bias) for layer, f in enumerate(self.hidden)])    \n",
    "        #gPool\n",
    "        self.gPool = nn.ModuleList([gPool(in_features=self.hidden[i],\n",
    "                                          pooling_ratio=pooling_ratios[i],\n",
    "                                          dropout_gPool=dropout_gPool,\n",
    "                                          bias_gPool=bias_gPool,\n",
    "                                          activation_gPool=activation_gPool) for i in range(len(self.hidden)-1)])\n",
    "        #gUnpool\n",
    "        self.gUnpool = nn.ModuleList([gUnpool() for layer, f in enumerate(filters_up_gcn)])\n",
    "        \n",
    "        # up GCN\n",
    "        if final_gcn:\n",
    "            self.up_gconv = nn.ModuleList([GraphConv(in_features=filters_gcn[-1] if layer == 0 else filters_up_gcn[layer - 1], \n",
    "                                                     out_features=f, \n",
    "                                                     activation=gcn_activation,\n",
    "                                                     dropout_gcn=dropout_gcn,\n",
    "                                                     gcn_bias=gcn_bias) for layer, f in enumerate(filters_up_gcn)])  \n",
    "            # final GCN\n",
    "            self.final_gconv = nn.ModuleList([GraphConv(in_features=filters_up_gcn[-1],\n",
    "                                                        out_features=out_features,\n",
    "                                                        activation=nn.Identity(),\n",
    "                                                        dropout_gcn=dropout_gcn,\n",
    "                                                        gcn_bias=gcn_bias)])\n",
    "        else:\n",
    "            self.up_gconv = nn.ModuleList([GraphConv(in_features=filters_gcn[-1] if layer == 0 else filters_up_gcn[layer - 1], \n",
    "                                                     out_features=f if layer < len(filters_up_gcn)-1 else out_features, \n",
    "                                                     activation=gcn_activation if layer < len(filters_up_gcn)-1 else nn.Identity(),\n",
    "                                                     dropout_gcn=dropout_gcn,\n",
    "                                                     gcn_bias=gcn_bias) for layer, f in enumerate(filters_up_gcn)])         \n",
    "    def forward(self, data):\n",
    "        #GCN-->gPool-->GCN-->gPool-->.......-->GCN\n",
    "        up_idx = []\n",
    "        up_As = [data[1]]\n",
    "        for i in range(len(self.down_gconv)):\n",
    "            data = self.down_gconv[i](data)\n",
    "            if i != len(self.gPool):\n",
    "                data, idx = self.gPool[i](data)\n",
    "                up_idx.append(idx)\n",
    "                up_As.append(data[1])\n",
    "        up_idx.reverse()\n",
    "        up_As = up_As[:-1]\n",
    "        up_As.reverse()\n",
    "        #gUnpool-->GCN-->......-->gUnpool-->GCN\n",
    "        for i in range(len(self.up_gconv)):\n",
    "            data = self.gUnpool[i](data,up_As[i],up_idx[i])\n",
    "            data = self.up_gconv[i](data)\n",
    "        #final GCN\n",
    "        if final_gcn:\n",
    "            for layer, gconv in enumerate(self.final_gconv):\n",
    "                x = gconv(data)[0]\n",
    "        else:\n",
    "            x = data[0]\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphUnet(\n",
      "  (down_gconv): ModuleList(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=1433, out_features=64, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): GraphConv(\n",
      "      (fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): GraphConv(\n",
      "      (fc): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gPool): ModuleList(\n",
      "    (0): gPool(\n",
      "      (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): gPool(\n",
      "      (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gUnpool): ModuleList(\n",
      "    (0): gUnpool()\n",
      "    (1): gUnpool()\n",
      "  )\n",
      "  (up_gconv): ModuleList(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): GraphConv(\n",
      "      (fc): Linear(in_features=16, out_features=12, bias=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_gconv): ModuleList(\n",
      "    (0): GraphConv(\n",
      "      (fc): Linear(in_features=12, out_features=7, bias=True)\n",
      "      (activation): Identity()\n",
      "      (drop): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "FOLD 1\n",
      "training:2166/2707\n",
      "validation:541/2707\n",
      "Epoch: 0001 loss_train: 1.9361 acc_train: 16.48% loss_val: 1.9326 acc_val: 17.38% time: 1.0372s\n",
      "Epoch: 0050 loss_train: 1.8368 acc_train: 30.47% loss_val: 1.8569 acc_val: 29.02% time: 1.0183s\n",
      "Epoch: 0100 loss_train: 1.8374 acc_train: 30.47% loss_val: 1.8554 acc_val: 29.02% time: 0.9641s\n"
     ]
    }
   ],
   "source": [
    "train_acc_folds = []\n",
    "val_acc_folds = []\n",
    "for fold_id in range(n_folds):\n",
    "    model = GraphUnet(in_features=features.shape[1],\n",
    "                out_features=n_class,\n",
    "                filters_gcn=filters_gcn,\n",
    "                dropout_gcn=dropout_gcn,\n",
    "                gcn_bias=gcn_bias,\n",
    "                gcn_activation=gcn_activation,\n",
    "                pooling_ratios=pooling_ratios,\n",
    "                dropout_gPool=dropout_gPool,\n",
    "                bias_gPool=bias_gPool,\n",
    "                activation_gPool=activation_gPool,\n",
    "                filters_up_gcn=filters_up_gcn)  \n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=wdecay)    \n",
    "    #if fold_id == 0:\n",
    "    print(model)\n",
    "    print('\\nFOLD', fold_id+1)\n",
    "    print('training:%s/%s'%(len(idx_train_ls[fold_id]),len(idx_train_ls[fold_id])+len(idx_val_ls[fold_id])))\n",
    "    print('validation:%s/%s'%(len(idx_val_ls[fold_id]),len(idx_train_ls[fold_id])+len(idx_val_ls[fold_id])))\n",
    "    Loss_Train = []\n",
    "    Acc_Train = []\n",
    "    Loss_Val = []\n",
    "    Acc_Val = []\n",
    "    def train(epoch):\n",
    "        t = time.time()\n",
    "        #training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        data = (features,adj)\n",
    "        output = model(data)\n",
    "        loss_train = F.nll_loss(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]]) \n",
    "        acc_train = accuracy(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]])\n",
    "        Loss_Train.append(loss_train.tolist())\n",
    "        Acc_Train.append(acc_train.tolist())\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        data = (features,adj)\n",
    "        output = model(data)\n",
    "        loss_val = F.nll_loss(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]]) \n",
    "        acc_val = accuracy(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        Loss_Val.append(loss_val.tolist())\n",
    "        Acc_Val.append(acc_val.tolist())\n",
    "        if ((epoch+1) % log_interval == 0) or (epoch+1==epochs)or (epoch==0):\n",
    "            print('Epoch: {:04d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'acc_train: {:.2f}%'.format(acc_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_val: {:.2f}%'.format(acc_val.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    prediction = []\n",
    "    prob = []\n",
    "    def test():\n",
    "        model.eval()\n",
    "        data = (features,adj)\n",
    "        output = model(data)\n",
    "        prob.append(output)\n",
    "        loss_val = F.nll_loss(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        acc_train = accuracy(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]])\n",
    "        acc_val = accuracy(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        train_acc_folds.append(acc_train)\n",
    "        val_acc_folds.append(acc_val)\n",
    "        preds = output.max(1)[1].type_as(labels)\n",
    "        preds = preds.tolist()\n",
    "        preds = [onehot_to_class[i] for i in preds]        \n",
    "        prediction.extend(preds)\n",
    "        print(\"Validation set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_val.item()),\n",
    "              \"accuracy= {:.2f}%\".format(acc_val.item()))\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    test()\n",
    "    \n",
    "    #########################################################################\n",
    "    writer = pd.ExcelWriter(output_url+'/result_fold%s.xlsx'%(fold_id+1), engine = 'xlsxwriter')\n",
    "    training_df = pd.DataFrame(zip(list(range(1,epochs+1)),Loss_Train,Acc_Train,Loss_Val,Acc_Val))\n",
    "    training_df.columns = ['epoch','training loss','training accuracy','validation loss','validation accuracy']\n",
    "    training_df.to_excel(writer, sheet_name = 'training&validation',index=False)\n",
    "    #########################################################################\n",
    "    for i in range(len(model.down_gconv)):\n",
    "        f_down_GCN_weight = pd.DataFrame(np.matrix(model.down_gconv[i].fc.weight.tolist()).T)\n",
    "        f_down_GCN_weight.to_excel(writer,sheet_name='downGCN%s_weight'%i,header=False,index=False)\n",
    "        if gcn_bias:\n",
    "            f_down_GCN_bias = pd.DataFrame(np.matrix(model.down_gconv[i].fc.bias.tolist()).T)\n",
    "            f_down_GCN_bias.to_excel(writer,sheet_name='downGCN%s_bias'%i,header=False,index=False) \n",
    "        if i != len(model.gPool):\n",
    "            f_gPool_weight = pd.DataFrame(np.matrix(model.gPool[i].fc.weight.tolist()).T)\n",
    "            f_gPool_weight.to_excel(writer,sheet_name='gPool%s_weight'%i,header=False,index=False)\n",
    "            if bias_gPool:\n",
    "                f_gPool_bias = pd.DataFrame(np.matrix(model.gPool[i].fc.bias.tolist()).T)\n",
    "                f_gPool_bias.to_excel(writer,sheet_name='gPool%s_bias'%i,header=False,index=False) \n",
    "    for i in range(len(model.up_gconv)):\n",
    "        f_up_gconv_weight = pd.DataFrame(np.matrix(model.up_gconv[i].fc.weight.tolist()).T) \n",
    "        f_up_gconv_weight.to_excel(writer,sheet_name='upGCN%s_weight'%i,header=False,index=False)\n",
    "        if gcn_bias:\n",
    "            f_up_GCN_bias = pd.DataFrame(np.matrix(model.up_gconv[i].fc.bias.tolist()).T)\n",
    "            f_up_GCN_bias.to_excel(writer,sheet_name='upGCN%s_bias'%i,header=False,index=False) \n",
    "    if final_gcn:\n",
    "        for i in range(len(model.final_gconv)):\n",
    "            f_final_gconv_weight = pd.DataFrame(np.matrix(model.final_gconv[i].fc.weight.tolist()).T) \n",
    "            f_final_gconv_weight.to_excel(writer,sheet_name='GCN%s_weight'%i,header=False,index=False)       \n",
    "            if gcn_bias:\n",
    "                f_final_GCN_bias = pd.DataFrame(np.matrix(model.final_gconv[i].fc.bias.tolist()).T)\n",
    "                f_final_GCN_bias.to_excel(writer,sheet_name='GCN%s_bias'%i,header=False,index=False) \n",
    "    ################################################################################\n",
    "    split = np.empty((len(labels_name)),dtype=np.object)\n",
    "    for i in idx_train_ls[fold_id].tolist():\n",
    "        split[i] = 'training'\n",
    "    for i in idx_val_ls[fold_id].tolist():\n",
    "        split[i] = 'validation'\n",
    "    corrects = []\n",
    "    for i,j in zip(prediction,labels_name):\n",
    "        if j == '9999':\n",
    "            corrects.append('')\n",
    "        elif i == j:\n",
    "            corrects.append(1)\n",
    "        elif i != j:\n",
    "            corrects.append(0)\n",
    "    preds_df = pd.DataFrame(zip(ID,prediction,labels_name,split,corrects))\n",
    "    preds_df.columns = ['ID','prediction','label','splits','correct']\n",
    "    \n",
    "    class_prob = pd.DataFrame(np.exp(prob[0].tolist())*100)\n",
    "    columns = []\n",
    "    for c in class_prob.columns:\n",
    "        class_prob[c] = class_prob[c].map('{:,.2f}%'.format)\n",
    "        columns.append('p('+onehot_to_class[c]+')')\n",
    "    class_prob.columns = columns\n",
    "    preds_df = pd.concat([preds_df,class_prob], axis=1)\n",
    "    \n",
    "    preds_df.to_excel(writer, sheet_name = 'prediction',index=False)\n",
    "    ################################################################################\n",
    "    #sampling info.\n",
    "    class_number_train = []\n",
    "    class_proportion_train = []\n",
    "    class_number_val = []\n",
    "    class_proportion_val = []    \n",
    "    \n",
    "    label_set = sorted(list(set(labels_name)))\n",
    "    if '9999' in label_set:\n",
    "        label_set.remove('9999')\n",
    "    for i in label_set:\n",
    "        class_number_train.append(sum(labels_name[idx_train_ls[fold_id]]== i))\n",
    "        class_proportion_train.append('%.1f%%'%(sum(labels_name[idx_train_ls[fold_id]]== i)*100/len(idx_train_ls[fold_id])))\n",
    "        class_number_val.append(sum(labels_name[idx_val_ls[fold_id]]== i))\n",
    "        class_proportion_val.append('%.1f%%'%(sum(labels_name[idx_val_ls[fold_id]]== i)*100/len(idx_val_ls[fold_id])))  \n",
    "    sampling_info = pd.DataFrame(zip(label_set,class_number_train,class_proportion_train,class_number_val,class_proportion_val))\n",
    "    sampling_info.columns = ['label','training set','proportion of training set','validation set','proportion of validation set']\n",
    "    sampling_info.to_excel(writer, sheet_name = 'sampling_info',index=False)    \n",
    "    #################################################################################################\n",
    "    #class_acc\n",
    "    Number_Train = []\n",
    "    Correct_Train = []\n",
    "    Number_Val = []\n",
    "    Correct_Val = []\n",
    "    Values = onehot_to_class.values()\n",
    "    for i in Values:\n",
    "        Correct_Train.append(len(preds_df[(preds_df['correct']==1)&(preds_df['label']==i)&(preds_df['splits']=='training')]))\n",
    "        Number_Train.append(len(preds_df[(preds_df['label']==i)&(preds_df['splits']=='training')]))\n",
    "        Correct_Val.append(len(preds_df[(preds_df['correct']==1)&(preds_df['label']==i)&(preds_df['splits']=='validation')]))\n",
    "        Number_Val.append(len(preds_df[(preds_df['label']==i)&(preds_df['splits']=='validation')]))\n",
    "        Rate_Train = np.array(Correct_Train)*100/np.array(Number_Train)\n",
    "        Rate_Val = np.array(Correct_Val)*100/np.array(Number_Val)\n",
    "    Class_Prob = pd.DataFrame(zip(Values,Correct_Train,Number_Train,Rate_Train,Correct_Val,Number_Val,Rate_Val))\n",
    "    Class_Prob.columns = ['class','train_correct','train_number','train_accuracy','validation_correct','validation_number','validation_accuracy']\n",
    "    Class_Prob['train_accuracy'] = Class_Prob['train_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob['validation_accuracy'] = Class_Prob['validation_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob.to_excel(writer, sheet_name = 'class_acc',index=False)      \n",
    "    #################################################################################################\n",
    "    #label_pred_mx_train\n",
    "    #label_pred_mx_val\n",
    "    class_to_onehot = dict(zip([onehot_to_class[i] for i in range(len(onehot_to_class))],range(len(onehot_to_class))))\n",
    "    label_pred_mx_train = np.zeros([len(onehot_to_class),len(onehot_to_class)])\n",
    "    label_pred_mx_val = np.zeros([len(onehot_to_class),len(onehot_to_class)])\n",
    "    for i in range(len(preds_df)):\n",
    "        if preds_df.at[i,'label'] == '9999':\n",
    "            continue\n",
    "        if preds_df.at[i,'splits'] == 'training':\n",
    "            label_pred_mx_train[class_to_onehot[preds_df.at[i,'label']]][class_to_onehot[preds_df.at[i,'prediction']]] += 1\n",
    "        elif preds_df.at[i,'splits'] == 'validation':\n",
    "            label_pred_mx_val[class_to_onehot[preds_df.at[i,'label']]][class_to_onehot[preds_df.at[i,'prediction']]] += 1\n",
    "    label_pred_mx_train = pd.DataFrame(label_pred_mx_train,dtype=int)\n",
    "    label_pred_mx_val = pd.DataFrame(label_pred_mx_val,dtype=int)        \n",
    "    label_pred_mx_train.columns = [onehot_to_class[i] for i in range(len(onehot_to_class))]\n",
    "    label_pred_mx_val.columns = [onehot_to_class[i] for i in range(len(onehot_to_class))]       \n",
    "    label_pred_mx_train['label\\pred'] = [onehot_to_class[i] for i in range(len(onehot_to_class))]\n",
    "    label_pred_mx_val['label\\pred'] = [onehot_to_class[i] for i in range(len(onehot_to_class))]        \n",
    "    label_pred_mx_train = pd.concat([label_pred_mx_train['label\\pred'],label_pred_mx_train[[onehot_to_class[i] for i in range(len(onehot_to_class))]]],axis=1)\n",
    "    label_pred_mx_val = pd.concat([label_pred_mx_val['label\\pred'],label_pred_mx_val[[onehot_to_class[i] for i in range(len(onehot_to_class))]]],axis=1)\n",
    "    label_pred_mx_train.to_excel(writer, sheet_name = 'train_label_pred',index=False)         \n",
    "    label_pred_mx_val.to_excel(writer, sheet_name = 'val_label_pred',index=False)          \n",
    "    writer.save()\n",
    "\n",
    "\n",
    "    Epoch = range(1,epochs+1)\n",
    "    plt.plot(Epoch,Loss_Train,label='training loss')\n",
    "    plt.plot(Epoch,Loss_Val,label='validation loss')\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('loss',fontsize=18)    \n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_url+'/loss_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.plot(Epoch,np.array(Acc_Train)/100,label='training accuracy')\n",
    "    plt.plot(Epoch,np.array(Acc_Val)/100,label='validation accuracy')\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('accuracy',fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_url+'/acc_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "summary.append('traing set:')\n",
    "for i in range(len(train_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,train_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(train_acc_folds),np.std(train_acc_folds)))\n",
    "summary.append('validation set:')\n",
    "for i in range(len(val_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,val_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(val_acc_folds),np.std(val_acc_folds)))\n",
    "f1 = open(output_url+'/summary_acc.txt','w')\n",
    "f1.write('\\n'.join(summary))\n",
    "f1.close()    \n",
    "########################################################################\n",
    "model_structure = []\n",
    "model_structure.append('gPooling(')\n",
    "for i in range(len(model.down_gconv)):\n",
    "    model_structure.append('  '+'\\n  '.join(('(%s): '%i+str(model.down_gconv[i])).split('\\n')))\n",
    "    if i != len(model.gPool):\n",
    "        tmp = ('(%s): '%i+str(model.gPool[i])).split('\\n')[:-1]\n",
    "        tmp.append('  (top k): %s'%pooling_ratios[i])\n",
    "        tmp.append('  (activation): %s'%activation_gPool.__name__)\n",
    "        tmp.append(')')\n",
    "        model_structure.append('  '+'\\n  '.join(tmp))\n",
    "model_structure.append(')')     \n",
    "\n",
    "model_structure.append('gUnpooling(')\n",
    "for i in range(len(model.up_gconv)):\n",
    "    model_structure.append('  '+'\\n  '.join(('(%s): '%i+str(model.gUnpool[i])).split('\\n')))\n",
    "    model_structure.append('  '+'\\n  '.join(('(%s): '%i+str(model.up_gconv[i])).split('\\n')))   \n",
    "model_structure.append(')')        \n",
    "\n",
    "if final_gcn:\n",
    "    model_structure.append('GCN(')\n",
    "    for i in range(len(model.final_gconv)):\n",
    "        model_structure.append('  '+'\\n  '.join(('(%s): '%i+str(model.final_gconv[i])).split('\\n')))\n",
    "    model_structure.append(')')      \n",
    "tmp = model_structure[-2].split('\\n')\n",
    "tmp[-3]='    (activation): Softmax()'\n",
    "model_structure[-2] = '\\n'.join(tmp)\n",
    "f1 = open(output_url+'/model.txt','w')\n",
    "f1.write('\\n'.join(model_structure))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
