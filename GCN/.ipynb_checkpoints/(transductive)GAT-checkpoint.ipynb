{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cora'\n",
    "n_folds = 1\n",
    "training_size = 0.05\n",
    "epochs = 2\n",
    "lr = 0.01\n",
    "wdecay = 5e-4\n",
    "model_name = 'GAT'\n",
    "#GAT\n",
    "n_hidden = 8\n",
    "dropout_gat = 0.6\n",
    "alpha_leakyReLU = 0.2\n",
    "n_att = 8\n",
    "\n",
    "#\n",
    "seed = 'Random'\n",
    "if seed in ['Random','random']:\n",
    "    seed = random.randrange(0, 1000000, 1)\n",
    "else:\n",
    "    seed = int(seed)\n",
    "log_interval = 1\n",
    "  \n",
    "output_url = './cora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = sorted(list(set(labels)))\n",
    "    if '9999' in classes:\n",
    "        classes.remove('9999')\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    onehot_to_class = {np.where(np.identity(len(classes))[i, :])[0][0]:c for i, c in enumerate(classes)}\n",
    "    classes_dict['9999'] = [0]+[0]*(len(classes)-1)\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),dtype=np.int32)\n",
    "    return labels_onehot, onehot_to_class\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    #csr matrix轉回coo matrix\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    #indices:儲存row和col\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    #values:儲存非0數值\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    if mx.toarray()[0][0] == 0:\n",
    "        mx = mx + sp.eye(mx.shape[0])\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "def laplacian(mx):\n",
    "    \"\"\"compute L=D^-0.5 * (mx) * D^-0.5\"\"\"\n",
    "    if mx.toarray()[0][0] == 0:\n",
    "        mx = mx + sp.eye(mx.shape[0])    \n",
    "    degree = np.array(mx.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    laplacian_mx = d_hat.dot(mx).dot(d_hat)\n",
    "    return laplacian_mx\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct*100 / len(labels)\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"\",n_folds=None,training_size=None):\n",
    "    print('loading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    #idx_features\n",
    "    idx_features_url = path+'/'+list(filter(lambda f: f.find('_node_features') >= 0, files))[0]\n",
    "    f1 = open(idx_features_url)\n",
    "    if ',' in f1.readline():\n",
    "        idx_features = np.genfromtxt(idx_features_url,delimiter=',',dtype=np.dtype(str))\n",
    "    else:\n",
    "        idx_features = np.genfromtxt(idx_features_url,dtype=np.dtype(str))\n",
    "\n",
    "    #features (normalized)\n",
    "    features = sp.csr_matrix(idx_features[:, 1:], dtype=np.float32)\n",
    "    #features = normalize_features(features)\n",
    "\n",
    "    #labels (one-hot)\n",
    "    labels_url = path+'/'+list(filter(lambda f: f.find('_node_label') >= 0, files))[0]\n",
    "    labels, onehot_to_class = encode_onehot(np.genfromtxt(labels_url,dtype=np.dtype(str)))\n",
    "    labels_name = np.genfromtxt(labels_url,dtype=np.dtype(str))\n",
    "    #ID\n",
    "    ID = idx_features[:, 0]\n",
    "    ID_to_idx = {j: i for i, j in enumerate(ID)}\n",
    "    idx_to_ID = {i: j for i, j in enumerate(ID)}\n",
    "\n",
    "    #edges\n",
    "    files = os.listdir(path)\n",
    "    edges_url = path+'/'+list(filter(lambda f: f.find('_A') >= 0, files))[0]\n",
    "    f1 = open(edges_url)\n",
    "    if ',' in f1.readline():\n",
    "        edges_unordered = np.genfromtxt(edges_url,delimiter=',',dtype=np.dtype(str))\n",
    "    else:\n",
    "        edges_unordered = np.genfromtxt(edges_url,dtype=np.dtype(str))\n",
    "    f1.close()\n",
    "    edges = np.array(list(map(ID_to_idx.get, edges_unordered.flatten())),dtype=np.int32).reshape(edges_unordered.shape)\n",
    "\n",
    "    #A (symmetric)\n",
    "    edges = edges[edges[:,0]!=edges[:,1]\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    \n",
    "    #setting training set & val set\n",
    "    labels_df = pd.DataFrame(labels)\n",
    "    gp = set(labels_df[labels_df.sum(1)!=0].index)\n",
    "    idx_unknown = list(labels_df[labels_df.sum(1)==0].index)\n",
    "\n",
    "    assert n_folds >= 1, \"'n_folds' should >= 1\"\n",
    "    if n_folds == 1:\n",
    "        #size\n",
    "        known_size = len(gp)\n",
    "        train_size = round(known_size*training_size)\n",
    "        #sampling\n",
    "        idx_train = set(random.sample(gp,k=train_size))\n",
    "        idx_val = gp - idx_train\n",
    "        #tolist\n",
    "        idx_train = torch.LongTensor(sorted(list(idx_train)))\n",
    "        idx_val = torch.LongTensor(sorted(list(idx_val)))\n",
    "        #to Multiple list\n",
    "        idx_train_ls = [idx_train]\n",
    "        idx_val_ls = [idx_val]\n",
    "    else:\n",
    "        #size\n",
    "        gp_full = set(labels_df[labels_df.sum(1)!=0].index)\n",
    "        known_size = len(gp)\n",
    "        val_size = round(known_size*(1/n_folds))\n",
    "\n",
    "        idx_val_ls = []\n",
    "        idx_train_ls = []\n",
    "        for i in range(n_folds-1):\n",
    "            #sampling\n",
    "            idx_val = set(random.sample(gp,k=val_size))\n",
    "            idx_train = gp_full - idx_val\n",
    "            idx_val_ls.append(torch.LongTensor(sorted(list(idx_val))))\n",
    "            idx_train_ls.append(torch.LongTensor(sorted(list(idx_train))))\n",
    "            gp = gp - idx_val\n",
    "        idx_val_ls.append(torch.LongTensor(sorted(list(gp))))\n",
    "        idx_train_ls.append(torch.LongTensor(sorted(list(gp_full-gp))))\n",
    "\n",
    "    #轉Tensor\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels_location = []\n",
    "    for i in labels:\n",
    "        if sum(i) == 0:\n",
    "            labels_location.append(9999)\n",
    "        else:\n",
    "            labels_location.append(np.where(labels[i])[0][0])\n",
    "    labels = torch.LongTensor(np.array(labels_location))\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    #adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    \n",
    "    #n_class\n",
    "    n_class = len(onehot_to_class)\n",
    "\n",
    "    return adj, features, labels, idx_train_ls, idx_val_ls, ID, labels_name, n_class, onehot_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5430"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges2 = edges.tolist()\n",
    "edges2.append([11111,11111])\n",
    "edges2 = np.array(edges2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 163,  402],\n",
       "       [ 163,  659],\n",
       "       [ 163, 1696],\n",
       "       ...,\n",
       "       [1887, 2258],\n",
       "       [1902, 1887],\n",
       "       [ 837, 1686]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges2[edges2[:,0]!=edges2[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train_ls, idx_val_ls, ID, labels_name, n_class, onehot_to_class = load_data(path=path, n_folds=n_folds, training_size=training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nn.Linear(in_features=features.shape[1], out_features=8,bias=False)\n",
    "a = nn.Linear(in_features=2*8, out_features=1,bias=False)\n",
    "h = W(features)\n",
    "N = h.size()[0]\n",
    "a_inp = torch.cat([h.repeat(1,N).view(N * N, -1), h.repeat(N,1)], dim=1).view(N, -1, 2*8)\n",
    "leakyrelu = nn.LeakyReLU(0.2)\n",
    "e = leakyrelu(a(a_inp).squeeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vec = -9e15*torch.ones_like(e)\n",
    "attention = torch.where(adj>0, e, zero_vec)\n",
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime = torch.matmul(attention, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 16])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([h_prime,h_prime],dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.W = nn.Linear(in_features=in_features, out_features=out_features,bias=False)\n",
    "        self.a = nn.Linear(in_features=2*out_features, out_features=1,bias=False)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.concatenate = torch.cat\n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "    def forward(self, inp, adj):\n",
    "        h = self.W(inp)\n",
    "        N = h.size()[0] #node size\n",
    "        a_inp = self.concatenate([h.repeat(1,N).view(N * N, -1), h.repeat(N,1)], dim=1).view(N, -1, 2*self.out_features)\n",
    "        e = self.leakyrelu(self.a(a_inp).squeeze(2)) #shape=[node size, node size]\n",
    "        \n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj>0, e, zero_vec) #沒鄰接:負很大，有鄰接:e\n",
    "        attention = F.softmax(attention, dim=1) #負很大->softmax->0\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        if self.concat:\n",
    "            return self.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime        \n",
    "    def extra_repr(self):\n",
    "        lines = []\n",
    "        lines.append('(hidden_features): Linear(in_features=%s, out_features=%s, bias=False)'%(self.in_features,self.out_features))\n",
    "        lines.append('(attetion): Attetion(')\n",
    "        lines.append('  (concat_ij): Concat(in_features=%s, out_features=%s*2)'%(self.out_features,self.out_features))\n",
    "        lines.append('  (a): Linear(in_features=%s, out_features=1, bias=False)'%(self.out_features*2))\n",
    "        lines.append('  (leakyrelu): LeakyReLU(negative_slope=%s)'%(self.alpha))\n",
    "        lines.append('  (concat_edges): Concat(in_features=1, out_features=1-hop edges)')\n",
    "        lines.append('  (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)')\n",
    "        lines.append('  (dropout_attetion): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append(')')\n",
    "        lines.append('(weighted_hidden_features): Matmul(attention, hidden_features)')\n",
    "        if self.concat:\n",
    "            lines.append('(elu): ELU(alpha=1.0)')        \n",
    "        lines = '\\n'.join(lines)\n",
    "        return lines\n",
    "        \n",
    "\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.nhid = nhid\n",
    "        self.nheads = nheads\n",
    "        self.nclass = nclass\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(in_features=nfeat,\n",
    "                                               out_features=nhid, \n",
    "                                               dropout=dropout, \n",
    "                                               alpha=alpha, \n",
    "                                               concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)      \n",
    "    def extra_repr(self):\n",
    "        lines = []\n",
    "        lines.append('GAT(')\n",
    "        lines.append('  (dropout): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append('->')\n",
    "        for i, att in enumerate(self.attentions):\n",
    "            lines.append('  (attention_%d): GraphAttentionLayer('%(i))\n",
    "            for j in att.extra_repr().split('\\n'):\n",
    "                lines.append('    '+j)\n",
    "            lines.append('  )')\n",
    "        lines.append('->')             \n",
    "        lines.append('  (concat_attention_0-%s): Concat(in_features=%s, out_features=%s*%slayers)'%(self.nheads-1,\n",
    "                                                                                    self.nhid, self.nhid, self.nheads))\n",
    "        lines.append('->')               \n",
    "        lines.append('  (dropout): Dropout(p=%s)'%self.dropout)\n",
    "        lines.append('->')  \n",
    "        lines.append('  (attention_out): GraphAttentionLayer(')\n",
    "        for j in self.out_att.extra_repr().split('\\n'):\n",
    "            lines.append('    '+j)\n",
    "        lines.append('  )')\n",
    "        lines.append('->')\n",
    "        lines.append('  (elu): ELU(alpha=1.0)')     \n",
    "        lines.append('->')\n",
    "        lines.append('  (softmax): Softmax(in_features=%s, out_features=%s)'%(self.nclass,self.nclass))\n",
    "        lines.append(')')\n",
    "        lines='\\n'.join(lines)\n",
    "        \n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (dropout): Dropout(p=0.6)\n",
      "->\n",
      "  (attention_0): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_1): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_2): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_3): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_4): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_5): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_6): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (attention_7): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=1433, out_features=8, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=8, out_features=8*2)\n",
      "      (a): Linear(in_features=16, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "->\n",
      "  (concat_attention_0-7): Concat(in_features=8, out_features=8*8layers)\n",
      "->\n",
      "  (dropout): Dropout(p=0.6)\n",
      "->\n",
      "  (attention_out): GraphAttentionLayer(\n",
      "    (hidden_features): Linear(in_features=64, out_features=7, bias=False)\n",
      "    (attetion): Attetion(\n",
      "      (concat_ij): Concat(in_features=7, out_features=7*2)\n",
      "      (a): Linear(in_features=14, out_features=1, bias=False)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "      (concat_edges): Concat(in_features=1, out_features=1-hop edges)\n",
      "      (softmax): Softmax(in_features=1-hop edges, out_features=1-hop edges)\n",
      "      (dropout_attetion): Dropout(p=0.6)\n",
      "    )\n",
      "    (weighted_hidden_features): Matmul(attention, hidden_features)\n",
      "  )\n",
      "->\n",
      "  (softmax): Softmax(in_features=7, out_features=7)\n",
      ")\n",
      "seed: 363606\n",
      "\n",
      "FOLD 1\n",
      "training:135/2707\n",
      "validation:2572/2707\n",
      "Epoch: 0001 loss_train: 1.9481 acc_train: 12.59% loss_val: 1.8695 acc_val: 61.47% time: 29.6327s\n",
      "Epoch: 0002 loss_train: 1.8495 acc_train: 45.93% loss_val: 1.7855 acc_val: 62.01% time: 29.1210s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 59.5477s\n",
      "Validation set results: loss= 1.7855 accuracy= 62.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chienhua\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:96: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc_folds = []\n",
    "val_acc_folds = []\n",
    "for fold_id in range(n_folds):\n",
    "    model = GAT(nfeat=features.shape[1],\n",
    "                nhid=n_hidden,\n",
    "                nclass=n_class,\n",
    "                dropout=dropout_gat,\n",
    "                alpha=alpha_leakyReLU,\n",
    "                nheads=n_att)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=wdecay)    \n",
    "    #if fold_id == 0:\n",
    "    print(model.extra_repr())\n",
    "    print('seed:',seed)\n",
    "    print('\\nFOLD', fold_id+1)\n",
    "    print('training:%s/%s'%(len(idx_train_ls[fold_id]),len(idx_train_ls[fold_id])+len(idx_val_ls[fold_id])))\n",
    "    print('validation:%s/%s'%(len(idx_val_ls[fold_id]),len(idx_train_ls[fold_id])+len(idx_val_ls[fold_id])))\n",
    "    Loss_Train = []\n",
    "    Acc_Train = []\n",
    "    Loss_Val = []\n",
    "    Acc_Val = []\n",
    "    def train(epoch):\n",
    "        t = time.time()\n",
    "        #training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features,adj)\n",
    "        loss_train = F.nll_loss(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]]) \n",
    "        acc_train = accuracy(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]])\n",
    "        Loss_Train.append(loss_train.tolist())\n",
    "        Acc_Train.append(acc_train.tolist())\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        output = model(features,adj)\n",
    "        loss_val = F.nll_loss(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]]) \n",
    "        acc_val = accuracy(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        Loss_Val.append(loss_val.tolist())\n",
    "        Acc_Val.append(acc_val.tolist())\n",
    "        if ((epoch+1) % log_interval == 0) or (epoch+1==epochs)or (epoch==0):\n",
    "            print('Epoch: {:04d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'acc_train: {:.2f}%'.format(acc_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_val: {:.2f}%'.format(acc_val.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    prediction = []\n",
    "    prob = []\n",
    "    def test():\n",
    "        model.eval()\n",
    "        output = model(features,adj)\n",
    "        prob.append(output)\n",
    "        loss_val = F.nll_loss(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        acc_train = accuracy(output[idx_train_ls[fold_id]], labels[idx_train_ls[fold_id]])\n",
    "        acc_val = accuracy(output[idx_val_ls[fold_id]], labels[idx_val_ls[fold_id]])\n",
    "        train_acc_folds.append(acc_train)\n",
    "        val_acc_folds.append(acc_val)\n",
    "        preds = output.max(1)[1].type_as(labels)\n",
    "        preds = preds.tolist()\n",
    "        preds = [onehot_to_class[i] for i in preds]        \n",
    "        prediction.extend(preds)\n",
    "        print(\"Validation set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_val.item()),\n",
    "              \"accuracy= {:.2f}%\".format(acc_val.item()))\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    test()\n",
    "    \n",
    "    #########################################################################\n",
    "    writer = pd.ExcelWriter(output_url+'/result_fold%s.xlsx'%(fold_id+1), engine = 'xlsxwriter')\n",
    "    training_df = pd.DataFrame(zip(list(range(1,epochs+1)),Loss_Train,Acc_Train,Loss_Val,Acc_Val))\n",
    "    training_df.columns = ['epoch','training loss','training accuracy','validation loss','validation accuracy']\n",
    "    training_df.to_excel(writer, sheet_name = 'training&validation',index=False)\n",
    "    #########################################################################   \n",
    "    for i in range(len(model.attentions)):\n",
    "        W_weight = pd.DataFrame(np.matrix(model.attentions[i].W.weight.tolist()).T)\n",
    "        W_weight.to_excel(writer,sheet_name='W%s_weight'%i,header=False,index=False)  \n",
    "        a_weight = pd.DataFrame(np.matrix(model.attentions[i].a.weight.tolist()).T)\n",
    "        a_weight.to_excel(writer,sheet_name='a%s_weight'%i,header=False,index=False)    \n",
    "        \n",
    "    W_weight = pd.DataFrame(np.matrix(model.out_att.W.weight.tolist()).T)\n",
    "    W_weight.to_excel(writer,sheet_name='W_out_weight',header=False,index=False)  \n",
    "    a_weight = pd.DataFrame(np.matrix(model.out_att.a.weight.tolist()).T)\n",
    "    a_weight.to_excel(writer,sheet_name='a_out_weight',header=False,index=False)   \n",
    "    ################################################################################\n",
    "    split = np.empty((len(labels_name)),dtype=np.object)\n",
    "    for i in idx_train_ls[fold_id].tolist():\n",
    "        split[i] = 'training'\n",
    "    for i in idx_val_ls[fold_id].tolist():\n",
    "        split[i] = 'validation'\n",
    "    corrects = []\n",
    "    for i,j in zip(prediction,labels_name):\n",
    "        if j == '9999':\n",
    "            corrects.append('')\n",
    "        elif i == j:\n",
    "            corrects.append(1)\n",
    "        elif i != j:\n",
    "            corrects.append(0)\n",
    "    preds_df = pd.DataFrame(zip(ID,prediction,labels_name,split,corrects))\n",
    "    preds_df.columns = ['ID','prediction','label','splits','correct']\n",
    "    \n",
    "    class_prob = pd.DataFrame(np.exp(prob[0].tolist())*100)\n",
    "    columns = []\n",
    "    for c in class_prob.columns:\n",
    "        class_prob[c] = class_prob[c].map('{:,.2f}%'.format)\n",
    "        columns.append('p('+onehot_to_class[c]+')')\n",
    "    class_prob.columns = columns\n",
    "    preds_df = pd.concat([preds_df,class_prob], axis=1)\n",
    "    \n",
    "    preds_df.to_excel(writer, sheet_name = 'prediction',index=False)\n",
    "    ################################################################################\n",
    "    #sampling info.\n",
    "    class_number_train = []\n",
    "    class_proportion_train = []\n",
    "    class_number_val = []\n",
    "    class_proportion_val = []    \n",
    "    \n",
    "    label_set = sorted(list(set(labels_name)))\n",
    "    if '9999' in label_set:\n",
    "        label_set.remove('9999')\n",
    "    for i in label_set:\n",
    "        class_number_train.append(sum(labels_name[idx_train_ls[fold_id]]== i))\n",
    "        class_proportion_train.append('%.1f%%'%(sum(labels_name[idx_train_ls[fold_id]]== i)*100/len(idx_train_ls[fold_id])))\n",
    "        class_number_val.append(sum(labels_name[idx_val_ls[fold_id]]== i))\n",
    "        class_proportion_val.append('%.1f%%'%(sum(labels_name[idx_val_ls[fold_id]]== i)*100/len(idx_val_ls[fold_id])))  \n",
    "    sampling_info = pd.DataFrame(zip(label_set,class_number_train,class_proportion_train,class_number_val,class_proportion_val))\n",
    "    sampling_info.columns = ['label','training set','proportion of training set','validation set','proportion of validation set']\n",
    "    sampling_info.to_excel(writer, sheet_name = 'sampling_info',index=False)    \n",
    "    #################################################################################################\n",
    "    #class_acc\n",
    "    Number_Train = []\n",
    "    Correct_Train = []\n",
    "    Number_Val = []\n",
    "    Correct_Val = []\n",
    "    Values = onehot_to_class.values()\n",
    "    for i in Values:\n",
    "        Correct_Train.append(len(preds_df[(preds_df['correct']==1)&(preds_df['label']==i)&(preds_df['splits']=='training')]))\n",
    "        Number_Train.append(len(preds_df[(preds_df['label']==i)&(preds_df['splits']=='training')]))\n",
    "        Correct_Val.append(len(preds_df[(preds_df['correct']==1)&(preds_df['label']==i)&(preds_df['splits']=='validation')]))\n",
    "        Number_Val.append(len(preds_df[(preds_df['label']==i)&(preds_df['splits']=='validation')]))\n",
    "        Rate_Train = np.array(Correct_Train)*100/np.array(Number_Train)\n",
    "        Rate_Val = np.array(Correct_Val)*100/np.array(Number_Val)\n",
    "    Class_Prob = pd.DataFrame(zip(Values,Correct_Train,Number_Train,Rate_Train,Correct_Val,Number_Val,Rate_Val))\n",
    "    Class_Prob.columns = ['class','train_correct','train_number','train_accuracy','validation_correct','validation_number','validation_accuracy']\n",
    "    Class_Prob['train_accuracy'] = Class_Prob['train_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob['validation_accuracy'] = Class_Prob['validation_accuracy'].map('{:,.2f}%'.format)\n",
    "    Class_Prob.to_excel(writer, sheet_name = 'class_acc',index=False)      \n",
    "    #################################################################################################\n",
    "    #label_pred_mx_train\n",
    "    #label_pred_mx_val\n",
    "    class_to_onehot = dict(zip([onehot_to_class[i] for i in range(len(onehot_to_class))],range(len(onehot_to_class))))\n",
    "    label_pred_mx_train = np.zeros([len(onehot_to_class),len(onehot_to_class)])\n",
    "    label_pred_mx_val = np.zeros([len(onehot_to_class),len(onehot_to_class)])\n",
    "    for i in range(len(preds_df)):\n",
    "        if preds_df.at[i,'label'] == '9999':\n",
    "            continue\n",
    "        if preds_df.at[i,'splits'] == 'training':\n",
    "            label_pred_mx_train[class_to_onehot[preds_df.at[i,'label']]][class_to_onehot[preds_df.at[i,'prediction']]] += 1\n",
    "        elif preds_df.at[i,'splits'] == 'validation':\n",
    "            label_pred_mx_val[class_to_onehot[preds_df.at[i,'label']]][class_to_onehot[preds_df.at[i,'prediction']]] += 1\n",
    "    label_pred_mx_train = pd.DataFrame(label_pred_mx_train,dtype=int)\n",
    "    label_pred_mx_val = pd.DataFrame(label_pred_mx_val,dtype=int)        \n",
    "    label_pred_mx_train.columns = [onehot_to_class[i] for i in range(len(onehot_to_class))]\n",
    "    label_pred_mx_val.columns = [onehot_to_class[i] for i in range(len(onehot_to_class))]       \n",
    "    label_pred_mx_train['lable\\pred'] = [onehot_to_class[i] for i in range(len(onehot_to_class))]\n",
    "    label_pred_mx_val['lable\\pred'] = [onehot_to_class[i] for i in range(len(onehot_to_class))]        \n",
    "    label_pred_mx_train = pd.concat([label_pred_mx_train['lable\\pred'],label_pred_mx_train[[onehot_to_class[i] for i in range(len(onehot_to_class))]]],axis=1)\n",
    "    label_pred_mx_val = pd.concat([label_pred_mx_val['lable\\pred'],label_pred_mx_val[[onehot_to_class[i] for i in range(len(onehot_to_class))]]],axis=1)\n",
    "    label_pred_mx_train.to_excel(writer, sheet_name = 'train_label_pred',index=False)         \n",
    "    label_pred_mx_val.to_excel(writer, sheet_name = 'val_label_pred',index=False)          \n",
    "    writer.save()\n",
    "\n",
    "    Epoch = range(1,epochs+1)\n",
    "    plt.plot(Epoch,Loss_Train,label='training loss')\n",
    "    plt.plot(Epoch,Loss_Val,label='validation loss')\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('loss',fontsize=18)    \n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_url+'/loss_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.plot(Epoch,np.array(Acc_Train)/100,label='training accuracy')\n",
    "    plt.plot(Epoch,np.array(Acc_Val)/100,label='validation accuracy')\n",
    "    plt.xlabel('epoch',fontsize=18)\n",
    "    plt.ylabel('accuracy',fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.savefig(output_url+'/acc_fold%s.png'%(fold_id+1))\n",
    "    plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "summary.append('traing set:')\n",
    "for i in range(len(train_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,train_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(train_acc_folds),np.std(train_acc_folds)))\n",
    "summary.append('validation set:')\n",
    "for i in range(len(val_acc_folds)):\n",
    "    summary.append('accuracy of fold #%2d: %.2f%%'%(i+1,val_acc_folds[i]))\n",
    "summary.append('%s-folds accuracy: %.2f%% (std=%.2f%%)'%(n_folds,np.mean(val_acc_folds),np.std(val_acc_folds)))\n",
    "f1 = open(output_url+'/summary_acc.txt','w')\n",
    "f1.write('\\n'.join(summary))\n",
    "f1.close()    \n",
    "\n",
    "\n",
    "f1 = open(output_url+'/model.txt','w')\n",
    "f1.write(model.extra_repr())\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.5823 acc_train: 77.78% loss_val: 0.5940 acc_val: 80.02% time: 28.9640s\n",
      "Epoch: 0002 loss_train: 0.5316 acc_train: 85.19% loss_val: 0.5944 acc_val: 79.90% time: 29.6539s\n",
      "Epoch: 0003 loss_train: 0.4992 acc_train: 77.04% loss_val: 0.5965 acc_val: 79.78% time: 28.6491s\n",
      "Epoch: 0004 loss_train: 0.4621 acc_train: 83.70% loss_val: 0.6004 acc_val: 79.55% time: 30.2080s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.082691</td>\n",
       "      <td>-0.022906</td>\n",
       "      <td>0.020578</td>\n",
       "      <td>-0.041755</td>\n",
       "      <td>0.092967</td>\n",
       "      <td>0.099629</td>\n",
       "      <td>0.004731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.123588</td>\n",
       "      <td>0.055139</td>\n",
       "      <td>-0.086906</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.093543</td>\n",
       "      <td>0.103626</td>\n",
       "      <td>0.064859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040451</td>\n",
       "      <td>-0.019063</td>\n",
       "      <td>0.114515</td>\n",
       "      <td>-0.054234</td>\n",
       "      <td>-0.089843</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>-0.023062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.073082</td>\n",
       "      <td>-0.125102</td>\n",
       "      <td>0.077213</td>\n",
       "      <td>-0.046260</td>\n",
       "      <td>0.087568</td>\n",
       "      <td>0.043720</td>\n",
       "      <td>-0.004812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.106277</td>\n",
       "      <td>-0.085882</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.033988</td>\n",
       "      <td>-0.118319</td>\n",
       "      <td>0.035555</td>\n",
       "      <td>-0.128406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.044560</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>-0.138128</td>\n",
       "      <td>0.070408</td>\n",
       "      <td>-0.100860</td>\n",
       "      <td>0.121184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.053933</td>\n",
       "      <td>-0.055062</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>-0.018133</td>\n",
       "      <td>-0.143845</td>\n",
       "      <td>0.057650</td>\n",
       "      <td>-0.125771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.022230</td>\n",
       "      <td>0.059033</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>-0.113566</td>\n",
       "      <td>-0.082434</td>\n",
       "      <td>-0.043190</td>\n",
       "      <td>-0.037768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.034038</td>\n",
       "      <td>0.015380</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.112047</td>\n",
       "      <td>0.087950</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.080823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.035669</td>\n",
       "      <td>-0.084586</td>\n",
       "      <td>-0.055770</td>\n",
       "      <td>0.098411</td>\n",
       "      <td>-0.108314</td>\n",
       "      <td>-0.018753</td>\n",
       "      <td>-0.004905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6\n",
       "0  -0.082691 -0.022906  0.020578 -0.041755  0.092967  0.099629  0.004731\n",
       "1  -0.123588  0.055139 -0.086906  0.003448  0.093543  0.103626  0.064859\n",
       "2   0.040451 -0.019063  0.114515 -0.054234 -0.089843  0.020971 -0.023062\n",
       "3  -0.073082 -0.125102  0.077213 -0.046260  0.087568  0.043720 -0.004812\n",
       "4  -0.106277 -0.085882  0.010378  0.033988 -0.118319  0.035555 -0.128406\n",
       "..       ...       ...       ...       ...       ...       ...       ...\n",
       "59  0.044560  0.044651  0.044944 -0.138128  0.070408 -0.100860  0.121184\n",
       "60 -0.053933 -0.055062  0.019197 -0.018133 -0.143845  0.057650 -0.125771\n",
       "61  0.022230  0.059033  0.092900 -0.113566 -0.082434 -0.043190 -0.037768\n",
       "62  0.034038  0.015380  0.000613  0.112047  0.087950  0.002220  0.080823\n",
       "63  0.035669 -0.084586 -0.055770  0.098411 -0.108314 -0.018753 -0.004905\n",
       "\n",
       "[64 rows x 7 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " W_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
