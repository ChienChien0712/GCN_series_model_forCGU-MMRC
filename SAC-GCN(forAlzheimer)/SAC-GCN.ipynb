{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ChebNet import *\n",
    "from utility import *\n",
    "from dataloader import *\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter\n",
    "n_folds = 2\n",
    "repeat = 100\n",
    "path = 'Dataset/TotalGCN2PET'\n",
    "Top_nFeatures = 50\n",
    "\n",
    "epochs = 70\n",
    "lr = 0.005\n",
    "wdecay = 5e-4\n",
    "log_interval = 10\n",
    "K_ChebNet = 3\n",
    "mid_layer_dim = [25]\n",
    "bias = True\n",
    "droprate = 0.8\n",
    "w_score1 = 0.5\n",
    "\n",
    "#控制S A C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 0\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 1\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 2\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 3\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 4\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 5\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 6\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 7\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 8\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 9\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 10\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 11\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 12\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 13\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 14\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 15\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 16\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 17\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 18\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 19\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 20\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 21\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 22\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 23\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 24\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 25\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 26\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 27\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 28\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 29\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 30\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 31\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 32\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 33\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 34\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 35\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 36\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 37\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 38\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 39\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 40\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 41\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 42\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 43\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 44\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 45\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 46\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 47\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 48\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 49\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 50\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 51\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 52\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 53\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 54\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 55\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 56\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 57\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 58\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 59\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 60\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 61\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 62\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 63\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 64\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 65\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 66\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 67\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 68\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 69\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 70\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 71\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 72\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 73\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 74\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 75\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 76\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 77\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 78\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 79\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 80\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 81\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 82\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 83\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 84\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 85\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 86\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 87\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 88\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 89\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 90\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 91\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 92\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 93\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 94\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 95\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 96\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 97\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 98\n",
      "fold 0\n",
      "fold 1\n",
      "sampling 99\n",
      "fold 0\n",
      "fold 1\n"
     ]
    }
   ],
   "source": [
    "ROW_NAME = []\n",
    "SEN = []\n",
    "SPE = []\n",
    "FN = []\n",
    "FP = []\n",
    "ACC = []\n",
    "PROB_train0 = []\n",
    "PROB_train1 = []\n",
    "PROB_test0 = []\n",
    "PROB_test1 = []\n",
    "for sampling in range(repeat):\n",
    "    print('sampling %s'%sampling)\n",
    "    Features1, Features2, Subject_data, train_ids, test_ids = load_data_nFolds(path=path,n_folds=n_folds)\n",
    "    ###>>>Process Adjacency Matrix (A1,A2)<<<###\n",
    "    for fold in range(n_folds):\n",
    "        print('fold %s'%fold)\n",
    "        y = [1 if i=='AD' else 0 for i in Subject_data[0][train_ids[fold]]]\n",
    "\n",
    "        #SVM-RFE (by training set) Features1\n",
    "        Features1_train = np.array(pd.DataFrame(Features1).loc[train_ids[fold]])\n",
    "        svc = SVC(kernel=\"linear\", C=1)\n",
    "        rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "        rfe.fit(Features1_train, y)\n",
    "        idx = []\n",
    "        for i in range(1,Top_nFeatures+1): #Find Top n Features\n",
    "            idx.append(list(rfe.ranking_).index(i))\n",
    "        Features1_low_dim = pd.DataFrame(Features1)[idx]\n",
    "        #SVM-RFE (by training set) Features2\n",
    "        Features2_train = np.array(pd.DataFrame(Features2).loc[train_ids[fold]])\n",
    "        svc = SVC(kernel=\"linear\", C=1)\n",
    "        rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "        rfe.fit(Features2_train, y)\n",
    "        idx = []\n",
    "        for i in range(1,Top_nFeatures+1): #Find Top n Features\n",
    "            idx.append(list(rfe.ranking_).index(i))\n",
    "        Features2_low_dim = pd.DataFrame(Features2)[idx]\n",
    "\n",
    "        #Graph Edge Connection\n",
    "        Subject_data_train = Subject_data.loc[train_ids[fold]]\n",
    "        idx_train_NC = Subject_data_train[Subject_data_train[0]=='MCI'].index #'''Negative Control'''\n",
    "        idx_train_Pt = Subject_data_train[Subject_data_train[0]=='AD'].index #'''Patient'''\n",
    "        idx_test = test_ids[fold]\n",
    "        #Adjacency Matrix (A) only connection: NC-->Pt-->test\n",
    "        n = len(Subject_data)\n",
    "        A = np.zeros([n,n])\n",
    "        for i in range(len(idx_train_NC)):\n",
    "            for j in range(len(idx_train_NC)):\n",
    "                A[i,j] = 1\n",
    "        for i in range(len(idx_train_NC),len(idx_train_NC)+len(idx_train_Pt)):\n",
    "             for j in range(len(idx_train_NC),len(idx_train_NC)+len(idx_train_Pt)):\n",
    "                A[i,j] = 1   \n",
    "        for i in range(len(idx_train_NC)+len(idx_train_Pt),n):\n",
    "             for j in range(n):\n",
    "                A[i,j] = 1  \n",
    "        #################################################\n",
    "        '''\n",
    "        A = sp.coo_matrix(A)\n",
    "        A = A + A.T.multiply(A.T > A) - A.multiply(A.T > A)\n",
    "        A = np.array(A.todense())\n",
    "        '''\n",
    "        #################################################\n",
    "        #Edge Weight Initialization\n",
    "        idx_order = idx_train_NC.append(idx_train_Pt).append(idx_test)\n",
    "        #Similarity (Features1)\n",
    "        Features1_low_dim = Features1_low_dim.loc[idx_order]\n",
    "        corr_dist = distance.pdist(np.array(Features1_low_dim),metric='correlation')\n",
    "        corr_dist = distance.squareform(corr_dist)\n",
    "        sigma = np.mean(corr_dist)\n",
    "        Features1_simirality = np.exp(-corr_dist**2/(2*sigma**2))\n",
    "        #Similarity (Features2)\n",
    "        Features2_low_dim = Features2_low_dim.loc[idx_order]\n",
    "        corr_dist = distance.pdist(np.array(Features2_low_dim),metric='correlation')\n",
    "        corr_dist = distance.squareform(corr_dist)\n",
    "        sigma = np.mean(corr_dist)\n",
    "        Features2_simirality = np.exp(-corr_dist**2/(2*sigma**2))\n",
    "        #Phenotypic Information\n",
    "        n = len(Subject_data)\n",
    "        R = np.zeros([n,n])\n",
    "        for i in Subject_data.columns[1:]:\n",
    "            a = list(Subject_data.loc[idx_order][i])\n",
    "            b = list(Subject_data.loc[idx_order][i])\n",
    "            for j in range(len(a)):\n",
    "                for k in range(len(b)):\n",
    "                    if a[j]==b[k]:\n",
    "                        R[j,k]+=1        \n",
    "        #################################################\n",
    "        #Initialized A\n",
    "        A1 = A*Features1_simirality*R\n",
    "        A2 = A*Features2_simirality*R\n",
    "        A1 = sp.coo_matrix(A1)\n",
    "        A2 = sp.coo_matrix(A2)\n",
    "        #################################################\n",
    "        L1 = torch.FloatTensor(calc_sym_norm_lap(A1))\n",
    "        L2 = torch.FloatTensor(calc_sym_norm_lap(A2))\n",
    "        #################################################\n",
    "        #Feature matrix X\n",
    "        X1 = torch.FloatTensor(np.array(pd.DataFrame(Features1_low_dim).loc[idx_order]))\n",
    "        X2 = torch.FloatTensor(np.array(pd.DataFrame(Features2_low_dim).loc[idx_order]))\n",
    "        #Y\n",
    "        Y = torch.LongTensor([1 if i=='AD' else 0 for i in Subject_data[0][idx_order]])\n",
    "        #################################################\n",
    "        #ChebNet\n",
    "        model1 = ChebyNet(K=K_ChebNet,\n",
    "                          in_features=X1.shape[1],\n",
    "                          out_features=len(set(Y.tolist())),\n",
    "                          filters_gcn=mid_layer_dim,\n",
    "                          enable_bias=bias,\n",
    "                          droprate=droprate,\n",
    "                          act_func=nn.ReLU(inplace=True))\n",
    "        model2 = ChebyNet(K=K_ChebNet,\n",
    "                          in_features=X2.shape[1],\n",
    "                          out_features=len(set(Y.tolist())),\n",
    "                          filters_gcn=mid_layer_dim,\n",
    "                          enable_bias=bias,\n",
    "                          droprate=droprate,\n",
    "                          act_func=nn.ReLU(inplace=True))\n",
    "        #info\n",
    "        n_train = len(idx_train_NC.append(idx_train_Pt))\n",
    "        #print('[Network]',model1,sep='\\n')\n",
    "        #print('------------------------------------------')\n",
    "        #print('[fold %s]'%fold)\n",
    "        #print('  n_training data:%s'%n_train)\n",
    "        #print(\"  Y's dist. of training data:%s\"%[(i,Y[:n_train].tolist().count(i)) for i in np.unique(Y.tolist())])\n",
    "        #print('  n_validation data:%s'%(len(Y)-n_train))\n",
    "        #print(\"  Y's dist. of validation data:%s\"%[(i,Y[n_train:].tolist().count(i)) for i in np.unique(Y.tolist())])\n",
    "        #print('')\n",
    "\n",
    "        #Optimizer\n",
    "        optimizer1 = optim.Adam(model1.parameters(),lr=lr, weight_decay=wdecay)\n",
    "        optimizer2 = optim.Adam(model2.parameters(),lr=lr, weight_decay=wdecay)\n",
    "        #################################################\n",
    "        def train1(epoch):\n",
    "            t = time.time()\n",
    "            Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "            #training\n",
    "            model1.train()\n",
    "            optimizer1.zero_grad()\n",
    "            data = (X1,L1)\n",
    "            output = model1(data)\n",
    "            loss_train = F.nll_loss(output[:n_train], Y[:n_train]) \n",
    "            acc_train = accuracy(output[:n_train], Y[:n_train])\n",
    "            loss_train.backward()\n",
    "            optimizer1.step()\n",
    "            #validation (先用test)\n",
    "            model1.eval()\n",
    "            data = (X1,L1)\n",
    "            output = model1(data)\n",
    "            loss_val = F.nll_loss(output[n_train:], Y[n_train:])  \n",
    "            acc_val = accuracy(output[n_train:], Y[n_train:]) \n",
    "            '''\n",
    "            if ((epoch+1) % log_interval == 0) or (epoch+1==epochs)or (epoch==0):\n",
    "                print('  Epoch: {:03d}'.format(epoch+1),\n",
    "                      'loss_train: {:.3f}'.format(loss_train.item()),\n",
    "                      'acc_train: {:.2f}%'.format(acc_train.item()),\n",
    "                      'loss_val: {:.3f}'.format(loss_val.item()),\n",
    "                      'acc_val: {:.2f}%'.format(acc_val.item()),\n",
    "                      'time: {:.3f}s'.format(time.time() - t))\n",
    "            '''\n",
    "            return loss_train.tolist(), acc_train.tolist(), loss_val.tolist(), acc_val.tolist()\n",
    "        def predict1():\n",
    "            model1.eval()\n",
    "            data = (X1,L1)\n",
    "            output = model1(data)\n",
    "            return output\n",
    "\n",
    "        def train2(epoch):\n",
    "            t = time.time()\n",
    "            #training\n",
    "            model2.train()\n",
    "            optimizer2.zero_grad()\n",
    "            data = (X2,L2)\n",
    "            output = model2(data)\n",
    "            loss_train = F.nll_loss(output[:n_train], Y[:n_train]) \n",
    "            acc_train = accuracy(output[:n_train], Y[:n_train])\n",
    "            loss_train.backward()\n",
    "            optimizer2.step()\n",
    "            #validation (先用test)\n",
    "            model2.eval()\n",
    "            data = (X2,L2)\n",
    "            output = model2(data)\n",
    "            loss_val = F.nll_loss(output[n_train:], Y[n_train:])  \n",
    "            acc_val = accuracy(output[n_train:], Y[n_train:]) \n",
    "            '''\n",
    "            if ((epoch+1) % log_interval == 0) or (epoch+1==epochs)or (epoch==0):\n",
    "                print('  Epoch: {:03d}'.format(epoch+1),\n",
    "                      'loss_train: {:.3f}'.format(loss_train.item()),\n",
    "                      'acc_train: {:.2f}%'.format(acc_train.item()),\n",
    "                      'loss_val: {:.3f}'.format(loss_val.item()),\n",
    "                      'acc_val: {:.2f}%'.format(acc_val.item()),\n",
    "                      'time: {:.3f}s'.format(time.time() - t))\n",
    "            '''\n",
    "            return loss_train.tolist(), acc_train.tolist(), loss_val.tolist(), acc_val.tolist()  \n",
    "        def predict2():\n",
    "            model2.eval()\n",
    "            data = (X2,L2)\n",
    "            output = model2(data)\n",
    "            return output\n",
    "\n",
    "\n",
    "        # Train model\n",
    "        t_total = time.time()\n",
    "        #print('  [model1]')\n",
    "        Loss_Train1, Acc_Train1, Loss_Val1, Acc_Val1 = [], [], [], []\n",
    "        for epoch in range(epochs):\n",
    "            loss_t, acc_t, loss_v, acc_v = train1(epoch)\n",
    "            Loss_Train1.append(loss_t)\n",
    "            Acc_Train1.append(acc_t)\n",
    "            Loss_Val1.append(loss_v)\n",
    "            Acc_Val1.append(acc_v)    \n",
    "        #print(\"  Optimization Finished!\")\n",
    "        #print('  [model2]')    \n",
    "        Loss_Train2, Acc_Train2, Loss_Val2, Acc_Val2 = [], [], [], []\n",
    "        for epoch in range(epochs):\n",
    "            loss_t, acc_t, loss_v, acc_v = train2(epoch)\n",
    "            Loss_Train2.append(loss_t)\n",
    "            Acc_Train2.append(acc_t)\n",
    "            Loss_Val2.append(loss_v)\n",
    "            Acc_Val2.append(acc_v)        \n",
    "        #print(\"  Optimization Finished!\")\n",
    "\n",
    "        #Total Time Elapsed\n",
    "        #print(\"  Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "\n",
    "        #################################################\n",
    "        # Get Score (Probability)\n",
    "        score1 = np.exp(predict1().tolist()).T[1]\n",
    "        score2 = np.exp(predict2().tolist()).T[1]\n",
    "        #################################################\n",
    "        #Adaptive Mechanism (Similarity of Score)\n",
    "        score1_dist = distance.squareform(distance.pdist(score1.reshape(-1,1), metric='euclidean'))\n",
    "        score2_dist = distance.squareform(distance.pdist(score2.reshape(-1,1), metric='euclidean'))\n",
    "        sigma1 = np.mean(score1_dist)\n",
    "        sigma2 = np.mean(score2_dist)\n",
    "        score1_simirality = np.exp(-score1_dist**2/(2*sigma1**2))\n",
    "        score2_simirality = np.exp(-score2_dist**2/(2*sigma2**2))\n",
    "\n",
    "        #Calibration Mechanism \n",
    "        A1_sa = A*score1_simirality*R\n",
    "        A2_sa = A*score2_simirality*R\n",
    "        A_sac = A1_sa*A2_sa \n",
    "        A_sac = (A_sac.T/A_sac.sum(axis=1)).T #normalization by row sum\n",
    "        A_sac = sp.coo_matrix(A_sac)    \n",
    "        #################################################\n",
    "        L_sac = torch.FloatTensor(calc_sym_norm_lap(A_sac))\n",
    "        #################################################\n",
    "        #training after getting new Adjacency Matrix\n",
    "\n",
    "        #initialize Parameters\n",
    "        for i in range(len(model1.gconv)):\n",
    "            model1.gconv[i].initialize_parameters()\n",
    "            model2.gconv[i].initialize_parameters()\n",
    "\n",
    "        def train_sac(epoch):\n",
    "            t = time.time()\n",
    "            Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "            #training\n",
    "            model1.train()\n",
    "            model2.train()\n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "            data1 = (X1,L_sac)\n",
    "            data2 = (X2,L_sac)\n",
    "            output1 = model1(data1)\n",
    "            output2 = model2(data2)\n",
    "            output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))\n",
    "            loss_train = F.nll_loss(output[:n_train], Y[:n_train]) \n",
    "            acc_train = accuracy(output[:n_train], Y[:n_train])\n",
    "            loss_train.backward()\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            #validation (先用test)\n",
    "            model1.eval()\n",
    "            model2.eval()\n",
    "            data1 = (X1,L_sac)\n",
    "            data2 = (X2,L_sac)\n",
    "            output1 = model1(data1)\n",
    "            output2 = model2(data2)\n",
    "            output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))   \n",
    "            loss_val = F.nll_loss(output[n_train:], Y[n_train:])  \n",
    "            acc_val = accuracy(output[n_train:], Y[n_train:]) \n",
    "            '''\n",
    "            if ((epoch+1) % log_interval == 0) or (epoch+1==epochs)or (epoch==0):\n",
    "                print('  Epoch: {:03d}'.format(epoch+1),\n",
    "                      'loss_train: {:.3f}'.format(loss_train.item()),\n",
    "                      'acc_train: {:.2f}%'.format(acc_train.item()),\n",
    "                      'loss_val: {:.3f}'.format(loss_val.item()),\n",
    "                      'acc_val: {:.2f}%'.format(acc_val.item()),\n",
    "                      'time: {:.3f}s'.format(time.time() - t))\n",
    "            '''          \n",
    "            return loss_train.tolist(), acc_train.tolist(), loss_val.tolist(), acc_val.tolist()\n",
    "        def predict_sac():\n",
    "            model1.eval()\n",
    "            model2.eval()    \n",
    "            data1 = (X1,L_sac)\n",
    "            data2 = (X2,L_sac)\n",
    "            output1 = model1(data1)\n",
    "            output2 = model2(data2)\n",
    "            output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))  \n",
    "            return output    \n",
    "        #################################################\n",
    "        # Train model\n",
    "        t_total = time.time()\n",
    "        #print('  [model_sac]')\n",
    "        Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "        for epoch in range(epochs):\n",
    "            loss_t, acc_t, loss_v, acc_v = train_sac(epoch)\n",
    "            Loss_Train.append(loss_t)\n",
    "            Acc_Train.append(acc_t)\n",
    "            Loss_Val.append(loss_v)\n",
    "            Acc_Val.append(acc_v)    \n",
    "        #print(\"  Optimization Finished!\")\n",
    "        #Total Time Elapsed\n",
    "        #print(\"  Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        #################################################\n",
    "        pred_val = predict_sac().max(1)[1].type_as(Y)[n_train:].tolist()\n",
    "        Y_val = Y[n_train:].tolist()\n",
    "\n",
    "        a,b,c,d = 0,0,0,0\n",
    "        for i in range(len(pred_val)):\n",
    "            if (pred_val[i]==1)&(Y_val[i]==1):\n",
    "                a+=1\n",
    "            elif (pred_val[i]==1)&(Y_val[i]==0): \n",
    "                b+=1\n",
    "            elif (pred_val[i]==0)&(Y_val[i]==1): \n",
    "                c+=1\n",
    "            elif (pred_val[i]==0)&(Y_val[i]==0): \n",
    "                d+=1\n",
    "        ROW_NAME.append('sampling%s_fold%s'%(sampling,fold))\n",
    "        SEN.append(a/(a+c))\n",
    "        SPE.append(d/(b+d))\n",
    "        FN.append(c/(a+c))\n",
    "        FP.append(b/(b+d))\n",
    "        ACC.append((a+d)/(a+b+c+d))\n",
    "        ####################################################\n",
    "        #機率儲存\n",
    "        prob = np.round(np.exp(predict_sac().T.tolist()[1]),4)\n",
    "        PROB_train0.append(prob[:len(idx_train_NC)])\n",
    "        PROB_train1.append(prob[len(idx_train_NC):len(idx_train_NC)+len(idx_train_Pt)])\n",
    "        prob_test0 = []\n",
    "        prob_test1 = []\n",
    "        Y_test = Y[len(idx_train_NC)+len(idx_train_Pt):].tolist()\n",
    "        for i in range(len(Y_test)):\n",
    "            if Y_test[i] == 0:\n",
    "                prob_test0.append(prob[len(idx_train_NC)+len(idx_train_Pt):][i])\n",
    "            elif Y_test[i] == 1:\n",
    "                prob_test1.append(prob[len(idx_train_NC)+len(idx_train_Pt):][i])\n",
    "        PROB_test0.append(prob_test0)\n",
    "        PROB_test1.append(prob_test1)\n",
    "PROB_train0 = pd.DataFrame(PROB_train0)\n",
    "PROB_train1 = pd.DataFrame(PROB_train1)\n",
    "PROB_test0 = pd.DataFrame(PROB_test0)   \n",
    "PROB_test1 = pd.DataFrame(PROB_test1)   \n",
    "\n",
    "PROB_train0.index = ROW_NAME\n",
    "PROB_train1.index = ROW_NAME \n",
    "PROB_test0.index = ROW_NAME    \n",
    "PROB_test1.index = ROW_NAME    \n",
    "PROB_train0.to_csv(path+'/'+'PROB_train0.csv',header=False)\n",
    "PROB_train1.to_csv(path+'/'+'PROB_train1.csv',header=False)\n",
    "PROB_test0.to_csv(path+'/'+'PROB_test0.csv',header=False)\n",
    "PROB_test1.to_csv(path+'/'+'PROB_test1.csv',header=False)\n",
    "\n",
    "ROW_NAME.append('mean')\n",
    "SEN.append(np.mean(SEN))\n",
    "SPE.append(np.mean(SPE))\n",
    "FN.append(np.mean(FN))\n",
    "FP.append(np.mean(FP))\n",
    "ACC.append(np.mean(ACC))\n",
    "ROW_NAME.append('std')\n",
    "SEN.append(np.std(SEN))\n",
    "SPE.append(np.std(SPE))\n",
    "FN.append(np.std(FN))\n",
    "FP.append(np.std(FP))\n",
    "ACC.append(np.std(ACC))\n",
    "df = pd.DataFrame([ROW_NAME,SEN,SPE,FN,FP,ACC]).T\n",
    "df.columns = ['run','sen','spe','fn','fp','acc']\n",
    "df.to_csv(path+'/'+'%sRun%sfold_result.csv'%(repeat,n_folds),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
