{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ChebNet import *\n",
    "from utility import *\n",
    "from dataloader_SamplingbyTrainRate import *\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter\n",
    "path = 'Dataset/TotalGCN2PET'\n",
    "repeat = 1\n",
    "training_rate = 0.5\n",
    "Top_nFeatures = 50\n",
    "\n",
    "epochs = 70\n",
    "lr = 0.005\n",
    "wdecay = 5e-4\n",
    "K_ChebNet = 3\n",
    "bias = True\n",
    "droprate = 0.8\n",
    "mid_layer_dim = [25]\n",
    "w_score1 = 0.5\n",
    "#控制S A C\n",
    "GCN_model = 'SA-GCN'\n",
    "\n",
    "#similarity_aware_receptive_field\n",
    "    #True:Similarity-aware Receptive Field, False: Similarity only\n",
    "if GCN_model == 'GCN':\n",
    "    similarity_aware_receptive_field = False #True:Similarity-aware Receptive Field, False: Similarity only\n",
    "    adaptive_mechanism = False\n",
    "    calibration_mechanism = False\n",
    "elif GCN_model == 'S-GCN':\n",
    "    similarity_aware_receptive_field = True\n",
    "    adaptive_mechanism = False\n",
    "    calibration_mechanism = False\n",
    "elif GCN_model == 'SA-GCN':\n",
    "    similarity_aware_receptive_field = True \n",
    "    adaptive_mechanism = True\n",
    "    calibration_mechanism = False\n",
    "elif GCN_model == 'SAC-GCN':\n",
    "    similarity_aware_receptive_field = True\n",
    "    adaptive_mechanism = True\n",
    "    calibration_mechanism = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat 0\n"
     ]
    }
   ],
   "source": [
    "ROW_NAME = []\n",
    "SEN = []\n",
    "SPE = []\n",
    "FN = []\n",
    "FP = []\n",
    "ACC = []\n",
    "PROB_train0 = []\n",
    "PROB_train1 = []\n",
    "PROB_test0 = []\n",
    "PROB_test1 = []\n",
    "writer = pd.ExcelWriter(path+'/'+'(%s)%srepeat_result.xlsx'%(GCN_model,repeat), engine = 'xlsxwriter')\n",
    "for sampling in range(repeat):\n",
    "    print('repeat %s'%sampling)\n",
    "    Features1, Features2, Subject_data, train_ids, test_ids = load_data_TrainRate(path=path,training_rate=training_rate)\n",
    "    ###>>>Process Adjacency Matrix (A1,A2)<<<###\n",
    "    y = [1 if i=='AD' else 0 for i in Subject_data[0][train_ids]]\n",
    "\n",
    "    #SVM-RFE (by training set) Features1\n",
    "    Features1_train = np.array(pd.DataFrame(Features1).loc[train_ids])\n",
    "    svc = SVC(kernel=\"linear\", C=1)\n",
    "    rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "    rfe.fit(Features1_train, y)\n",
    "    idx = []\n",
    "    for i in range(1,Top_nFeatures+1): #Find Top n Features\n",
    "        idx.append(list(rfe.ranking_).index(i))\n",
    "    Features1_low_dim = pd.DataFrame(Features1)[idx]\n",
    "    #SVM-RFE (by training set) Features2\n",
    "    Features2_train = np.array(pd.DataFrame(Features2).loc[train_ids])\n",
    "    svc = SVC(kernel=\"linear\", C=1)\n",
    "    rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "    rfe.fit(Features2_train, y)\n",
    "    idx = []\n",
    "    for i in range(1,Top_nFeatures+1): #Find Top n Features\n",
    "        idx.append(list(rfe.ranking_).index(i))\n",
    "    Features2_low_dim = pd.DataFrame(Features2)[idx]\n",
    "\n",
    "    #Graph Edge Connection\n",
    "    Subject_data_train = Subject_data.loc[train_ids]\n",
    "    idx_train_NC = Subject_data_train[Subject_data_train[0]=='MCI'].index #'''Negative Control'''\n",
    "    idx_train_Pt = Subject_data_train[Subject_data_train[0]=='AD'].index #'''Patient'''\n",
    "    idx_test = test_ids\n",
    "    #Adjacency Matrix (A) only connection: NC-->Pt-->test\n",
    "    n = len(Subject_data)\n",
    "    A = np.zeros([n,n])\n",
    "    for i in range(len(idx_train_NC)):\n",
    "        for j in range(len(idx_train_NC)):\n",
    "            A[i,j] = 1\n",
    "    for i in range(len(idx_train_NC),len(idx_train_NC)+len(idx_train_Pt)):\n",
    "         for j in range(len(idx_train_NC),len(idx_train_NC)+len(idx_train_Pt)):\n",
    "            A[i,j] = 1   \n",
    "    for i in range(len(idx_train_NC)+len(idx_train_Pt),n):\n",
    "         for j in range(n):\n",
    "            A[i,j] = 1  \n",
    "    #################################################\n",
    "    '''\n",
    "    A = sp.coo_matrix(A)\n",
    "    A = A + A.T.multiply(A.T > A) - A.multiply(A.T > A)\n",
    "    A = np.array(A.todense())\n",
    "    '''\n",
    "    #################################################\n",
    "    #Edge Weight Initialization\n",
    "    idx_order = idx_train_NC.append(idx_train_Pt).append(idx_test)\n",
    "    #Similarity (Features1)\n",
    "    Features1_low_dim = Features1_low_dim.loc[idx_order]\n",
    "    corr_dist = distance.pdist(np.array(Features1_low_dim),metric='correlation')\n",
    "    corr_dist = distance.squareform(corr_dist)\n",
    "    sigma = np.mean(corr_dist)\n",
    "    Features1_simirality = np.exp(-corr_dist**2/(2*sigma**2))\n",
    "    #Similarity (Features2)\n",
    "    Features2_low_dim = Features2_low_dim.loc[idx_order]\n",
    "    corr_dist = distance.pdist(np.array(Features2_low_dim),metric='correlation')\n",
    "    corr_dist = distance.squareform(corr_dist)\n",
    "    sigma = np.mean(corr_dist)\n",
    "    Features2_simirality = np.exp(-corr_dist**2/(2*sigma**2))\n",
    "    #Phenotypic Information\n",
    "    n = len(Subject_data)\n",
    "    R = np.zeros([n,n])\n",
    "    for i in Subject_data.columns[1:]:\n",
    "        a = list(Subject_data.loc[idx_order][i])\n",
    "        b = list(Subject_data.loc[idx_order][i])\n",
    "        for j in range(len(a)):\n",
    "            for k in range(len(b)):\n",
    "                if a[j]==b[k]:\n",
    "                    R[j,k]+=1        \n",
    "    #################################################\n",
    "    #Initialized A\n",
    "    if similarity_aware_receptive_field:\n",
    "        A1 = A*Features1_simirality*R\n",
    "        A2 = A*Features2_simirality*R\n",
    "    else:\n",
    "        A1 = A*Features1_simirality\n",
    "        A2 = A*Features2_simirality\n",
    "    A1 = sp.coo_matrix(A1)\n",
    "    A2 = sp.coo_matrix(A2)\n",
    "    #################################################\n",
    "    L1 = torch.FloatTensor(calc_sym_norm_lap(A1))\n",
    "    L2 = torch.FloatTensor(calc_sym_norm_lap(A2))\n",
    "    #################################################\n",
    "    #Feature matrix X\n",
    "    X1 = torch.FloatTensor(np.array(pd.DataFrame(Features1_low_dim).loc[idx_order]))\n",
    "    X2 = torch.FloatTensor(np.array(pd.DataFrame(Features2_low_dim).loc[idx_order]))\n",
    "    #Y\n",
    "    Y = torch.LongTensor([1 if i=='AD' else 0 for i in Subject_data[0][idx_order]])\n",
    "    out_feature = len(set(Y.tolist()))\n",
    "    #################################################\n",
    "    #Create Model\n",
    "    #ChebNet\n",
    "    model1 = ChebyNet(K=K_ChebNet,\n",
    "                      in_features=X1.shape[1],\n",
    "                      out_features=out_feature,\n",
    "                      filters_gcn=mid_layer_dim,\n",
    "                      enable_bias=bias,\n",
    "                      droprate=droprate,\n",
    "                      act_func=nn.ReLU(inplace=True))\n",
    "    model2 = ChebyNet(K=K_ChebNet,\n",
    "                      in_features=X2.shape[1],\n",
    "                      out_features=out_feature,\n",
    "                      filters_gcn=mid_layer_dim,\n",
    "                      enable_bias=bias,\n",
    "                      droprate=droprate,\n",
    "                      act_func=nn.ReLU(inplace=True))\n",
    "    #info\n",
    "    n_train = len(idx_train_NC.append(idx_train_Pt))\n",
    "\n",
    "    #Optimizer\n",
    "    optimizer1 = optim.Adam(model1.parameters(),lr=lr, weight_decay=wdecay)\n",
    "    optimizer2 = optim.Adam(model2.parameters(),lr=lr, weight_decay=wdecay)\n",
    "    #################################################\n",
    "    \n",
    "\n",
    "    ############# Train model #####################\n",
    "    #GCN or S-GCN\n",
    "    if adaptive_mechanism == False:\n",
    "        #training\n",
    "        Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "        for epoch in range(epochs):\n",
    "            loss_t, acc_t, loss_v, acc_v = train_12(epoch)\n",
    "            Loss_Train.append(loss_t)\n",
    "            Acc_Train.append(acc_t)\n",
    "            Loss_Val.append(loss_v)\n",
    "            Acc_Val.append(acc_v)    \n",
    "        #get validation results\n",
    "        pred_val = predict_12().max(1)[1].type_as(Y)[n_train:].tolist()\n",
    "        Y_val = Y[n_train:].tolist()               \n",
    "    #SA-GCN or SAC-GCN    \n",
    "    elif adaptive_mechanism == True:\n",
    "        #training\n",
    "        Loss_Train1, Acc_Train1, Loss_Val1, Acc_Val1 = [], [], [], []\n",
    "        for epoch in range(epochs):\n",
    "            loss_t, acc_t, loss_v, acc_v = train1(epoch)\n",
    "            Loss_Train1.append(loss_t)\n",
    "            Acc_Train1.append(acc_t)\n",
    "            Loss_Val1.append(loss_v)\n",
    "            Acc_Val1.append(acc_v)       \n",
    "        Loss_Train2, Acc_Train2, Loss_Val2, Acc_Val2 = [], [], [], []\n",
    "        for epoch in range(epochs):\n",
    "            loss_t, acc_t, loss_v, acc_v = train2(epoch)\n",
    "            Loss_Train2.append(loss_t)\n",
    "            Acc_Train2.append(acc_t)\n",
    "            Loss_Val2.append(loss_v)\n",
    "            Acc_Val2.append(acc_v) \n",
    "        ###########################################################################################    \n",
    "        # Get Score (Probability)\n",
    "        score1 = np.exp(predict1().tolist()).T[1]\n",
    "        score2 = np.exp(predict2().tolist()).T[1]            \n",
    "        #Adaptive Mechanism (Similarity of Score)\n",
    "        score1_dist = distance.squareform(distance.pdist(score1.reshape(-1,1), metric='euclidean'))\n",
    "        score2_dist = distance.squareform(distance.pdist(score2.reshape(-1,1), metric='euclidean'))\n",
    "        sigma1 = np.mean(score1_dist)\n",
    "        sigma2 = np.mean(score2_dist)\n",
    "        score1_simirality = np.exp(-score1_dist**2/(2*sigma1**2))\n",
    "        score2_simirality = np.exp(-score2_dist**2/(2*sigma2**2))\n",
    "        A1_sa = A*score1_simirality*R\n",
    "        A2_sa = A*score2_simirality*R            \n",
    "            \n",
    "        #SA-GCN\n",
    "        if calibration_mechanism == False:\n",
    "            A1_sa = sp.coo_matrix(A1_sa)\n",
    "            A2_sa = sp.coo_matrix(A2_sa)\n",
    "            L1_sa = torch.FloatTensor(calc_sym_norm_lap(A1_sa))\n",
    "            L2_sa = torch.FloatTensor(calc_sym_norm_lap(A2_sa))\n",
    "            #Train Model\n",
    "            #initialize Parameters\n",
    "            for i in range(len(model1.gconv)):\n",
    "                model1.gconv[i].initialize_parameters()\n",
    "                model2.gconv[i].initialize_parameters()\n",
    "            Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "            for epoch in range(epochs):\n",
    "                loss_t, acc_t, loss_v, acc_v = train_sa(epoch)\n",
    "                Loss_Train.append(loss_t)\n",
    "                Acc_Train.append(acc_t)\n",
    "                Loss_Val.append(loss_v)\n",
    "                Acc_Val.append(acc_v)   \n",
    "            #get validation results    \n",
    "            pred_val = predict_sa().max(1)[1].type_as(Y)[n_train:].tolist()\n",
    "            Y_val = Y[n_train:].tolist()  \n",
    "            \n",
    "        #SAC-GCN\n",
    "        elif calibration_mechanism == True:\n",
    "            #Calibration Mechanism \n",
    "            A_sac = A1_sa*A2_sa \n",
    "            A_sac = (A_sac.T/A_sac.sum(axis=1)).T #normalization by row sum\n",
    "            A_sac = sp.coo_matrix(A_sac)     \n",
    "            L_sac = torch.FloatTensor(calc_sym_norm_lap(A_sac))\n",
    "    \n",
    "            #Train Model\n",
    "            #initialize Parameters\n",
    "            for i in range(len(model1.gconv)):\n",
    "                model1.gconv[i].initialize_parameters()\n",
    "                model2.gconv[i].initialize_parameters()\n",
    "            Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "            for epoch in range(epochs):\n",
    "                loss_t, acc_t, loss_v, acc_v = train_sac(epoch)\n",
    "                Loss_Train.append(loss_t)\n",
    "                Acc_Train.append(acc_t)\n",
    "                Loss_Val.append(loss_v)\n",
    "                Acc_Val.append(acc_v)   \n",
    "            #get validation results    \n",
    "            pred_val = predict_sac().max(1)[1].type_as(Y)[n_train:].tolist()\n",
    "            Y_val = Y[n_train:].tolist()\n",
    "\n",
    "\n",
    "    a,b,c,d = 0,0,0,0\n",
    "    for i in range(len(pred_val)):\n",
    "        if (pred_val[i]==1)&(Y_val[i]==1):\n",
    "            a+=1\n",
    "        elif (pred_val[i]==1)&(Y_val[i]==0): \n",
    "            b+=1\n",
    "        elif (pred_val[i]==0)&(Y_val[i]==1): \n",
    "            c+=1\n",
    "        elif (pred_val[i]==0)&(Y_val[i]==0): \n",
    "            d+=1\n",
    "    ROW_NAME.append('repeat %s'%(sampling))\n",
    "    SEN.append(a/(a+c))\n",
    "    SPE.append(d/(b+d))\n",
    "    FN.append(c/(a+c))\n",
    "    FP.append(b/(b+d))\n",
    "    ACC.append((a+d)/(a+b+c+d))\n",
    "    ####################################################\n",
    "    #機率儲存\n",
    "    prob = np.round(np.exp(predict_sac().T.tolist()[1]),4)\n",
    "    PROB_train0.append(prob[:len(idx_train_NC)])\n",
    "    PROB_train1.append(prob[len(idx_train_NC):len(idx_train_NC)+len(idx_train_Pt)])\n",
    "    prob_test0 = []\n",
    "    prob_test1 = []\n",
    "    Y_test = Y[len(idx_train_NC)+len(idx_train_Pt):].tolist()\n",
    "    for i in range(len(Y_test)):\n",
    "        if Y_test[i] == 0:\n",
    "            prob_test0.append(prob[len(idx_train_NC)+len(idx_train_Pt):][i])\n",
    "        elif Y_test[i] == 1:\n",
    "            prob_test1.append(prob[len(idx_train_NC)+len(idx_train_Pt):][i])\n",
    "    PROB_test0.append(prob_test0)\n",
    "    PROB_test1.append(prob_test1)\n",
    "    \n",
    "PROB_train0 = pd.DataFrame(PROB_train0)\n",
    "PROB_train1 = pd.DataFrame(PROB_train1)\n",
    "PROB_test0 = pd.DataFrame(PROB_test0)   \n",
    "PROB_test1 = pd.DataFrame(PROB_test1)   \n",
    "\n",
    "PROB_train0.index = ROW_NAME\n",
    "PROB_train1.index = ROW_NAME \n",
    "PROB_test0.index = ROW_NAME    \n",
    "PROB_test1.index = ROW_NAME    \n",
    "PROB_train0.to_csv(path+'/'+'PROB_train0.csv',header=False)\n",
    "PROB_train1.to_csv(path+'/'+'PROB_train1.csv',header=False)\n",
    "PROB_test0.to_csv(path+'/'+'PROB_test0.csv',header=False)\n",
    "PROB_test1.to_csv(path+'/'+'PROB_test1.csv',header=False)\n",
    "\n",
    "ROW_NAME.append('mean')\n",
    "SEN.append(np.mean(SEN))\n",
    "SPE.append(np.mean(SPE))\n",
    "FN.append(np.mean(FN))\n",
    "FP.append(np.mean(FP))\n",
    "ACC.append(np.mean(ACC))\n",
    "ROW_NAME.append('std')\n",
    "SEN.append(np.std(SEN))\n",
    "SPE.append(np.std(SPE))\n",
    "FN.append(np.std(FN))\n",
    "FP.append(np.std(FP))\n",
    "ACC.append(np.std(ACC))\n",
    "df = pd.DataFrame([ROW_NAME,SEN,SPE,FN,FP,ACC]).T\n",
    "df.columns = ['run','sen','spe','fn','fp','acc']\n",
    "df.to_excel(writer, sheet_name='validation', index=False)\n",
    "\n",
    "#hyperparameter\n",
    "files = os.listdir(path)    \n",
    "r = re.compile(\".*_network.xlsx\")\n",
    "Feature_mx = list(filter(r.match, files))  \n",
    "hyperparameters = [repeat,training_rate, 1-training_rate, Top_nFeatures, epochs, lr, wdecay, K_ChebNet, bias, droprate, \\\n",
    "                   Top_nFeatures, '/'.join(list(map(str,mid_layer_dim))), out_feature, \\\n",
    "                   Feature_mx[0], Feature_mx[1], w_score1, 1-w_score1, \\\n",
    "                   GCN_model, similarity_aware_receptive_field, adaptive_mechanism, calibration_mechanism] \n",
    "hyperparameters_index = ['repeat','training proportion','validation proportion','#feature selected by SVM-RFE', 'epoch', 'learning rate', \\\n",
    "                         'w_decay of L2-regularization','K of ChebNet','bias','dropout rate','input features', \\\n",
    "                         'hidden layer', 'output features', 'feature1','feature2','w_feature1','w_feature2', \\\n",
    "                         'Model','similarity aware receptive field', 'adaptive mechanism', 'calibration_mechanism']\n",
    "hyperparameters = pd.DataFrame(hyperparameters)\n",
    "hyperparameters.index = hyperparameters_index\n",
    "hyperparameters.to_excel(writer, sheet_name='hyperparameters', header=False)\n",
    "\n",
    "writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(epoch):\n",
    "    t = time.time()\n",
    "    Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "    #training\n",
    "    model1.train()\n",
    "    optimizer1.zero_grad()\n",
    "    data = (X1,L1)\n",
    "    output = model1(data)\n",
    "    loss_train = F.nll_loss(output[:n_train], Y[:n_train]) \n",
    "    acc_train = accuracy(output[:n_train], Y[:n_train])\n",
    "    loss_train.backward()\n",
    "    optimizer1.step()\n",
    "    #validation (先用test)\n",
    "    model1.eval()\n",
    "    data = (X1,L1)\n",
    "    output = model1(data)\n",
    "    loss_val = F.nll_loss(output[n_train:], Y[n_train:])  \n",
    "    acc_val = accuracy(output[n_train:], Y[n_train:]) \n",
    "    return loss_train.tolist(), acc_train.tolist(), loss_val.tolist(), acc_val.tolist()\n",
    "def predict1():\n",
    "    model1.eval()\n",
    "    data = (X1,L1)\n",
    "    output = model1(data)\n",
    "    return output\n",
    "\n",
    "def train2(epoch):\n",
    "    t = time.time()\n",
    "    #training\n",
    "    model2.train()\n",
    "    optimizer2.zero_grad()\n",
    "    data = (X2,L2)\n",
    "    output = model2(data)\n",
    "    loss_train = F.nll_loss(output[:n_train], Y[:n_train]) \n",
    "    acc_train = accuracy(output[:n_train], Y[:n_train])\n",
    "    loss_train.backward()\n",
    "    optimizer2.step()\n",
    "    #validation (先用test)\n",
    "    model2.eval()\n",
    "    data = (X2,L2)\n",
    "    output = model2(data)\n",
    "    loss_val = F.nll_loss(output[n_train:], Y[n_train:])  \n",
    "    acc_val = accuracy(output[n_train:], Y[n_train:]) \n",
    "    return loss_train.tolist(), acc_train.tolist(), loss_val.tolist(), acc_val.tolist()  \n",
    "def predict2():\n",
    "    model2.eval()\n",
    "    data = (X2,L2)\n",
    "    output = model2(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_12(epoch):\n",
    "    t = time.time()\n",
    "    Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "    #training\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    data1 = (X1,L1)\n",
    "    data2 = (X2,L2)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))\n",
    "    loss_train = F.nll_loss(output[:n_train], Y[:n_train]) \n",
    "    acc_train = accuracy(output[:n_train], Y[:n_train])\n",
    "    loss_train.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    #validation (先用test)\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    data1 = (X1,L1)\n",
    "    data2 = (X2,L2)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))   \n",
    "    loss_val = F.nll_loss(output[n_train:], Y[n_train:])  \n",
    "    acc_val = accuracy(output[n_train:], Y[n_train:])       \n",
    "    return loss_train.tolist(), acc_train.tolist(), loss_val.tolist(), acc_val.tolist()\n",
    "def predict_12():\n",
    "    model1.eval()\n",
    "    model2.eval()    \n",
    "    data1 = (X1,L1)\n",
    "    data2 = (X2,L2)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))  \n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sa(epoch):\n",
    "    t = time.time()\n",
    "    Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "    #training\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    data1 = (X1,L1_sa)\n",
    "    data2 = (X2,L2_sa)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))\n",
    "    loss_train = F.nll_loss(output[:n_train], Y[:n_train]) \n",
    "    acc_train = accuracy(output[:n_train], Y[:n_train])\n",
    "    loss_train.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    #validation (先用test)\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    data1 = (X1,L1_sa)\n",
    "    data2 = (X2,L2_sa)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))   \n",
    "    loss_val = F.nll_loss(output[n_train:], Y[n_train:])  \n",
    "    acc_val = accuracy(output[n_train:], Y[n_train:])       \n",
    "    return loss_train.tolist(), acc_train.tolist(), loss_val.tolist(), acc_val.tolist()\n",
    "def predict_sa():\n",
    "    model1.eval()\n",
    "    model2.eval()    \n",
    "    data1 = (X1,L1_sa)\n",
    "    data2 = (X2,L2_sa)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))  \n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac(epoch):\n",
    "    t = time.time()\n",
    "    Loss_Train, Acc_Train, Loss_Val, Acc_Val = [], [], [], []\n",
    "    #training\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    data1 = (X1,L_sac)\n",
    "    data2 = (X2,L_sac)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))\n",
    "    loss_train = F.nll_loss(output[:n_train], Y[:n_train]) \n",
    "    acc_train = accuracy(output[:n_train], Y[:n_train])\n",
    "    loss_train.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    #validation (先用test)\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    data1 = (X1,L_sac)\n",
    "    data2 = (X2,L_sac)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))   \n",
    "    loss_val = F.nll_loss(output[n_train:], Y[n_train:])  \n",
    "    acc_val = accuracy(output[n_train:], Y[n_train:])       \n",
    "    return loss_train.tolist(), acc_train.tolist(), loss_val.tolist(), acc_val.tolist()\n",
    "def predict_sac():\n",
    "    model1.eval()\n",
    "    model2.eval()    \n",
    "    data1 = (X1,L_sac)\n",
    "    data2 = (X2,L_sac)\n",
    "    output1 = model1(data1)\n",
    "    output2 = model2(data2)\n",
    "    output = torch.log(w_score1*torch.exp(output1)+(1-w_score1)*torch.exp(output2))  \n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
